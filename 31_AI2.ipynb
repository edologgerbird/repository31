{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Small Scale Implementation of AI2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploratory Data Analysis\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from numpy import where\n",
    "from numpy.testing import assert_array_almost_equal\n",
    "import seaborn as sns\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, roc_curve, auc, recall_score, precision_score, f1_score \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Deep Learning\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import layers, backend\n",
    "from keras.losses import binary_crossentropy\n",
    "\n",
    "# Settings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import gc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Prepocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>trans_date_trans_time</th>\n",
       "      <th>cc_num</th>\n",
       "      <th>merchant</th>\n",
       "      <th>category</th>\n",
       "      <th>amt</th>\n",
       "      <th>first</th>\n",
       "      <th>last</th>\n",
       "      <th>gender</th>\n",
       "      <th>street</th>\n",
       "      <th>...</th>\n",
       "      <th>lat</th>\n",
       "      <th>long</th>\n",
       "      <th>city_pop</th>\n",
       "      <th>job</th>\n",
       "      <th>dob</th>\n",
       "      <th>trans_num</th>\n",
       "      <th>unix_time</th>\n",
       "      <th>merch_lat</th>\n",
       "      <th>merch_long</th>\n",
       "      <th>is_fraud</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2019-01-01 00:00:18</td>\n",
       "      <td>2703186189652095</td>\n",
       "      <td>fraud_Rippin, Kub and Mann</td>\n",
       "      <td>misc_net</td>\n",
       "      <td>4.97</td>\n",
       "      <td>Jennifer</td>\n",
       "      <td>Banks</td>\n",
       "      <td>F</td>\n",
       "      <td>561 Perry Cove</td>\n",
       "      <td>...</td>\n",
       "      <td>36.0788</td>\n",
       "      <td>-81.1781</td>\n",
       "      <td>3495</td>\n",
       "      <td>Psychologist, counselling</td>\n",
       "      <td>1988-03-09</td>\n",
       "      <td>0b242abb623afc578575680df30655b9</td>\n",
       "      <td>1325376018</td>\n",
       "      <td>36.011293</td>\n",
       "      <td>-82.048315</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2019-01-01 00:00:44</td>\n",
       "      <td>630423337322</td>\n",
       "      <td>fraud_Heller, Gutmann and Zieme</td>\n",
       "      <td>grocery_pos</td>\n",
       "      <td>107.23</td>\n",
       "      <td>Stephanie</td>\n",
       "      <td>Gill</td>\n",
       "      <td>F</td>\n",
       "      <td>43039 Riley Greens Suite 393</td>\n",
       "      <td>...</td>\n",
       "      <td>48.8878</td>\n",
       "      <td>-118.2105</td>\n",
       "      <td>149</td>\n",
       "      <td>Special educational needs teacher</td>\n",
       "      <td>1978-06-21</td>\n",
       "      <td>1f76529f8574734946361c461b024d99</td>\n",
       "      <td>1325376044</td>\n",
       "      <td>49.159047</td>\n",
       "      <td>-118.186462</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2019-01-01 00:00:51</td>\n",
       "      <td>38859492057661</td>\n",
       "      <td>fraud_Lind-Buckridge</td>\n",
       "      <td>entertainment</td>\n",
       "      <td>220.11</td>\n",
       "      <td>Edward</td>\n",
       "      <td>Sanchez</td>\n",
       "      <td>M</td>\n",
       "      <td>594 White Dale Suite 530</td>\n",
       "      <td>...</td>\n",
       "      <td>42.1808</td>\n",
       "      <td>-112.2620</td>\n",
       "      <td>4154</td>\n",
       "      <td>Nature conservation officer</td>\n",
       "      <td>1962-01-19</td>\n",
       "      <td>a1a22d70485983eac12b5b88dad1cf95</td>\n",
       "      <td>1325376051</td>\n",
       "      <td>43.150704</td>\n",
       "      <td>-112.154481</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2019-01-01 00:01:16</td>\n",
       "      <td>3534093764340240</td>\n",
       "      <td>fraud_Kutch, Hermiston and Farrell</td>\n",
       "      <td>gas_transport</td>\n",
       "      <td>45.00</td>\n",
       "      <td>Jeremy</td>\n",
       "      <td>White</td>\n",
       "      <td>M</td>\n",
       "      <td>9443 Cynthia Court Apt. 038</td>\n",
       "      <td>...</td>\n",
       "      <td>46.2306</td>\n",
       "      <td>-112.1138</td>\n",
       "      <td>1939</td>\n",
       "      <td>Patent attorney</td>\n",
       "      <td>1967-01-12</td>\n",
       "      <td>6b849c168bdad6f867558c3793159a81</td>\n",
       "      <td>1325376076</td>\n",
       "      <td>47.034331</td>\n",
       "      <td>-112.561071</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2019-01-01 00:03:06</td>\n",
       "      <td>375534208663984</td>\n",
       "      <td>fraud_Keeling-Crist</td>\n",
       "      <td>misc_pos</td>\n",
       "      <td>41.96</td>\n",
       "      <td>Tyler</td>\n",
       "      <td>Garcia</td>\n",
       "      <td>M</td>\n",
       "      <td>408 Bradley Rest</td>\n",
       "      <td>...</td>\n",
       "      <td>38.4207</td>\n",
       "      <td>-79.4629</td>\n",
       "      <td>99</td>\n",
       "      <td>Dance movement psychotherapist</td>\n",
       "      <td>1986-03-28</td>\n",
       "      <td>a41d7549acf90789359a9aa5346dcb46</td>\n",
       "      <td>1325376186</td>\n",
       "      <td>38.674999</td>\n",
       "      <td>-78.632459</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>2019-01-01 00:04:08</td>\n",
       "      <td>4767265376804500</td>\n",
       "      <td>fraud_Stroman, Hudson and Erdman</td>\n",
       "      <td>gas_transport</td>\n",
       "      <td>94.63</td>\n",
       "      <td>Jennifer</td>\n",
       "      <td>Conner</td>\n",
       "      <td>F</td>\n",
       "      <td>4655 David Island</td>\n",
       "      <td>...</td>\n",
       "      <td>40.3750</td>\n",
       "      <td>-75.2045</td>\n",
       "      <td>2158</td>\n",
       "      <td>Transport planner</td>\n",
       "      <td>1961-06-19</td>\n",
       "      <td>189a841a0a8ba03058526bcfe566aab5</td>\n",
       "      <td>1325376248</td>\n",
       "      <td>40.653382</td>\n",
       "      <td>-76.152667</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>2019-01-01 00:04:42</td>\n",
       "      <td>30074693890476</td>\n",
       "      <td>fraud_Rowe-Vandervort</td>\n",
       "      <td>grocery_net</td>\n",
       "      <td>44.54</td>\n",
       "      <td>Kelsey</td>\n",
       "      <td>Richards</td>\n",
       "      <td>F</td>\n",
       "      <td>889 Sarah Station Suite 624</td>\n",
       "      <td>...</td>\n",
       "      <td>37.9931</td>\n",
       "      <td>-100.9893</td>\n",
       "      <td>2691</td>\n",
       "      <td>Arboriculturist</td>\n",
       "      <td>1993-08-16</td>\n",
       "      <td>83ec1cc84142af6e2acf10c44949e720</td>\n",
       "      <td>1325376282</td>\n",
       "      <td>37.162705</td>\n",
       "      <td>-100.153370</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>2019-01-01 00:05:08</td>\n",
       "      <td>6011360759745864</td>\n",
       "      <td>fraud_Corwin-Collins</td>\n",
       "      <td>gas_transport</td>\n",
       "      <td>71.65</td>\n",
       "      <td>Steven</td>\n",
       "      <td>Williams</td>\n",
       "      <td>M</td>\n",
       "      <td>231 Flores Pass Suite 720</td>\n",
       "      <td>...</td>\n",
       "      <td>38.8432</td>\n",
       "      <td>-78.6003</td>\n",
       "      <td>6018</td>\n",
       "      <td>Designer, multimedia</td>\n",
       "      <td>1947-08-21</td>\n",
       "      <td>6d294ed2cc447d2c71c7171a3d54967c</td>\n",
       "      <td>1325376308</td>\n",
       "      <td>38.948089</td>\n",
       "      <td>-78.540296</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>2019-01-01 00:05:18</td>\n",
       "      <td>4922710831011201</td>\n",
       "      <td>fraud_Herzog Ltd</td>\n",
       "      <td>misc_pos</td>\n",
       "      <td>4.27</td>\n",
       "      <td>Heather</td>\n",
       "      <td>Chase</td>\n",
       "      <td>F</td>\n",
       "      <td>6888 Hicks Stream Suite 954</td>\n",
       "      <td>...</td>\n",
       "      <td>40.3359</td>\n",
       "      <td>-79.6607</td>\n",
       "      <td>1472</td>\n",
       "      <td>Public affairs consultant</td>\n",
       "      <td>1941-03-07</td>\n",
       "      <td>fc28024ce480f8ef21a32d64c93a29f5</td>\n",
       "      <td>1325376318</td>\n",
       "      <td>40.351813</td>\n",
       "      <td>-79.958146</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>2019-01-01 00:06:01</td>\n",
       "      <td>2720830304681674</td>\n",
       "      <td>fraud_Schoen, Kuphal and Nitzsche</td>\n",
       "      <td>grocery_pos</td>\n",
       "      <td>198.39</td>\n",
       "      <td>Melissa</td>\n",
       "      <td>Aguilar</td>\n",
       "      <td>F</td>\n",
       "      <td>21326 Taylor Squares Suite 708</td>\n",
       "      <td>...</td>\n",
       "      <td>36.5220</td>\n",
       "      <td>-87.3490</td>\n",
       "      <td>151785</td>\n",
       "      <td>Pathologist</td>\n",
       "      <td>1974-03-28</td>\n",
       "      <td>3b9014ea8fb80bd65de0b1463b00b00e</td>\n",
       "      <td>1325376361</td>\n",
       "      <td>37.179198</td>\n",
       "      <td>-87.485381</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0 trans_date_trans_time            cc_num  \\\n",
       "0           0   2019-01-01 00:00:18  2703186189652095   \n",
       "1           1   2019-01-01 00:00:44      630423337322   \n",
       "2           2   2019-01-01 00:00:51    38859492057661   \n",
       "3           3   2019-01-01 00:01:16  3534093764340240   \n",
       "4           4   2019-01-01 00:03:06   375534208663984   \n",
       "5           5   2019-01-01 00:04:08  4767265376804500   \n",
       "6           6   2019-01-01 00:04:42    30074693890476   \n",
       "7           7   2019-01-01 00:05:08  6011360759745864   \n",
       "8           8   2019-01-01 00:05:18  4922710831011201   \n",
       "9           9   2019-01-01 00:06:01  2720830304681674   \n",
       "\n",
       "                             merchant       category     amt      first  \\\n",
       "0          fraud_Rippin, Kub and Mann       misc_net    4.97   Jennifer   \n",
       "1     fraud_Heller, Gutmann and Zieme    grocery_pos  107.23  Stephanie   \n",
       "2                fraud_Lind-Buckridge  entertainment  220.11     Edward   \n",
       "3  fraud_Kutch, Hermiston and Farrell  gas_transport   45.00     Jeremy   \n",
       "4                 fraud_Keeling-Crist       misc_pos   41.96      Tyler   \n",
       "5    fraud_Stroman, Hudson and Erdman  gas_transport   94.63   Jennifer   \n",
       "6               fraud_Rowe-Vandervort    grocery_net   44.54     Kelsey   \n",
       "7                fraud_Corwin-Collins  gas_transport   71.65     Steven   \n",
       "8                    fraud_Herzog Ltd       misc_pos    4.27    Heather   \n",
       "9   fraud_Schoen, Kuphal and Nitzsche    grocery_pos  198.39    Melissa   \n",
       "\n",
       "       last gender                          street  ...      lat      long  \\\n",
       "0     Banks      F                  561 Perry Cove  ...  36.0788  -81.1781   \n",
       "1      Gill      F    43039 Riley Greens Suite 393  ...  48.8878 -118.2105   \n",
       "2   Sanchez      M        594 White Dale Suite 530  ...  42.1808 -112.2620   \n",
       "3     White      M     9443 Cynthia Court Apt. 038  ...  46.2306 -112.1138   \n",
       "4    Garcia      M                408 Bradley Rest  ...  38.4207  -79.4629   \n",
       "5    Conner      F               4655 David Island  ...  40.3750  -75.2045   \n",
       "6  Richards      F     889 Sarah Station Suite 624  ...  37.9931 -100.9893   \n",
       "7  Williams      M       231 Flores Pass Suite 720  ...  38.8432  -78.6003   \n",
       "8     Chase      F     6888 Hicks Stream Suite 954  ...  40.3359  -79.6607   \n",
       "9   Aguilar      F  21326 Taylor Squares Suite 708  ...  36.5220  -87.3490   \n",
       "\n",
       "   city_pop                                job         dob  \\\n",
       "0      3495          Psychologist, counselling  1988-03-09   \n",
       "1       149  Special educational needs teacher  1978-06-21   \n",
       "2      4154        Nature conservation officer  1962-01-19   \n",
       "3      1939                    Patent attorney  1967-01-12   \n",
       "4        99     Dance movement psychotherapist  1986-03-28   \n",
       "5      2158                  Transport planner  1961-06-19   \n",
       "6      2691                    Arboriculturist  1993-08-16   \n",
       "7      6018               Designer, multimedia  1947-08-21   \n",
       "8      1472          Public affairs consultant  1941-03-07   \n",
       "9    151785                        Pathologist  1974-03-28   \n",
       "\n",
       "                          trans_num   unix_time  merch_lat  merch_long  \\\n",
       "0  0b242abb623afc578575680df30655b9  1325376018  36.011293  -82.048315   \n",
       "1  1f76529f8574734946361c461b024d99  1325376044  49.159047 -118.186462   \n",
       "2  a1a22d70485983eac12b5b88dad1cf95  1325376051  43.150704 -112.154481   \n",
       "3  6b849c168bdad6f867558c3793159a81  1325376076  47.034331 -112.561071   \n",
       "4  a41d7549acf90789359a9aa5346dcb46  1325376186  38.674999  -78.632459   \n",
       "5  189a841a0a8ba03058526bcfe566aab5  1325376248  40.653382  -76.152667   \n",
       "6  83ec1cc84142af6e2acf10c44949e720  1325376282  37.162705 -100.153370   \n",
       "7  6d294ed2cc447d2c71c7171a3d54967c  1325376308  38.948089  -78.540296   \n",
       "8  fc28024ce480f8ef21a32d64c93a29f5  1325376318  40.351813  -79.958146   \n",
       "9  3b9014ea8fb80bd65de0b1463b00b00e  1325376361  37.179198  -87.485381   \n",
       "\n",
       "   is_fraud  \n",
       "0         0  \n",
       "1         0  \n",
       "2         0  \n",
       "3         0  \n",
       "4         0  \n",
       "5         0  \n",
       "6         0  \n",
       "7         0  \n",
       "8         0  \n",
       "9         0  \n",
       "\n",
       "[10 rows x 23 columns]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading and previewing data\n",
    "\n",
    "ds = pd.read_csv('fraudTrain.csv')[:100000] \n",
    "ds.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>cc_num</th>\n",
       "      <th>amt</th>\n",
       "      <th>zip</th>\n",
       "      <th>lat</th>\n",
       "      <th>long</th>\n",
       "      <th>city_pop</th>\n",
       "      <th>unix_time</th>\n",
       "      <th>merch_lat</th>\n",
       "      <th>merch_long</th>\n",
       "      <th>is_fraud</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>100000.000000</td>\n",
       "      <td>1.000000e+05</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>1.000000e+05</td>\n",
       "      <td>1.000000e+05</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>49999.500000</td>\n",
       "      <td>4.183992e+17</td>\n",
       "      <td>71.908232</td>\n",
       "      <td>48720.506960</td>\n",
       "      <td>38.538809</td>\n",
       "      <td>-90.179763</td>\n",
       "      <td>8.918773e+04</td>\n",
       "      <td>1.327916e+09</td>\n",
       "      <td>38.539719</td>\n",
       "      <td>-90.180833</td>\n",
       "      <td>0.009900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>28867.657797</td>\n",
       "      <td>1.309964e+18</td>\n",
       "      <td>145.895400</td>\n",
       "      <td>26895.490176</td>\n",
       "      <td>5.077738</td>\n",
       "      <td>13.771942</td>\n",
       "      <td>3.015788e+05</td>\n",
       "      <td>1.459296e+06</td>\n",
       "      <td>5.110508</td>\n",
       "      <td>13.783471</td>\n",
       "      <td>0.099005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.041621e+10</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1257.000000</td>\n",
       "      <td>20.027100</td>\n",
       "      <td>-165.672300</td>\n",
       "      <td>2.300000e+01</td>\n",
       "      <td>1.325376e+09</td>\n",
       "      <td>19.029798</td>\n",
       "      <td>-166.670132</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>24999.750000</td>\n",
       "      <td>1.800365e+14</td>\n",
       "      <td>9.710000</td>\n",
       "      <td>26041.000000</td>\n",
       "      <td>34.668900</td>\n",
       "      <td>-96.790900</td>\n",
       "      <td>7.430000e+02</td>\n",
       "      <td>1.326627e+09</td>\n",
       "      <td>34.750439</td>\n",
       "      <td>-96.867861</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>49999.500000</td>\n",
       "      <td>3.519607e+15</td>\n",
       "      <td>48.150000</td>\n",
       "      <td>48174.000000</td>\n",
       "      <td>39.354300</td>\n",
       "      <td>-87.458100</td>\n",
       "      <td>2.456000e+03</td>\n",
       "      <td>1.327877e+09</td>\n",
       "      <td>39.365135</td>\n",
       "      <td>-87.407044</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>74999.250000</td>\n",
       "      <td>4.642255e+15</td>\n",
       "      <td>83.870000</td>\n",
       "      <td>72011.000000</td>\n",
       "      <td>41.894800</td>\n",
       "      <td>-80.128400</td>\n",
       "      <td>2.047800e+04</td>\n",
       "      <td>1.329150e+09</td>\n",
       "      <td>41.943393</td>\n",
       "      <td>-80.180297</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>99999.000000</td>\n",
       "      <td>4.992346e+18</td>\n",
       "      <td>12788.070000</td>\n",
       "      <td>99783.000000</td>\n",
       "      <td>65.689900</td>\n",
       "      <td>-67.950300</td>\n",
       "      <td>2.906700e+06</td>\n",
       "      <td>1.330444e+09</td>\n",
       "      <td>66.659242</td>\n",
       "      <td>-66.967742</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Unnamed: 0        cc_num            amt            zip  \\\n",
       "count  100000.000000  1.000000e+05  100000.000000  100000.000000   \n",
       "mean    49999.500000  4.183992e+17      71.908232   48720.506960   \n",
       "std     28867.657797  1.309964e+18     145.895400   26895.490176   \n",
       "min         0.000000  6.041621e+10       1.000000    1257.000000   \n",
       "25%     24999.750000  1.800365e+14       9.710000   26041.000000   \n",
       "50%     49999.500000  3.519607e+15      48.150000   48174.000000   \n",
       "75%     74999.250000  4.642255e+15      83.870000   72011.000000   \n",
       "max     99999.000000  4.992346e+18   12788.070000   99783.000000   \n",
       "\n",
       "                 lat           long      city_pop     unix_time  \\\n",
       "count  100000.000000  100000.000000  1.000000e+05  1.000000e+05   \n",
       "mean       38.538809     -90.179763  8.918773e+04  1.327916e+09   \n",
       "std         5.077738      13.771942  3.015788e+05  1.459296e+06   \n",
       "min        20.027100    -165.672300  2.300000e+01  1.325376e+09   \n",
       "25%        34.668900     -96.790900  7.430000e+02  1.326627e+09   \n",
       "50%        39.354300     -87.458100  2.456000e+03  1.327877e+09   \n",
       "75%        41.894800     -80.128400  2.047800e+04  1.329150e+09   \n",
       "max        65.689900     -67.950300  2.906700e+06  1.330444e+09   \n",
       "\n",
       "           merch_lat     merch_long       is_fraud  \n",
       "count  100000.000000  100000.000000  100000.000000  \n",
       "mean       38.539719     -90.180833       0.009900  \n",
       "std         5.110508      13.783471       0.099005  \n",
       "min        19.029798    -166.670132       0.000000  \n",
       "25%        34.750439     -96.867861       0.000000  \n",
       "50%        39.365135     -87.407044       0.000000  \n",
       "75%        41.943393     -80.180297       0.000000  \n",
       "max        66.659242     -66.967742       1.000000  "
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sanity Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 100000 entries, 0 to 99999\n",
      "Data columns (total 23 columns):\n",
      " #   Column                 Non-Null Count   Dtype  \n",
      "---  ------                 --------------   -----  \n",
      " 0   Unnamed: 0             100000 non-null  int64  \n",
      " 1   trans_date_trans_time  100000 non-null  object \n",
      " 2   cc_num                 100000 non-null  int64  \n",
      " 3   merchant               100000 non-null  object \n",
      " 4   category               100000 non-null  object \n",
      " 5   amt                    100000 non-null  float64\n",
      " 6   first                  100000 non-null  object \n",
      " 7   last                   100000 non-null  object \n",
      " 8   gender                 100000 non-null  object \n",
      " 9   street                 100000 non-null  object \n",
      " 10  city                   100000 non-null  object \n",
      " 11  state                  100000 non-null  object \n",
      " 12  zip                    100000 non-null  int64  \n",
      " 13  lat                    100000 non-null  float64\n",
      " 14  long                   100000 non-null  float64\n",
      " 15  city_pop               100000 non-null  int64  \n",
      " 16  job                    100000 non-null  object \n",
      " 17  dob                    100000 non-null  object \n",
      " 18  trans_num              100000 non-null  object \n",
      " 19  unix_time              100000 non-null  int64  \n",
      " 20  merch_lat              100000 non-null  float64\n",
      " 21  merch_long             100000 non-null  float64\n",
      " 22  is_fraud               100000 non-null  int64  \n",
      "dtypes: float64(5), int64(6), object(12)\n",
      "memory usage: 17.5+ MB\n"
     ]
    }
   ],
   "source": [
    "ds.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ds.drop(labels = ['Unnamed: 0', 'trans_date_trans_time','street', 'first', 'last', 'city', 'zip', 'trans_num'], axis = 1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(ds.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({0: 99010, 1: 990})"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(ds.is_fraud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "labels, values = zip(*Counter(ds.is_fraud).items())\n",
    "\n",
    "indexes = np.arange(len(labels))\n",
    "width = 1\n",
    "\n",
    "plt.bar(indexes, values, width)\n",
    "plt.xticks(indexes, labels)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting date of birth into 3 integer variables year, month and day\n",
    "\n",
    "ds['birth_year'] = ds['dob'].apply(lambda x: int(x[0:4]))\n",
    "ds['birth_month'] = ds['dob'].apply(lambda x: int(x[5:7]))\n",
    "ds['birth_day'] = ds['dob'].apply(lambda x: int(x[8:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cc_num</th>\n",
       "      <th>merchant</th>\n",
       "      <th>category</th>\n",
       "      <th>amt</th>\n",
       "      <th>gender</th>\n",
       "      <th>state</th>\n",
       "      <th>lat</th>\n",
       "      <th>long</th>\n",
       "      <th>city_pop</th>\n",
       "      <th>job</th>\n",
       "      <th>dob</th>\n",
       "      <th>unix_time</th>\n",
       "      <th>merch_lat</th>\n",
       "      <th>merch_long</th>\n",
       "      <th>is_fraud</th>\n",
       "      <th>birth_year</th>\n",
       "      <th>birth_month</th>\n",
       "      <th>birth_day</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2703186189652095</td>\n",
       "      <td>fraud_Rippin, Kub and Mann</td>\n",
       "      <td>misc_net</td>\n",
       "      <td>4.97</td>\n",
       "      <td>F</td>\n",
       "      <td>NC</td>\n",
       "      <td>36.0788</td>\n",
       "      <td>-81.1781</td>\n",
       "      <td>3495</td>\n",
       "      <td>Psychologist, counselling</td>\n",
       "      <td>1988-03-09</td>\n",
       "      <td>1325376018</td>\n",
       "      <td>36.011293</td>\n",
       "      <td>-82.048315</td>\n",
       "      <td>0</td>\n",
       "      <td>1988</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>630423337322</td>\n",
       "      <td>fraud_Heller, Gutmann and Zieme</td>\n",
       "      <td>grocery_pos</td>\n",
       "      <td>107.23</td>\n",
       "      <td>F</td>\n",
       "      <td>WA</td>\n",
       "      <td>48.8878</td>\n",
       "      <td>-118.2105</td>\n",
       "      <td>149</td>\n",
       "      <td>Special educational needs teacher</td>\n",
       "      <td>1978-06-21</td>\n",
       "      <td>1325376044</td>\n",
       "      <td>49.159047</td>\n",
       "      <td>-118.186462</td>\n",
       "      <td>0</td>\n",
       "      <td>1978</td>\n",
       "      <td>6</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>38859492057661</td>\n",
       "      <td>fraud_Lind-Buckridge</td>\n",
       "      <td>entertainment</td>\n",
       "      <td>220.11</td>\n",
       "      <td>M</td>\n",
       "      <td>ID</td>\n",
       "      <td>42.1808</td>\n",
       "      <td>-112.2620</td>\n",
       "      <td>4154</td>\n",
       "      <td>Nature conservation officer</td>\n",
       "      <td>1962-01-19</td>\n",
       "      <td>1325376051</td>\n",
       "      <td>43.150704</td>\n",
       "      <td>-112.154481</td>\n",
       "      <td>0</td>\n",
       "      <td>1962</td>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3534093764340240</td>\n",
       "      <td>fraud_Kutch, Hermiston and Farrell</td>\n",
       "      <td>gas_transport</td>\n",
       "      <td>45.00</td>\n",
       "      <td>M</td>\n",
       "      <td>MT</td>\n",
       "      <td>46.2306</td>\n",
       "      <td>-112.1138</td>\n",
       "      <td>1939</td>\n",
       "      <td>Patent attorney</td>\n",
       "      <td>1967-01-12</td>\n",
       "      <td>1325376076</td>\n",
       "      <td>47.034331</td>\n",
       "      <td>-112.561071</td>\n",
       "      <td>0</td>\n",
       "      <td>1967</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>375534208663984</td>\n",
       "      <td>fraud_Keeling-Crist</td>\n",
       "      <td>misc_pos</td>\n",
       "      <td>41.96</td>\n",
       "      <td>M</td>\n",
       "      <td>VA</td>\n",
       "      <td>38.4207</td>\n",
       "      <td>-79.4629</td>\n",
       "      <td>99</td>\n",
       "      <td>Dance movement psychotherapist</td>\n",
       "      <td>1986-03-28</td>\n",
       "      <td>1325376186</td>\n",
       "      <td>38.674999</td>\n",
       "      <td>-78.632459</td>\n",
       "      <td>0</td>\n",
       "      <td>1986</td>\n",
       "      <td>3</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             cc_num                            merchant       category  \\\n",
       "0  2703186189652095          fraud_Rippin, Kub and Mann       misc_net   \n",
       "1      630423337322     fraud_Heller, Gutmann and Zieme    grocery_pos   \n",
       "2    38859492057661                fraud_Lind-Buckridge  entertainment   \n",
       "3  3534093764340240  fraud_Kutch, Hermiston and Farrell  gas_transport   \n",
       "4   375534208663984                 fraud_Keeling-Crist       misc_pos   \n",
       "\n",
       "      amt gender state      lat      long  city_pop  \\\n",
       "0    4.97      F    NC  36.0788  -81.1781      3495   \n",
       "1  107.23      F    WA  48.8878 -118.2105       149   \n",
       "2  220.11      M    ID  42.1808 -112.2620      4154   \n",
       "3   45.00      M    MT  46.2306 -112.1138      1939   \n",
       "4   41.96      M    VA  38.4207  -79.4629        99   \n",
       "\n",
       "                                 job         dob   unix_time  merch_lat  \\\n",
       "0          Psychologist, counselling  1988-03-09  1325376018  36.011293   \n",
       "1  Special educational needs teacher  1978-06-21  1325376044  49.159047   \n",
       "2        Nature conservation officer  1962-01-19  1325376051  43.150704   \n",
       "3                    Patent attorney  1967-01-12  1325376076  47.034331   \n",
       "4     Dance movement psychotherapist  1986-03-28  1325376186  38.674999   \n",
       "\n",
       "   merch_long  is_fraud  birth_year  birth_month  birth_day  \n",
       "0  -82.048315         0        1988            3          9  \n",
       "1 -118.186462         0        1978            6         21  \n",
       "2 -112.154481         0        1962            1         19  \n",
       "3 -112.561071         0        1967            1         12  \n",
       "4  -78.632459         0        1986            3         28  "
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ds.drop(['dob'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting all categorical values to dummy variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cc_num</th>\n",
       "      <th>amt</th>\n",
       "      <th>lat</th>\n",
       "      <th>long</th>\n",
       "      <th>city_pop</th>\n",
       "      <th>unix_time</th>\n",
       "      <th>merch_lat</th>\n",
       "      <th>merch_long</th>\n",
       "      <th>is_fraud</th>\n",
       "      <th>birth_year</th>\n",
       "      <th>...</th>\n",
       "      <th>Video editor</th>\n",
       "      <th>Visual merchandiser</th>\n",
       "      <th>Volunteer coordinator</th>\n",
       "      <th>Warden/ranger</th>\n",
       "      <th>Waste management officer</th>\n",
       "      <th>Water engineer</th>\n",
       "      <th>Water quality scientist</th>\n",
       "      <th>Web designer</th>\n",
       "      <th>Wellsite geologist</th>\n",
       "      <th>Writer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2703186189652095</td>\n",
       "      <td>4.97</td>\n",
       "      <td>36.0788</td>\n",
       "      <td>-81.1781</td>\n",
       "      <td>3495</td>\n",
       "      <td>1325376018</td>\n",
       "      <td>36.011293</td>\n",
       "      <td>-82.048315</td>\n",
       "      <td>0</td>\n",
       "      <td>1988</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>630423337322</td>\n",
       "      <td>107.23</td>\n",
       "      <td>48.8878</td>\n",
       "      <td>-118.2105</td>\n",
       "      <td>149</td>\n",
       "      <td>1325376044</td>\n",
       "      <td>49.159047</td>\n",
       "      <td>-118.186462</td>\n",
       "      <td>0</td>\n",
       "      <td>1978</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>38859492057661</td>\n",
       "      <td>220.11</td>\n",
       "      <td>42.1808</td>\n",
       "      <td>-112.2620</td>\n",
       "      <td>4154</td>\n",
       "      <td>1325376051</td>\n",
       "      <td>43.150704</td>\n",
       "      <td>-112.154481</td>\n",
       "      <td>0</td>\n",
       "      <td>1962</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3534093764340240</td>\n",
       "      <td>45.00</td>\n",
       "      <td>46.2306</td>\n",
       "      <td>-112.1138</td>\n",
       "      <td>1939</td>\n",
       "      <td>1325376076</td>\n",
       "      <td>47.034331</td>\n",
       "      <td>-112.561071</td>\n",
       "      <td>0</td>\n",
       "      <td>1967</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>375534208663984</td>\n",
       "      <td>41.96</td>\n",
       "      <td>38.4207</td>\n",
       "      <td>-79.4629</td>\n",
       "      <td>99</td>\n",
       "      <td>1325376186</td>\n",
       "      <td>38.674999</td>\n",
       "      <td>-78.632459</td>\n",
       "      <td>0</td>\n",
       "      <td>1986</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1250 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             cc_num     amt      lat      long  city_pop   unix_time  \\\n",
       "0  2703186189652095    4.97  36.0788  -81.1781      3495  1325376018   \n",
       "1      630423337322  107.23  48.8878 -118.2105       149  1325376044   \n",
       "2    38859492057661  220.11  42.1808 -112.2620      4154  1325376051   \n",
       "3  3534093764340240   45.00  46.2306 -112.1138      1939  1325376076   \n",
       "4   375534208663984   41.96  38.4207  -79.4629        99  1325376186   \n",
       "\n",
       "   merch_lat  merch_long  is_fraud  birth_year  ...  Video editor  \\\n",
       "0  36.011293  -82.048315         0        1988  ...             0   \n",
       "1  49.159047 -118.186462         0        1978  ...             0   \n",
       "2  43.150704 -112.154481         0        1962  ...             0   \n",
       "3  47.034331 -112.561071         0        1967  ...             0   \n",
       "4  38.674999  -78.632459         0        1986  ...             0   \n",
       "\n",
       "   Visual merchandiser  Volunteer coordinator  Warden/ranger  \\\n",
       "0                    0                      0              0   \n",
       "1                    0                      0              0   \n",
       "2                    0                      0              0   \n",
       "3                    0                      0              0   \n",
       "4                    0                      0              0   \n",
       "\n",
       "   Waste management officer  Water engineer  Water quality scientist  \\\n",
       "0                         0               0                        0   \n",
       "1                         0               0                        0   \n",
       "2                         0               0                        0   \n",
       "3                         0               0                        0   \n",
       "4                         0               0                        0   \n",
       "\n",
       "   Web designer  Wellsite geologist  Writer  \n",
       "0             0                   0       0  \n",
       "1             0                   0       0  \n",
       "2             0                   0       0  \n",
       "3             0                   0       0  \n",
       "4             0                   0       0  \n",
       "\n",
       "[5 rows x 1250 columns]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "for col in ds.columns:\n",
    "    if ds[col].dtypes == object:\n",
    "        dummy = pd.get_dummies(ds[col])\n",
    "        ds = ds.drop(col,axis=1)\n",
    "        ds = pd.concat([ds,dummy], axis=1)\n",
    "\n",
    "ds.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling numerical variables\n",
    "\n",
    "sc=StandardScaler()\n",
    "\n",
    "to_scale=['cc_num','amt','lat','long','city_pop','unix_time','merch_lat','merch_long','birth_year','birth_day','birth_month']\n",
    "\n",
    "for var in to_scale:\n",
    "    ds[var] = sc.fit_transform(ds[var].values.reshape(-1,1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cc_num</th>\n",
       "      <th>amt</th>\n",
       "      <th>lat</th>\n",
       "      <th>long</th>\n",
       "      <th>city_pop</th>\n",
       "      <th>unix_time</th>\n",
       "      <th>merch_lat</th>\n",
       "      <th>merch_long</th>\n",
       "      <th>is_fraud</th>\n",
       "      <th>birth_year</th>\n",
       "      <th>...</th>\n",
       "      <th>Video editor</th>\n",
       "      <th>Visual merchandiser</th>\n",
       "      <th>Volunteer coordinator</th>\n",
       "      <th>Warden/ranger</th>\n",
       "      <th>Waste management officer</th>\n",
       "      <th>Water engineer</th>\n",
       "      <th>Water quality scientist</th>\n",
       "      <th>Web designer</th>\n",
       "      <th>Wellsite geologist</th>\n",
       "      <th>Writer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.317336</td>\n",
       "      <td>-0.458812</td>\n",
       "      <td>-0.484472</td>\n",
       "      <td>0.653627</td>\n",
       "      <td>-0.284149</td>\n",
       "      <td>-1.740782</td>\n",
       "      <td>-0.494753</td>\n",
       "      <td>0.590023</td>\n",
       "      <td>0</td>\n",
       "      <td>0.851265</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.319399</td>\n",
       "      <td>0.242105</td>\n",
       "      <td>2.038121</td>\n",
       "      <td>-2.035361</td>\n",
       "      <td>-0.295244</td>\n",
       "      <td>-1.740764</td>\n",
       "      <td>2.077950</td>\n",
       "      <td>-2.031837</td>\n",
       "      <td>0</td>\n",
       "      <td>0.275315</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.319369</td>\n",
       "      <td>1.015813</td>\n",
       "      <td>0.717250</td>\n",
       "      <td>-1.603430</td>\n",
       "      <td>-0.281963</td>\n",
       "      <td>-1.740759</td>\n",
       "      <td>0.902260</td>\n",
       "      <td>-1.594211</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.646205</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.316701</td>\n",
       "      <td>-0.184436</td>\n",
       "      <td>1.514814</td>\n",
       "      <td>-1.592669</td>\n",
       "      <td>-0.289308</td>\n",
       "      <td>-1.740742</td>\n",
       "      <td>1.662194</td>\n",
       "      <td>-1.623709</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.358230</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.319112</td>\n",
       "      <td>-0.205273</td>\n",
       "      <td>-0.023260</td>\n",
       "      <td>0.778170</td>\n",
       "      <td>-0.295409</td>\n",
       "      <td>-1.740667</td>\n",
       "      <td>0.026471</td>\n",
       "      <td>0.837846</td>\n",
       "      <td>0</td>\n",
       "      <td>0.736075</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1250 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     cc_num       amt       lat      long  city_pop  unix_time  merch_lat  \\\n",
       "0 -0.317336 -0.458812 -0.484472  0.653627 -0.284149  -1.740782  -0.494753   \n",
       "1 -0.319399  0.242105  2.038121 -2.035361 -0.295244  -1.740764   2.077950   \n",
       "2 -0.319369  1.015813  0.717250 -1.603430 -0.281963  -1.740759   0.902260   \n",
       "3 -0.316701 -0.184436  1.514814 -1.592669 -0.289308  -1.740742   1.662194   \n",
       "4 -0.319112 -0.205273 -0.023260  0.778170 -0.295409  -1.740667   0.026471   \n",
       "\n",
       "   merch_long  is_fraud  birth_year  ...  Video editor  Visual merchandiser  \\\n",
       "0    0.590023         0    0.851265  ...             0                    0   \n",
       "1   -2.031837         0    0.275315  ...             0                    0   \n",
       "2   -1.594211         0   -0.646205  ...             0                    0   \n",
       "3   -1.623709         0   -0.358230  ...             0                    0   \n",
       "4    0.837846         0    0.736075  ...             0                    0   \n",
       "\n",
       "   Volunteer coordinator  Warden/ranger  Waste management officer  \\\n",
       "0                      0              0                         0   \n",
       "1                      0              0                         0   \n",
       "2                      0              0                         0   \n",
       "3                      0              0                         0   \n",
       "4                      0              0                         0   \n",
       "\n",
       "   Water engineer  Water quality scientist  Web designer  Wellsite geologist  \\\n",
       "0               0                        0             0                   0   \n",
       "1               0                        0             0                   0   \n",
       "2               0                        0             0                   0   \n",
       "3               0                        0             0                   0   \n",
       "4               0                        0             0                   0   \n",
       "\n",
       "   Writer  \n",
       "0       0  \n",
       "1       0  \n",
       "2       0  \n",
       "3       0  \n",
       "4       0  \n",
       "\n",
       "[5 rows x 1250 columns]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing of Train and Test Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = ds.drop('is_fraud', axis=1)\n",
    "y = ds.is_fraud.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100000, 1249)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.20,random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings\n",
    "\n",
    "We define contants for our trial. \n",
    "- BANDWIDTH refers to the number of transactions presented to the analyst per timestep\n",
    "- MSE_THRESHOLD refers to the MSE value that which the analyst will classify between fraud and normal for the unsupervised module\n",
    "- PROB_THRESHOLD refers to the probability value that which the analyst will classify between fraud and normal for the supervised module\n",
    "- results_dict stores the model performance over n timesteps\n",
    "- n_timesteps refer to how many timesteps to run and build our active model synthesis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "BANDWIDTH = 100\n",
    "MSE_THRESHOLD = 0.03\n",
    "PROB_THRESHOLD = 0.3\n",
    "\n",
    "results_dict = {}\n",
    "n_timesteps = 15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing Unseen Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4000, 1249)\n",
      "Counter({0: 3955, 1: 45})\n",
      "(4180, 1249)\n",
      "Counter({0: 4137, 1: 43})\n",
      "(4310, 1249)\n",
      "Counter({0: 4270, 1: 40})\n",
      "(4389, 1249)\n",
      "Counter({0: 4341, 1: 48})\n",
      "(4419, 1249)\n",
      "Counter({0: 4379, 1: 40})\n",
      "(4403, 1249)\n",
      "Counter({0: 4357, 1: 46})\n",
      "(4344, 1249)\n",
      "Counter({0: 4301, 1: 43})\n",
      "(4247, 1249)\n",
      "Counter({0: 4210, 1: 37})\n",
      "(4114, 1249)\n",
      "Counter({0: 4073, 1: 41})\n",
      "(3952, 1249)\n",
      "Counter({0: 3915, 1: 37})\n",
      "(3765, 1249)\n",
      "Counter({0: 3731, 1: 34})\n",
      "(3558, 1249)\n",
      "Counter({0: 3515, 1: 43})\n",
      "(3336, 1249)\n",
      "Counter({0: 3305, 1: 31})\n",
      "(3104, 1249)\n",
      "Counter({0: 3081, 1: 23})\n",
      "(2866, 1249)\n",
      "Counter({0: 2838, 1: 28})\n"
     ]
    }
   ],
   "source": [
    "unseen_data = []\n",
    "\n",
    "for i in range(n_timesteps):\n",
    "    x_train, x_unseen, y_train, y_unseen = train_test_split(x_train,y_train,test_size=(0.05+i*0.005), random_state=42)\n",
    "    unseen_data.append([x_train, x_unseen, y_train, y_unseen])\n",
    "    print(x_unseen.shape)\n",
    "    print(Counter(y_unseen))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Autoencoder\n",
    "\n",
    "# Function: get_normal_attack(x_train, y_train)\n",
    "# Input: n*m matrix, n*1 array\n",
    "# Return output: matrix of non-fraud training data and a matrix of fraud training data\n",
    "# Function Description: \n",
    "#   - Splits training data into fraud and non-fraud\n",
    "#   - prints fraud and non-fraud counts\n",
    "\n",
    "def get_normal_attack(x_train, y_train):\n",
    "    train = x_train\n",
    "    train['Class'] = y_train\n",
    "    train\n",
    "\n",
    "    normal_mask = train['Class']==0\n",
    "    attack_mask = train['Class']==1\n",
    "\n",
    "    train.drop('Class',axis=1,inplace=True)\n",
    "\n",
    "    train_normal = train[normal_mask]\n",
    "    train_attack = train[attack_mask]\n",
    "\n",
    "    print(f\"Normal count: {len(train_normal)}\")\n",
    "    print(f\"Attack count: {len(train_attack)}\")\n",
    "\n",
    "    x_normal_train = train_normal.values\n",
    "    x_attack_train = train_attack.values\n",
    "\n",
    "    return  x_normal_train, x_attack_train\n",
    "\n",
    "# Function: build_autoencoder(x, y)\n",
    "# Input: n*m matrix, n*1 array\n",
    "# Return output: autoencoder model and model history\n",
    "# Function Description: \n",
    "#   - builds and compiles a replicator neural network with dense layers\n",
    "#   - fits the model to input X and y data\n",
    "\n",
    "\n",
    "def build_autoencoder(x, y):\n",
    "    \n",
    "    # define our early stopping\n",
    "    early_stop = tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        min_delta=0.0001,\n",
    "        patience=8,\n",
    "        verbose=1, \n",
    "        mode='min',\n",
    "        restore_best_weights=True\n",
    "    )\n",
    "\n",
    "    tf.keras.backend.set_floatx('float64')\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(layers.Dense(32, activation='tanh'))\n",
    "    model.add(layers.Dense(16, activation='tanh'))\n",
    "    model.add(layers.Dense(4, activation='tanh')) \n",
    "    model.add(layers.Dense(16, activation='tanh'))\n",
    "    model.add(layers.Dense(32, activation='tanh'))\n",
    "    model.add(layers.Dense(x.shape[1])) \n",
    "    model.compile(loss='mean_squared_error', optimizer='nadam')\n",
    "    history = model.fit(x,\n",
    "                        x,\n",
    "                        epochs=32,\n",
    "                        batch_size=50,\n",
    "                        validation_split=0.1,\n",
    "                        callbacks=[early_stop]\n",
    "                    )\n",
    "\n",
    "    return model, history\n",
    "\n",
    "\n",
    "# Function: map_mse_to_res(x, threshold)\n",
    "# Input: float, float\n",
    "# Return output: 1 or 0 integer label\n",
    "# Function Description: \n",
    "#   - maps an mse value to 1 or 0 depending on threshold\n",
    "\n",
    "def map_mse_to_res(x, threshold):\n",
    "    if x > threshold:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "\n",
    "# Function: get_unseen_class_unsup(model, x_unseen, MSE_THRESHOLD)\n",
    "# Input: autoencoder model, unseen n*m matrix, threshold value\n",
    "# Return output: dataframe of ranked entries based on mse values and their corresponding mapped 1 or 0 label\n",
    "# Function Description: \n",
    "#   - predicts the unseen data using the autoencoder model\n",
    "#   - calculates reconstruction error as MSE between input and output data.\n",
    "#   - ranks the data in terms of mse, and slices to size depending on analyst daily bandwidth value \n",
    "#   - maps the MSE to labels, simulating how a analyst will label the data.\n",
    "\n",
    "\n",
    "def get_unseen_class_unsup(model, x_unseen, MSE_THRESHOLD):\n",
    "    unseen_proba = model.predict(x_unseen)\n",
    "    mse = np.mean(np.power(unseen_proba - x_unseen, 2), axis=1)\n",
    "    mse_table = pd.DataFrame([x_unseen.index,mse]).T\n",
    "    mse_table = mse_table.rename(columns={0:'index_', 1:'mse'})\n",
    "    mse_table = mse_table.sort_values(by='mse', ascending=False)[:int(BANDWIDTH/2)]\n",
    "    mse_table['Class'] = mse_table['mse'].apply(lambda x: map_mse_to_res(x, MSE_THRESHOLD))\n",
    "    return mse_table\n",
    "\n",
    "# Supervised Models\n",
    "\n",
    "# Function: ens_get_bceloss(pred,true)\n",
    "# Input: n*1 array, n*1 array\n",
    "# Return output: float of cross-entropy loss value\n",
    "# Function Description: \n",
    "#   - calculates binary crossentropy loss value given predicted and true values \n",
    "\n",
    "def ens_get_bceloss(pred,true):\n",
    "    y_true = backend.variable(true)\n",
    "    y_pred = backend.variable(pred)\n",
    "    mean_ce = backend.eval(binary_crossentropy(y_true, y_pred))\n",
    "    return mean_ce\n",
    "\n",
    "# Function: ens_get_auc(y_pred_proba, y)\n",
    "# Input: n*1 array, n*1 array\n",
    "# Return output: float of auc value\n",
    "# Function Description: \n",
    "#   - returns auc value given predicted class probabilities and true class \n",
    "\n",
    "def ens_get_auc(y_pred_proba, y):\n",
    "    [fpr, tpr, thr] = roc_curve(y, y_pred_proba)\n",
    "    return auc(fpr, tpr)\n",
    "\n",
    "# Function: get_results_dict(model, x_test, y_test)\n",
    "# Input: model, n*m matrix, n*1 array\n",
    "# Return output: dictionary of scores\n",
    "# Function Description: \n",
    "#   - predicts x_test class labels\n",
    "#   - calculates loss, accuracy, auc, recall and precision\n",
    "\n",
    "def get_results_dict(model, x_test, y_test):\n",
    "    y_pred = model.predict(x_test)\n",
    "    y_pred_proba = model.predict_proba(x_test)[:,1]\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    score_dict = {\n",
    "        \"loss\": ens_get_bceloss(y_pred_proba, y_test),\n",
    "        \"accuracy\": accuracy_score(y_test, y_pred),\n",
    "        \"auc\": ens_get_auc(y_pred_proba, y_test),\n",
    "        \"recall\": recall,\n",
    "        \"precision\": precision,\n",
    "        \"F1_score\": f1_score(y_test,y_pred),\n",
    "        \"F2_score\": (5 * precision * recall) / (4 * precision + recall)\n",
    "    }\n",
    "    return score_dict\n",
    "\n",
    "# Function: rfc_eval(x_train,y_train,x_test,y_test)\n",
    "# Input: n*m matrix, n*1 array, n*m matrix, n*1 array\n",
    "# Return output: RandomForest Model, results dictionary\n",
    "# Function Description: \n",
    "#   - builds and fits a RandomForest model\n",
    "#   - evaluate the model on test data\n",
    "\n",
    "def rfc_eval(x_train,y_train,x_test,y_test):\n",
    "    rfc = RandomForestClassifier(n_estimators=150, random_state=42)\n",
    "    rfc.fit(x_train,y_train)\n",
    "    res = get_results_dict(rfc, x_test, y_test)\n",
    "    return rfc, res\n",
    "\n",
    "# Function: map_to_res(x, threshold)\n",
    "# Input: float probabilities, float threshold\n",
    "# Return output: integer class labels\n",
    "# Function Description: \n",
    "#   - Maps a prediction probability x to a class label based on threshold\n",
    "\n",
    "def map_to_res(x, threshold):\n",
    "    if x > threshold:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "# Function: get_unseen_class(model, x_unseen, THRESHOLD)\n",
    "# Input: RandomForest model, n*m matrix, threshold value\n",
    "# Return output: dataframe \n",
    "# Function Description: \n",
    "#   - predicts the unseen data using the input model\n",
    "#   - ranks the data in terms of prediction probabilities, and slices to size depending on analyst daily bandwidth value \n",
    "#   - maps the prediction probabiltiies to labels, simulating how a analyst will label the data.\n",
    "\n",
    "def get_unseen_class(model, x_unseen, THRESHOLD):\n",
    "    pred_proba_unseen = model.predict_proba(x_unseen)\n",
    "    unseen_proba = pd.concat([pd.DataFrame(pred_proba_unseen, columns=['pred_0', 'pred_1']), pd.DataFrame(x_unseen.index, columns=['index_'])], axis=1)\n",
    "    unseen_proba = unseen_proba.sort_values(by='pred_1', ascending=False)[:int(BANDWIDTH/2)]\n",
    "    unseen_proba['Class'] = unseen_proba['pred_1'].apply(lambda x: map_to_res(x, THRESHOLD)) \n",
    "    return unseen_proba\n",
    "\n",
    "\n",
    "# Combining Data\n",
    "\n",
    "# Function: get_new_train(unseen_class_sup, unseen_class_unsup, x_unseen, x_train, y_train)\n",
    "# Input: Ranked Unseen dataframe from supervised model, Ranked Unseen dataframe from unsupervised model, n*m matrix of unseen data, historical data n*m matrix, historical label \n",
    "# Return output: dataframe of new train data and array of new labels\n",
    "# Function Description: \n",
    "#   - combines feedbacked data and its corresponding labels to historical data.\n",
    "\n",
    "def get_new_train(unseen_class_sup, unseen_class_unsup, x_unseen, x_train, y_train):\n",
    "    x_labeled_new_sup = x_unseen.loc[unseen_class_sup.index_]\n",
    "    y_labeled_new_sup = unseen_class_sup.Class.values\n",
    "    x_train_temp =  x_train.append(x_labeled_new_sup)\n",
    "    y_train_temp = np.concatenate((y_train,y_labeled_new_sup), axis=0)\n",
    "    x_labeled_new_unsup = x_unseen.loc[unseen_class_unsup.index_]\n",
    "    y_labeled_new_unsup = unseen_class_unsup.Class.values\n",
    "    x_train_new =  x_train_temp.append(x_labeled_new_unsup)\n",
    "    y_train_new = np.concatenate((y_train_temp,y_labeled_new_unsup), axis=0)\n",
    "    return x_train_new, y_train_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Timestep 0: Before introduction of new data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Unsupervised Model for next timestep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normal count: 20806\n",
      "Attack count: 207\n"
     ]
    }
   ],
   "source": [
    "x_normal_train_0, x_attack_train_0 = get_normal_attack(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2839"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# garbace collection\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/32\n",
      "375/375 [==============================] - 5s 12ms/step - loss: 0.0082 - val_loss: 0.0070\n",
      "Epoch 2/32\n",
      "375/375 [==============================] - 4s 11ms/step - loss: 0.0071 - val_loss: 0.0069\n",
      "Epoch 3/32\n",
      "375/375 [==============================] - 4s 11ms/step - loss: 0.0071 - val_loss: 0.0069\n",
      "Epoch 4/32\n",
      "375/375 [==============================] - 4s 11ms/step - loss: 0.0070 - val_loss: 0.0068\n",
      "Epoch 5/32\n",
      "375/375 [==============================] - 4s 11ms/step - loss: 0.0069 - val_loss: 0.0066\n",
      "Epoch 6/32\n",
      "375/375 [==============================] - 4s 11ms/step - loss: 0.0066 - val_loss: 0.0065\n",
      "Epoch 7/32\n",
      "375/375 [==============================] - 4s 11ms/step - loss: 0.0065 - val_loss: 0.0064\n",
      "Epoch 8/32\n",
      "375/375 [==============================] - 5s 12ms/step - loss: 0.0065 - val_loss: 0.0063\n",
      "Epoch 9/32\n",
      "375/375 [==============================] - 4s 12ms/step - loss: 0.0064 - val_loss: 0.0063\n",
      "Epoch 10/32\n",
      "375/375 [==============================] - 4s 11ms/step - loss: 0.0064 - val_loss: 0.0062\n",
      "Epoch 11/32\n",
      "375/375 [==============================] - 4s 11ms/step - loss: 0.0062 - val_loss: 0.0061\n",
      "Epoch 12/32\n",
      "375/375 [==============================] - 4s 12ms/step - loss: 0.0061 - val_loss: 0.0060\n",
      "Epoch 13/32\n",
      "375/375 [==============================] - 4s 12ms/step - loss: 0.0060 - val_loss: 0.0059\n",
      "Epoch 14/32\n",
      "375/375 [==============================] - 4s 11ms/step - loss: 0.0059 - val_loss: 0.0058\n",
      "Epoch 15/32\n",
      "375/375 [==============================] - 5s 12ms/step - loss: 0.0059 - val_loss: 0.0058\n",
      "Epoch 16/32\n",
      "375/375 [==============================] - 4s 11ms/step - loss: 0.0058 - val_loss: 0.0057\n",
      "Epoch 17/32\n",
      "375/375 [==============================] - 4s 11ms/step - loss: 0.0058 - val_loss: 0.0057\n",
      "Epoch 18/32\n",
      "375/375 [==============================] - 4s 11ms/step - loss: 0.0057 - val_loss: 0.0057\n",
      "Epoch 19/32\n",
      "375/375 [==============================] - 5s 13ms/step - loss: 0.0057 - val_loss: 0.0056\n",
      "Epoch 20/32\n",
      "375/375 [==============================] - 5s 12ms/step - loss: 0.0057 - val_loss: 0.0056\n",
      "Epoch 21/32\n",
      "375/375 [==============================] - 5s 12ms/step - loss: 0.0056 - val_loss: 0.0056\n",
      "Epoch 22/32\n",
      "375/375 [==============================] - 5s 12ms/step - loss: 0.0056 - val_loss: 0.0056\n",
      "Epoch 23/32\n",
      "375/375 [==============================] - 5s 12ms/step - loss: 0.0056 - val_loss: 0.0055\n",
      "Epoch 24/32\n",
      "375/375 [==============================] - 5s 12ms/step - loss: 0.0055 - val_loss: 0.0054\n",
      "Epoch 25/32\n",
      "375/375 [==============================] - 4s 12ms/step - loss: 0.0054 - val_loss: 0.0053\n",
      "Epoch 26/32\n",
      "375/375 [==============================] - 4s 12ms/step - loss: 0.0053 - val_loss: 0.0052\n",
      "Epoch 27/32\n",
      "375/375 [==============================] - 5s 13ms/step - loss: 0.0052 - val_loss: 0.0051\n",
      "Epoch 28/32\n",
      "375/375 [==============================] - 5s 13ms/step - loss: 0.0051 - val_loss: 0.0050\n",
      "Epoch 29/32\n",
      "375/375 [==============================] - 4s 12ms/step - loss: 0.0051 - val_loss: 0.0050\n",
      "Epoch 30/32\n",
      "375/375 [==============================] - 5s 12ms/step - loss: 0.0050 - val_loss: 0.0049\n",
      "Epoch 31/32\n",
      "375/375 [==============================] - 5s 12ms/step - loss: 0.0049 - val_loss: 0.0049\n",
      "Epoch 32/32\n",
      "375/375 [==============================] - 5s 12ms/step - loss: 0.0049 - val_loss: 0.0048\n"
     ]
    }
   ],
   "source": [
    "# Building initial unsupervised model based on historical labelled data\n",
    "\n",
    "model0, history0 = build_autoencoder(x_normal_train_0, x_attack_train_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestep 0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>F1_score</th>\n",
       "      <td>0.354839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F2_score</th>\n",
       "      <td>0.255814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.992000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>auc</th>\n",
       "      <td>0.973176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>loss</th>\n",
       "      <td>0.023700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>precision</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recall</th>\n",
       "      <td>0.215686</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           timestep 0\n",
       "F1_score     0.354839\n",
       "F2_score     0.255814\n",
       "accuracy     0.992000\n",
       "auc          0.973176\n",
       "loss         0.023700\n",
       "precision    1.000000\n",
       "recall       0.215686"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Building supervised model based on historical data\n",
    "\n",
    "rfc0, time_0_res = rfc_eval(x_train,y_train,x_test,y_test)\n",
    "results_dict['timestep 0'] = time_0_res\n",
    "\n",
    "# Results at timestep 0. This is our baseline results for RandomForest.\n",
    "\n",
    "pd.DataFrame(results_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# storing models for training across timesteps\n",
    "\n",
    "supervised_models = []\n",
    "unsupervised_models = []\n",
    "supervised_models.append(rfc0)\n",
    "unsupervised_models.append(model0)\n",
    "\n",
    "x_train_ = x_train\n",
    "y_train_ = y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Executing a simplified AI2 model algorithm for 15 timesteps. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " >>> TIMESTEP 1 =============================================================== \n",
      "\n",
      "       index_       mse  Class\n",
      "962    1784.0  0.372256      1\n",
      "2023   5627.0  0.275129      1\n",
      "2108  71305.0  0.063494      1\n",
      "927   64584.0  0.060540      1\n",
      "175   21334.0  0.060225      1\n",
      "3830  19576.0  0.055935      1\n",
      "101   88592.0  0.048764      1\n",
      "2938  76695.0  0.048007      1\n",
      "377   97938.0  0.047828      1\n",
      "1689  95593.0  0.044511      1\n",
      "3439  74502.0  0.042507      1\n",
      "432   33642.0  0.041871      1\n",
      "69    73601.0  0.039711      1\n",
      "679   78026.0  0.038823      1\n",
      "1615  76595.0  0.038623      1\n",
      "1445  16970.0  0.037187      1\n",
      "431   74054.0  0.036003      1\n",
      "748   37420.0  0.035290      1\n",
      "2249  33472.0  0.034880      1\n",
      "3998  85974.0  0.033983      1\n",
      "21    98055.0  0.033722      1\n",
      "2063   3722.0  0.033367      1\n",
      "2523  64542.0  0.032408      1\n",
      "1912  27919.0  0.031828      1\n",
      "2596  27616.0  0.031430      1\n",
      "1965  28885.0  0.030644      1\n",
      "3267   7880.0  0.029442      0\n",
      "1534  79391.0  0.028439      0\n",
      "392   74145.0  0.028169      0\n",
      "222   26350.0  0.028137      0\n",
      "3056  59591.0  0.027060      0\n",
      "2777  26660.0  0.026557      0\n",
      "184    4706.0  0.025510      0\n",
      "3666   5577.0  0.024911      0\n",
      "3959  86056.0  0.024815      0\n",
      "629   19735.0  0.020821      0\n",
      "1309  90776.0  0.019141      0\n",
      "407   46259.0  0.018266      0\n",
      "461   35072.0  0.018049      0\n",
      "221    5119.0  0.017277      0\n",
      "1025  37191.0  0.017103      0\n",
      "2627  91999.0  0.017094      0\n",
      "3916  39342.0  0.015973      0\n",
      "1028   7000.0  0.015454      0\n",
      "1151  12538.0  0.015354      0\n",
      "904   97163.0  0.014951      0\n",
      "2400  53066.0  0.014368      0\n",
      "2279   4898.0  0.014248      0\n",
      "3242  38000.0  0.014149      0\n",
      "1315  32578.0  0.014090      0\n",
      "        pred_0    pred_1  index_  Class\n",
      "927   0.340000  0.660000   64584      1\n",
      "2596  0.346667  0.653333   27616      1\n",
      "2142  0.380000  0.620000   12109      1\n",
      "431   0.386667  0.613333   74054      1\n",
      "974   0.393333  0.606667   48191      1\n",
      "3687  0.413333  0.586667   95974      1\n",
      "432   0.426667  0.573333   33642      1\n",
      "1689  0.440000  0.560000   95593      1\n",
      "392   0.460000  0.540000   74145      1\n",
      "3666  0.473333  0.526667    5577      1\n",
      "222   0.473333  0.526667   26350      1\n",
      "3488  0.480000  0.520000   52625      1\n",
      "1965  0.540000  0.460000   28885      1\n",
      "101   0.580000  0.420000   88592      1\n",
      "3439  0.600000  0.400000   74502      1\n",
      "3707  0.600000  0.400000   64606      1\n",
      "3713  0.600000  0.400000   63458      1\n",
      "1786  0.626667  0.373333   95778      1\n",
      "2938  0.640000  0.360000   76695      1\n",
      "1912  0.640000  0.360000   27919      1\n",
      "1615  0.646667  0.353333   76595      1\n",
      "1445  0.646667  0.353333   16970      1\n",
      "3263  0.653333  0.346667   70507      1\n",
      "2992  0.660000  0.340000   16920      1\n",
      "1658  0.666667  0.333333   30339      1\n",
      "377   0.666667  0.333333   97938      1\n",
      "1534  0.666667  0.333333   79391      1\n",
      "1499  0.666667  0.333333   93957      1\n",
      "679   0.673333  0.326667   78026      1\n",
      "2659  0.680000  0.320000   76909      1\n",
      "2523  0.686667  0.313333   64542      1\n",
      "3998  0.700000  0.300000   85974      0\n",
      "748   0.713333  0.286667   37420      0\n",
      "69    0.713333  0.286667   73601      0\n",
      "629   0.726667  0.273333   19735      0\n",
      "21    0.733333  0.266667   98055      0\n",
      "2077  0.746667  0.253333   18602      0\n",
      "3267  0.746667  0.253333    7880      0\n",
      "3056  0.753333  0.246667   59591      0\n",
      "3959  0.753333  0.246667   86056      0\n",
      "184   0.753333  0.246667    4706      0\n",
      "2070  0.773333  0.226667   88469      0\n",
      "2917  0.773333  0.226667   87066      0\n",
      "962   0.773333  0.226667    1784      0\n",
      "494   0.793333  0.206667   58610      0\n",
      "2063  0.793333  0.206667    3722      0\n",
      "2249  0.793333  0.206667   33472      0\n",
      "2777  0.800000  0.200000   26660      0\n",
      "3830  0.806667  0.193333   19576      0\n",
      "175   0.806667  0.193333   21334      0\n",
      "Normal count: 20849\n",
      "Attack count: 264\n",
      "Epoch 1/32\n",
      "376/376 [==============================] - 6s 13ms/step - loss: 0.0083 - val_loss: 0.0076\n",
      "Epoch 2/32\n",
      "376/376 [==============================] - 5s 13ms/step - loss: 0.0071 - val_loss: 0.0074\n",
      "Epoch 3/32\n",
      "376/376 [==============================] - 5s 13ms/step - loss: 0.0070 - val_loss: 0.0073\n",
      "Epoch 4/32\n",
      "376/376 [==============================] - 5s 12ms/step - loss: 0.0069 - val_loss: 0.0071\n",
      "Epoch 5/32\n",
      "376/376 [==============================] - 5s 13ms/step - loss: 0.0064 - val_loss: 0.0066\n",
      "Epoch 6/32\n",
      "376/376 [==============================] - 5s 14ms/step - loss: 0.0060 - val_loss: 0.0064\n",
      "Epoch 7/32\n",
      "376/376 [==============================] - 5s 12ms/step - loss: 0.0058 - val_loss: 0.0063\n",
      "Epoch 8/32\n",
      "376/376 [==============================] - 5s 12ms/step - loss: 0.0058 - val_loss: 0.0063\n",
      "Epoch 9/32\n",
      "376/376 [==============================] - 4s 12ms/step - loss: 0.0057 - val_loss: 0.0063\n",
      "Epoch 10/32\n",
      "376/376 [==============================] - 5s 12ms/step - loss: 0.0057 - val_loss: 0.0062\n",
      "Epoch 11/32\n",
      "376/376 [==============================] - 4s 12ms/step - loss: 0.0057 - val_loss: 0.0061\n",
      "Epoch 12/32\n",
      "376/376 [==============================] - 5s 14ms/step - loss: 0.0056 - val_loss: 0.0061\n",
      "Epoch 13/32\n",
      "376/376 [==============================] - 5s 13ms/step - loss: 0.0056 - val_loss: 0.0060\n",
      "Epoch 14/32\n",
      "376/376 [==============================] - 5s 12ms/step - loss: 0.0055 - val_loss: 0.0059\n",
      "Epoch 15/32\n",
      "376/376 [==============================] - 5s 13ms/step - loss: 0.0054 - val_loss: 0.0058\n",
      "Epoch 16/32\n",
      "376/376 [==============================] - 5s 12ms/step - loss: 0.0053 - val_loss: 0.0058\n",
      "Epoch 17/32\n",
      "376/376 [==============================] - 5s 12ms/step - loss: 0.0053 - val_loss: 0.0057\n",
      "Epoch 18/32\n",
      "376/376 [==============================] - 5s 12ms/step - loss: 0.0052 - val_loss: 0.0057\n",
      "Epoch 19/32\n",
      "376/376 [==============================] - 5s 13ms/step - loss: 0.0052 - val_loss: 0.0056\n",
      "Epoch 20/32\n",
      "376/376 [==============================] - 5s 13ms/step - loss: 0.0052 - val_loss: 0.0056\n",
      "Epoch 21/32\n",
      "376/376 [==============================] - 5s 13ms/step - loss: 0.0051 - val_loss: 0.0056\n",
      "Epoch 22/32\n",
      "376/376 [==============================] - 5s 12ms/step - loss: 0.0051 - val_loss: 0.0056\n",
      "Epoch 23/32\n",
      "376/376 [==============================] - 5s 13ms/step - loss: 0.0051 - val_loss: 0.0056\n",
      "Epoch 24/32\n",
      "376/376 [==============================] - 5s 12ms/step - loss: 0.0051 - val_loss: 0.0056\n",
      "Epoch 25/32\n",
      "376/376 [==============================] - 5s 12ms/step - loss: 0.0051 - val_loss: 0.0055\n",
      "Epoch 26/32\n",
      "376/376 [==============================] - 5s 14ms/step - loss: 0.0050 - val_loss: 0.0055\n",
      "Epoch 27/32\n",
      "376/376 [==============================] - 5s 14ms/step - loss: 0.0050 - val_loss: 0.0055\n",
      "Epoch 28/32\n",
      "376/376 [==============================] - 5s 13ms/step - loss: 0.0050 - val_loss: 0.0054\n",
      "Epoch 29/32\n",
      "376/376 [==============================] - 5s 14ms/step - loss: 0.0049 - val_loss: 0.0054\n",
      "Epoch 30/32\n",
      "376/376 [==============================] - 6s 17ms/step - loss: 0.0048 - val_loss: 0.0053\n",
      "Epoch 31/32\n",
      "376/376 [==============================] - 6s 16ms/step - loss: 0.0048 - val_loss: 0.0052\n",
      "Epoch 32/32\n",
      "376/376 [==============================] - 5s 13ms/step - loss: 0.0047 - val_loss: 0.0052\n",
      "\n",
      " >>> TIMESTEP 2 =============================================================== \n",
      "\n",
      "       index_       mse  Class\n",
      "221   62672.0  0.129528      1\n",
      "2454  74315.0  0.099163      1\n",
      "3195  27807.0  0.085892      1\n",
      "2155  20721.0  0.077903      1\n",
      "4058  64882.0  0.069136      1\n",
      "3665  85133.0  0.054023      1\n",
      "664   38649.0  0.053320      1\n",
      "2468  74271.0  0.052298      1\n",
      "2843  95564.0  0.050399      1\n",
      "2297  23951.0  0.049940      1\n",
      "2178  12329.0  0.045963      1\n",
      "1999    723.0  0.040990      1\n",
      "4033  64425.0  0.039739      1\n",
      "2543  50239.0  0.039211      1\n",
      "1232  67175.0  0.039188      1\n",
      "4162  18260.0  0.039159      1\n",
      "194   90745.0  0.038135      1\n",
      "1279  28836.0  0.038130      1\n",
      "4083  19780.0  0.034043      1\n",
      "2837  31614.0  0.032849      1\n",
      "1558  59222.0  0.031673      1\n",
      "2517  74055.0  0.031273      1\n",
      "2733  66094.0  0.031073      1\n",
      "1823  27635.0  0.030469      1\n",
      "408   81439.0  0.029555      0\n",
      "3583  36302.0  0.029269      0\n",
      "341   74891.0  0.029216      0\n",
      "3251  29087.0  0.027522      0\n",
      "4082  25779.0  0.027227      0\n",
      "837    7559.0  0.027103      0\n",
      "2383  86613.0  0.025772      0\n",
      "1665  51290.0  0.022731      0\n",
      "2699  24842.0  0.022433      0\n",
      "2314  81945.0  0.022417      0\n",
      "2219  99144.0  0.022092      0\n",
      "836   18724.0  0.021225      0\n",
      "2102   7350.0  0.019695      0\n",
      "1541  84423.0  0.019256      0\n",
      "2634  52888.0  0.018789      0\n",
      "2294  69697.0  0.018778      0\n",
      "723     138.0  0.018609      0\n",
      "1619  27268.0  0.016728      0\n",
      "372   27880.0  0.016078      0\n",
      "1767  14433.0  0.015679      0\n",
      "1610  11919.0  0.015150      0\n",
      "1557  98798.0  0.014746      0\n",
      "3594  49316.0  0.013984      0\n",
      "2758  70729.0  0.013904      0\n",
      "2410  85977.0  0.013746      0\n",
      "2854  67800.0  0.013634      0\n",
      "        pred_0    pred_1  index_  Class\n",
      "1823  0.278000  0.722000   27635      1\n",
      "2602  0.393333  0.606667   28000      1\n",
      "194   0.398889  0.601111   90745      1\n",
      "408   0.401667  0.598333   81439      1\n",
      "1279  0.453333  0.546667   28836      1\n",
      "3134  0.460000  0.540000   91937      1\n",
      "1844  0.466667  0.533333   21630      1\n",
      "1357  0.466667  0.533333   27722      1\n",
      "2178  0.471778  0.528222   12329      1\n",
      "2843  0.497778  0.502222   95564      1\n",
      "4162  0.506667  0.493333   18260      1\n",
      "2837  0.515000  0.485000   31614      1\n",
      "4033  0.530000  0.470000   64425      1\n",
      "2699  0.530000  0.470000   24842      1\n",
      "2733  0.534444  0.465556   66094      1\n",
      "341   0.541667  0.458333   74891      1\n",
      "1232  0.553333  0.446667   67175      1\n",
      "2543  0.562222  0.437778   50239      1\n",
      "2468  0.567222  0.432778   74271      1\n",
      "2297  0.570000  0.430000   23951      1\n",
      "3665  0.571111  0.428889   85133      1\n",
      "3583  0.593333  0.406667   36302      1\n",
      "664   0.594444  0.405556   38649      1\n",
      "2517  0.618333  0.381667   74055      1\n",
      "2854  0.673333  0.326667   67800      1\n",
      "3195  0.681667  0.318333   27807      1\n",
      "3251  0.687651  0.312349   29087      1\n",
      "2219  0.692302  0.307698   99144      1\n",
      "4083  0.698778  0.301222   19780      1\n",
      "2383  0.703333  0.296667   86613      0\n",
      "1558  0.713333  0.286667   59222      0\n",
      "4082  0.716414  0.283586   25779      0\n",
      "837   0.717111  0.282889    7559      0\n",
      "1560  0.733333  0.266667   21812      0\n",
      "2155  0.734222  0.265778   20721      0\n",
      "3913  0.750000  0.250000   95915      0\n",
      "1999  0.756111  0.243889     723      0\n",
      "221   0.757667  0.242333   62672      0\n",
      "836   0.764444  0.235556   18724      0\n",
      "4058  0.786667  0.213333   64882      0\n",
      "2454  0.787222  0.212778   74315      0\n",
      "2314  0.791667  0.208333   81945      0\n",
      "2000  0.793333  0.206667   70175      0\n",
      "1767  0.800000  0.200000   14433      0\n",
      "775   0.806667  0.193333   33962      0\n",
      "1551  0.806667  0.193333   25379      0\n",
      "1335  0.813333  0.186667   61827      0\n",
      "870   0.813333  0.186667   64643      0\n",
      "1665  0.830000  0.170000   51290      0\n",
      "1737  0.846667  0.153333   79706      0\n",
      "Normal count: 20896\n",
      "Attack count: 317\n",
      "Epoch 1/32\n",
      "377/377 [==============================] - 6s 13ms/step - loss: 0.0084 - val_loss: 0.0083\n",
      "Epoch 2/32\n",
      "377/377 [==============================] - 4s 11ms/step - loss: 0.0072 - val_loss: 0.0081\n",
      "Epoch 3/32\n",
      "377/377 [==============================] - 4s 11ms/step - loss: 0.0071 - val_loss: 0.0080\n",
      "Epoch 4/32\n",
      "377/377 [==============================] - 4s 12ms/step - loss: 0.0070 - val_loss: 0.0079\n",
      "Epoch 5/32\n",
      "377/377 [==============================] - 4s 12ms/step - loss: 0.0068 - val_loss: 0.0077\n",
      "Epoch 6/32\n",
      "377/377 [==============================] - 4s 11ms/step - loss: 0.0066 - val_loss: 0.0075\n",
      "Epoch 7/32\n",
      "377/377 [==============================] - 4s 11ms/step - loss: 0.0065 - val_loss: 0.0075\n",
      "Epoch 8/32\n",
      "377/377 [==============================] - 4s 11ms/step - loss: 0.0065 - val_loss: 0.0075\n",
      "Epoch 9/32\n",
      "377/377 [==============================] - 4s 12ms/step - loss: 0.0064 - val_loss: 0.0074\n",
      "Epoch 10/32\n",
      "377/377 [==============================] - 4s 11ms/step - loss: 0.0064 - val_loss: 0.0074\n",
      "Epoch 11/32\n",
      "377/377 [==============================] - 5s 14ms/step - loss: 0.0064 - val_loss: 0.0074\n",
      "Epoch 12/32\n",
      "377/377 [==============================] - 8s 22ms/step - loss: 0.0064 - val_loss: 0.0074\n",
      "Epoch 13/32\n",
      "377/377 [==============================] - 6s 15ms/step - loss: 0.0064 - val_loss: 0.0073\n",
      "Epoch 14/32\n",
      "377/377 [==============================] - 5s 14ms/step - loss: 0.0064 - val_loss: 0.0073\n",
      "Epoch 15/32\n",
      "377/377 [==============================] - 5s 14ms/step - loss: 0.0063 - val_loss: 0.0073\n",
      "Epoch 16/32\n",
      "377/377 [==============================] - 6s 16ms/step - loss: 0.0060 - val_loss: 0.0068\n",
      "Epoch 17/32\n",
      "377/377 [==============================] - 6s 16ms/step - loss: 0.0057 - val_loss: 0.0067\n",
      "Epoch 18/32\n",
      "377/377 [==============================] - 6s 17ms/step - loss: 0.0057 - val_loss: 0.0067\n",
      "Epoch 19/32\n",
      "377/377 [==============================] - 6s 16ms/step - loss: 0.0056 - val_loss: 0.0066\n",
      "Epoch 20/32\n",
      "377/377 [==============================] - 6s 15ms/step - loss: 0.0056 - val_loss: 0.0066\n",
      "Epoch 21/32\n",
      "377/377 [==============================] - 5s 12ms/step - loss: 0.0056 - val_loss: 0.0066\n",
      "Epoch 22/32\n",
      "377/377 [==============================] - 5s 12ms/step - loss: 0.0056 - val_loss: 0.0066\n",
      "Epoch 23/32\n",
      "377/377 [==============================] - 5s 13ms/step - loss: 0.0056 - val_loss: 0.0065\n",
      "Epoch 24/32\n",
      "377/377 [==============================] - 5s 14ms/step - loss: 0.0055 - val_loss: 0.0066\n",
      "Epoch 25/32\n",
      "377/377 [==============================] - 5s 12ms/step - loss: 0.0055 - val_loss: 0.0065\n",
      "Epoch 26/32\n",
      "377/377 [==============================] - 5s 13ms/step - loss: 0.0055 - val_loss: 0.0065\n",
      "Epoch 27/32\n",
      "377/377 [==============================] - 5s 14ms/step - loss: 0.0055 - val_loss: 0.0065\n",
      "Epoch 28/32\n",
      "377/377 [==============================] - 5s 13ms/step - loss: 0.0054 - val_loss: 0.0064\n",
      "Epoch 29/32\n",
      "377/377 [==============================] - 5s 14ms/step - loss: 0.0053 - val_loss: 0.0063\n",
      "Epoch 30/32\n",
      "377/377 [==============================] - 6s 15ms/step - loss: 0.0052 - val_loss: 0.0062\n",
      "Epoch 31/32\n",
      "377/377 [==============================] - 5s 13ms/step - loss: 0.0052 - val_loss: 0.0061\n",
      "Epoch 32/32\n",
      "377/377 [==============================] - 5s 13ms/step - loss: 0.0051 - val_loss: 0.0061\n",
      "\n",
      " >>> TIMESTEP 3 =============================================================== \n",
      "\n",
      "       index_       mse  Class\n",
      "3588  81688.0  0.138418      1\n",
      "2387  56959.0  0.107699      1\n",
      "1232  67178.0  0.107462      1\n",
      "3757  70710.0  0.081620      1\n",
      "846   19387.0  0.060612      1\n",
      "3068  98134.0  0.051598      1\n",
      "3479  98091.0  0.047680      1\n",
      "3782  71867.0  0.047363      1\n",
      "3416  14219.0  0.046339      1\n",
      "2150  58549.0  0.046309      1\n",
      "3535  54133.0  0.044781      1\n",
      "1075   5614.0  0.044454      1\n",
      "381   90072.0  0.042374      1\n",
      "2570  51374.0  0.039572      1\n",
      "3930  29791.0  0.039123      1\n",
      "1183  44985.0  0.038555      1\n",
      "880   33633.0  0.037682      1\n",
      "258   38137.0  0.036987      1\n",
      "1684  16315.0  0.036257      1\n",
      "2877  25265.0  0.036029      1\n",
      "2973  52407.0  0.031474      1\n",
      "2170  51331.0  0.031117      1\n",
      "4122  63285.0  0.029056      0\n",
      "3975   8810.0  0.029004      0\n",
      "166   14472.0  0.027803      0\n",
      "1821  33458.0  0.027659      0\n",
      "465   19578.0  0.027000      0\n",
      "377   20857.0  0.026393      0\n",
      "2857  26254.0  0.026322      0\n",
      "4128  21725.0  0.025140      0\n",
      "2467  28980.0  0.024347      0\n",
      "1043  26883.0  0.024330      0\n",
      "1102  83800.0  0.023721      0\n",
      "2060  44350.0  0.021140      0\n",
      "1239  39187.0  0.020071      0\n",
      "506   97603.0  0.019936      0\n",
      "4165  19683.0  0.019161      0\n",
      "3831  84310.0  0.017894      0\n",
      "3122   8165.0  0.017508      0\n",
      "4     18155.0  0.017219      0\n",
      "3968  48515.0  0.017069      0\n",
      "2807  19085.0  0.016372      0\n",
      "174     316.0  0.016163      0\n",
      "718   99889.0  0.015981      0\n",
      "123   35821.0  0.015738      0\n",
      "1797  96363.0  0.015190      0\n",
      "3137  53700.0  0.014797      0\n",
      "1738   3959.0  0.014514      0\n",
      "3963  43625.0  0.014461      0\n",
      "1741  61148.0  0.014423      0\n",
      "        pred_0    pred_1  index_  Class\n",
      "880   0.260000  0.740000   33633      1\n",
      "2877  0.291111  0.708889   25265      1\n",
      "678   0.311111  0.688889   16806      1\n",
      "311   0.320000  0.680000   27809      1\n",
      "4122  0.413333  0.586667   63285      1\n",
      "3419  0.440000  0.560000   47996      1\n",
      "3585  0.446667  0.553333   33871      1\n",
      "1183  0.452222  0.547778   44985      1\n",
      "3588  0.459524  0.540476   81688      1\n",
      "3535  0.460667  0.539333   54133      1\n",
      "3757  0.467556  0.532444   70710      1\n",
      "2570  0.481111  0.518889   51374      1\n",
      "3068  0.483889  0.516111   98134      1\n",
      "258   0.502444  0.497556   38137      1\n",
      "4128  0.503778  0.496222   21725      1\n",
      "1236  0.513333  0.486667   12069      1\n",
      "2663  0.522444  0.477556   79589      1\n",
      "4188  0.528889  0.471111   57340      1\n",
      "2008  0.546667  0.453333   81602      1\n",
      "2170  0.547778  0.452222   51331      1\n",
      "465   0.547778  0.452222   19578      1\n",
      "1821  0.550000  0.450000   33458      1\n",
      "3479  0.551635  0.448365   98091      1\n",
      "2973  0.552857  0.447143   52407      1\n",
      "3738  0.560000  0.440000   93511      1\n",
      "3782  0.575000  0.425000   71867      1\n",
      "3855  0.600000  0.400000   45752      1\n",
      "903   0.613333  0.386667   87407      1\n",
      "1342  0.620000  0.380000   18349      1\n",
      "2387  0.631111  0.368889   56959      1\n",
      "846   0.648778  0.351222   19387      1\n",
      "3975  0.702222  0.297778    8810      0\n",
      "2956  0.705556  0.294444   26300      0\n",
      "3288  0.706667  0.293333   99410      0\n",
      "166   0.716556  0.283444   14472      0\n",
      "381   0.718889  0.281111   90072      0\n",
      "1684  0.719778  0.280222   16315      0\n",
      "3740  0.722222  0.277778   96399      0\n",
      "1043  0.726667  0.273333   26883      0\n",
      "3416  0.729333  0.270667   14219      0\n",
      "3930  0.734794  0.265206   29791      0\n",
      "904   0.735000  0.265000   76854      0\n",
      "3084  0.746667  0.253333   46031      0\n",
      "1075  0.751000  0.249000    5614      0\n",
      "377   0.751111  0.248889   20857      0\n",
      "3455  0.753333  0.246667   56901      0\n",
      "1232  0.753889  0.246111   67178      0\n",
      "2807  0.760000  0.240000   19085      0\n",
      "2839  0.773333  0.226667   98523      0\n",
      "2866  0.780000  0.220000   18278      0\n",
      "Normal count: 20943\n",
      "Attack count: 370\n",
      "Epoch 1/32\n",
      "377/377 [==============================] - 6s 12ms/step - loss: 0.0084 - val_loss: 0.0085\n",
      "Epoch 2/32\n",
      "377/377 [==============================] - 4s 12ms/step - loss: 0.0072 - val_loss: 0.0084\n",
      "Epoch 3/32\n",
      "377/377 [==============================] - 4s 12ms/step - loss: 0.0071 - val_loss: 0.0083\n",
      "Epoch 4/32\n",
      "377/377 [==============================] - 4s 12ms/step - loss: 0.0070 - val_loss: 0.0083\n",
      "Epoch 5/32\n",
      "377/377 [==============================] - 5s 13ms/step - loss: 0.0069 - val_loss: 0.0081\n",
      "Epoch 6/32\n",
      "377/377 [==============================] - 4s 12ms/step - loss: 0.0066 - val_loss: 0.0079\n",
      "Epoch 7/32\n",
      "377/377 [==============================] - 4s 11ms/step - loss: 0.0065 - val_loss: 0.0078\n",
      "Epoch 8/32\n",
      "377/377 [==============================] - 5s 13ms/step - loss: 0.0064 - val_loss: 0.0078\n",
      "Epoch 9/32\n",
      "377/377 [==============================] - 5s 12ms/step - loss: 0.0064 - val_loss: 0.0077\n",
      "Epoch 10/32\n",
      "377/377 [==============================] - 4s 12ms/step - loss: 0.0064 - val_loss: 0.0077\n",
      "Epoch 11/32\n",
      "377/377 [==============================] - 5s 13ms/step - loss: 0.0063 - val_loss: 0.0077\n",
      "Epoch 12/32\n",
      "377/377 [==============================] - 4s 11ms/step - loss: 0.0061 - val_loss: 0.0072\n",
      "Epoch 13/32\n",
      "377/377 [==============================] - 4s 11ms/step - loss: 0.0057 - val_loss: 0.0070\n",
      "Epoch 14/32\n",
      "377/377 [==============================] - 4s 12ms/step - loss: 0.0057 - val_loss: 0.0070\n",
      "Epoch 15/32\n",
      "377/377 [==============================] - 5s 12ms/step - loss: 0.0056 - val_loss: 0.0070\n",
      "Epoch 16/32\n",
      "377/377 [==============================] - 4s 12ms/step - loss: 0.0056 - val_loss: 0.0069\n",
      "Epoch 17/32\n",
      "377/377 [==============================] - 5s 12ms/step - loss: 0.0055 - val_loss: 0.0068\n",
      "Epoch 18/32\n",
      "377/377 [==============================] - 4s 12ms/step - loss: 0.0054 - val_loss: 0.0067\n",
      "Epoch 19/32\n",
      "377/377 [==============================] - 5s 13ms/step - loss: 0.0053 - val_loss: 0.0067\n",
      "Epoch 20/32\n",
      "377/377 [==============================] - 5s 13ms/step - loss: 0.0053 - val_loss: 0.0066\n",
      "Epoch 21/32\n",
      "377/377 [==============================] - 4s 12ms/step - loss: 0.0052 - val_loss: 0.0066\n",
      "Epoch 22/32\n",
      "377/377 [==============================] - 4s 11ms/step - loss: 0.0052 - val_loss: 0.0066\n",
      "Epoch 23/32\n",
      "377/377 [==============================] - 4s 11ms/step - loss: 0.0052 - val_loss: 0.0065\n",
      "Epoch 24/32\n",
      "377/377 [==============================] - 4s 12ms/step - loss: 0.0052 - val_loss: 0.0065\n",
      "Epoch 25/32\n",
      "377/377 [==============================] - 4s 11ms/step - loss: 0.0051 - val_loss: 0.0065\n",
      "Epoch 26/32\n",
      "377/377 [==============================] - 4s 11ms/step - loss: 0.0051 - val_loss: 0.0065\n",
      "Epoch 27/32\n",
      "377/377 [==============================] - 4s 11ms/step - loss: 0.0051 - val_loss: 0.0064\n",
      "Epoch 28/32\n",
      "377/377 [==============================] - 4s 12ms/step - loss: 0.0051 - val_loss: 0.0064\n",
      "Epoch 29/32\n",
      "377/377 [==============================] - 4s 12ms/step - loss: 0.0051 - val_loss: 0.0064\n",
      "Epoch 30/32\n",
      "377/377 [==============================] - 4s 11ms/step - loss: 0.0051 - val_loss: 0.0064\n",
      "Epoch 31/32\n",
      "377/377 [==============================] - 5s 13ms/step - loss: 0.0050 - val_loss: 0.0064\n",
      "Epoch 32/32\n",
      "377/377 [==============================] - 5s 12ms/step - loss: 0.0050 - val_loss: 0.0064\n",
      "\n",
      " >>> TIMESTEP 4 =============================================================== \n",
      "\n",
      "       index_       mse  Class\n",
      "2061  37315.0  4.323840      1\n",
      "3636  97183.0  0.522574      1\n",
      "3234  87872.0  0.239712      1\n",
      "1533  77872.0  0.100121      1\n",
      "4357   9775.0  0.090123      1\n",
      "1352  65468.0  0.083421      1\n",
      "157     824.0  0.076544      1\n",
      "1683  34048.0  0.059077      1\n",
      "2412  91240.0  0.053866      1\n",
      "1655  12569.0  0.052501      1\n",
      "2155  63469.0  0.050830      1\n",
      "2263  66065.0  0.047469      1\n",
      "2260  19720.0  0.045957      1\n",
      "622   26305.0  0.044854      1\n",
      "3308  27570.0  0.044494      1\n",
      "557   42018.0  0.043450      1\n",
      "3051  78009.0  0.043424      1\n",
      "3937  57281.0  0.042884      1\n",
      "963   78477.0  0.041609      1\n",
      "221   93602.0  0.039879      1\n",
      "3909  88686.0  0.038522      1\n",
      "2728  95407.0  0.036658      1\n",
      "1674  69735.0  0.034134      1\n",
      "3382  22648.0  0.032310      1\n",
      "3531  26314.0  0.032142      1\n",
      "3142  21494.0  0.031857      1\n",
      "2520  28214.0  0.031702      1\n",
      "3046  52111.0  0.028133      0\n",
      "781   35968.0  0.027727      0\n",
      "1344  52587.0  0.026955      0\n",
      "2023  65996.0  0.025487      0\n",
      "4241  73612.0  0.023180      0\n",
      "3798  34926.0  0.022762      0\n",
      "2286  91435.0  0.022150      0\n",
      "3020   6934.0  0.022018      0\n",
      "3980  43645.0  0.022017      0\n",
      "1340  98446.0  0.021419      0\n",
      "2544  91541.0  0.020903      0\n",
      "3441  47908.0  0.020903      0\n",
      "3956  75452.0  0.020535      0\n",
      "2900   3552.0  0.020207      0\n",
      "3092  71187.0  0.020078      0\n",
      "4220  22554.0  0.019422      0\n",
      "4321  95385.0  0.019110      0\n",
      "1278  74038.0  0.018010      0\n",
      "2477  86858.0  0.016715      0\n",
      "1651  75930.0  0.015933      0\n",
      "2524  72214.0  0.015822      0\n",
      "2229  43538.0  0.015474      0\n",
      "3113  67633.0  0.014950      0\n",
      "        pred_0    pred_1  index_  Class\n",
      "1964  0.126667  0.873333   95943      1\n",
      "3937  0.170000  0.830000   57281      1\n",
      "2263  0.186667  0.813333   66065      1\n",
      "3051  0.190000  0.810000   78009      1\n",
      "1776  0.193333  0.806667   93778      1\n",
      "2520  0.213333  0.786667   28214      1\n",
      "3308  0.216667  0.783333   27570      1\n",
      "759   0.259744  0.740256   50212      1\n",
      "781   0.290000  0.710000   35968      1\n",
      "3531  0.377778  0.622222   26314      1\n",
      "2631  0.380000  0.620000   48079      1\n",
      "3157  0.380000  0.620000   93788      1\n",
      "187   0.393333  0.606667   45039      1\n",
      "557   0.421111  0.578889   42018      1\n",
      "3707  0.430556  0.569444   45769      1\n",
      "622   0.431556  0.568444   26305      1\n",
      "1364  0.433333  0.566667   93758      1\n",
      "4241  0.442222  0.557778   73612      1\n",
      "3142  0.472222  0.527778   21494      1\n",
      "2260  0.474444  0.525556   19720      1\n",
      "2728  0.476667  0.523333   95407      1\n",
      "2750  0.493333  0.506667   74247      1\n",
      "2155  0.498333  0.501667   63469      1\n",
      "3909  0.501111  0.498889   88686      1\n",
      "1683  0.508222  0.491778   34048      1\n",
      "1872  0.513333  0.486667   66019      1\n",
      "2516  0.513333  0.486667   92213      1\n",
      "479   0.513333  0.486667   84002      1\n",
      "713   0.526667  0.473333   14543      1\n",
      "3956  0.537333  0.462667   75452      1\n",
      "3046  0.549333  0.450667   52111      1\n",
      "963   0.550000  0.450000   78477      1\n",
      "1655  0.572778  0.427222   12569      1\n",
      "995   0.580000  0.420000   89364      1\n",
      "1674  0.582778  0.417222   69735      1\n",
      "3636  0.588000  0.412000   97183      1\n",
      "1344  0.591111  0.408889   52587      1\n",
      "157   0.606222  0.393778     824      1\n",
      "1533  0.628667  0.371333   77872      1\n",
      "221   0.646111  0.353889   93602      1\n",
      "3445  0.646667  0.353333   31588      1\n",
      "1352  0.648778  0.351222   65468      1\n",
      "2461  0.653333  0.346667   40784      1\n",
      "4357  0.657333  0.342667    9775      1\n",
      "2023  0.675556  0.324444   65996      1\n",
      "512   0.680000  0.320000   43548      1\n",
      "3382  0.722222  0.277778   22648      0\n",
      "1220  0.733333  0.266667   64978      0\n",
      "2412  0.733333  0.266667   91240      0\n",
      "3798  0.754222  0.245778   34926      0\n",
      "Normal count: 20970\n",
      "Attack count: 443\n",
      "Epoch 1/32\n",
      "378/378 [==============================] - 6s 13ms/step - loss: 0.0083 - val_loss: 0.0087\n",
      "Epoch 2/32\n",
      "378/378 [==============================] - 4s 12ms/step - loss: 0.0071 - val_loss: 0.0086\n",
      "Epoch 3/32\n",
      "378/378 [==============================] - 5s 12ms/step - loss: 0.0070 - val_loss: 0.0085\n",
      "Epoch 4/32\n",
      "378/378 [==============================] - 4s 12ms/step - loss: 0.0070 - val_loss: 0.0084\n",
      "Epoch 5/32\n",
      "378/378 [==============================] - 5s 12ms/step - loss: 0.0068 - val_loss: 0.0082\n",
      "Epoch 6/32\n",
      "378/378 [==============================] - 5s 13ms/step - loss: 0.0066 - val_loss: 0.0081\n",
      "Epoch 7/32\n",
      "378/378 [==============================] - 4s 11ms/step - loss: 0.0065 - val_loss: 0.0080\n",
      "Epoch 8/32\n",
      "378/378 [==============================] - 4s 12ms/step - loss: 0.0064 - val_loss: 0.0080\n",
      "Epoch 9/32\n",
      "378/378 [==============================] - 5s 13ms/step - loss: 0.0063 - val_loss: 0.0076\n",
      "Epoch 10/32\n",
      "378/378 [==============================] - 5s 12ms/step - loss: 0.0059 - val_loss: 0.0073\n",
      "Epoch 11/32\n",
      "378/378 [==============================] - 5s 13ms/step - loss: 0.0057 - val_loss: 0.0073\n",
      "Epoch 12/32\n",
      "378/378 [==============================] - 4s 11ms/step - loss: 0.0057 - val_loss: 0.0073\n",
      "Epoch 13/32\n",
      "378/378 [==============================] - 4s 12ms/step - loss: 0.0057 - val_loss: 0.0072\n",
      "Epoch 14/32\n",
      "378/378 [==============================] - 5s 12ms/step - loss: 0.0057 - val_loss: 0.0072\n",
      "Epoch 15/32\n",
      "378/378 [==============================] - 4s 11ms/step - loss: 0.0056 - val_loss: 0.0072\n",
      "Epoch 16/32\n",
      "378/378 [==============================] - 4s 12ms/step - loss: 0.0056 - val_loss: 0.0072\n",
      "Epoch 17/32\n",
      "378/378 [==============================] - 5s 12ms/step - loss: 0.0056 - val_loss: 0.0072\n",
      "Epoch 18/32\n",
      "378/378 [==============================] - 5s 12ms/step - loss: 0.0056 - val_loss: 0.0071\n",
      "Epoch 19/32\n",
      "378/378 [==============================] - 4s 12ms/step - loss: 0.0056 - val_loss: 0.0071\n",
      "Epoch 20/32\n",
      "378/378 [==============================] - 4s 12ms/step - loss: 0.0055 - val_loss: 0.0071\n",
      "Epoch 21/32\n",
      "378/378 [==============================] - 4s 12ms/step - loss: 0.0055 - val_loss: 0.0070\n",
      "Epoch 22/32\n",
      "378/378 [==============================] - 5s 12ms/step - loss: 0.0055 - val_loss: 0.0070\n",
      "Epoch 23/32\n",
      "378/378 [==============================] - 5s 12ms/step - loss: 0.0054 - val_loss: 0.0069\n",
      "Epoch 24/32\n",
      "378/378 [==============================] - 5s 13ms/step - loss: 0.0053 - val_loss: 0.0068\n",
      "Epoch 25/32\n",
      "378/378 [==============================] - 5s 12ms/step - loss: 0.0052 - val_loss: 0.0067\n",
      "Epoch 26/32\n",
      "378/378 [==============================] - 5s 12ms/step - loss: 0.0051 - val_loss: 0.0067\n",
      "Epoch 27/32\n",
      "378/378 [==============================] - 5s 12ms/step - loss: 0.0051 - val_loss: 0.0066\n",
      "Epoch 28/32\n",
      "378/378 [==============================] - 4s 12ms/step - loss: 0.0050 - val_loss: 0.0066\n",
      "Epoch 29/32\n",
      "378/378 [==============================] - 6s 16ms/step - loss: 0.0050 - val_loss: 0.0065\n",
      "Epoch 30/32\n",
      "378/378 [==============================] - 5s 13ms/step - loss: 0.0049 - val_loss: 0.0064\n",
      "Epoch 31/32\n",
      "378/378 [==============================] - 5s 13ms/step - loss: 0.0049 - val_loss: 0.0064\n",
      "Epoch 32/32\n",
      "378/378 [==============================] - 4s 11ms/step - loss: 0.0048 - val_loss: 0.0063\n",
      "\n",
      " >>> TIMESTEP 5 =============================================================== \n",
      "\n",
      "       index_       mse  Class\n",
      "4316  82742.0  0.217427      1\n",
      "1417  31575.0  0.101577      1\n",
      "2371  61220.0  0.100842      1\n",
      "4172    511.0  0.098405      1\n",
      "130   34553.0  0.096037      1\n",
      "1266  29789.0  0.084454      1\n",
      "3363  97512.0  0.079820      1\n",
      "2029  10962.0  0.073340      1\n",
      "809   91343.0  0.056561      1\n",
      "1851  45496.0  0.055632      1\n",
      "3166  41964.0  0.054945      1\n",
      "4183  23732.0  0.050280      1\n",
      "3682  48322.0  0.045739      1\n",
      "2822  75363.0  0.041233      1\n",
      "3791  45318.0  0.041075      1\n",
      "983   31807.0  0.039073      1\n",
      "2611  75450.0  0.034855      1\n",
      "2645  24071.0  0.033576      1\n",
      "3998  65980.0  0.032478      1\n",
      "3604  15471.0  0.031945      1\n",
      "815   92845.0  0.031548      1\n",
      "757   90668.0  0.030162      1\n",
      "3007  64536.0  0.030087      1\n",
      "1276  55752.0  0.029516      0\n",
      "1629   4029.0  0.029510      0\n",
      "4288  95808.0  0.029080      0\n",
      "695   41088.0  0.026600      0\n",
      "3925  19881.0  0.026318      0\n",
      "1097  52376.0  0.026199      0\n",
      "1187  27710.0  0.025817      0\n",
      "372   49597.0  0.025595      0\n",
      "3436  48471.0  0.024579      0\n",
      "3125  27749.0  0.024043      0\n",
      "3213  66083.0  0.023567      0\n",
      "2299  64688.0  0.022875      0\n",
      "964   46794.0  0.021279      0\n",
      "3482  27650.0  0.021240      0\n",
      "459   23911.0  0.021086      0\n",
      "3113  81353.0  0.020998      0\n",
      "968   78431.0  0.020690      0\n",
      "1601  36187.0  0.020600      0\n",
      "2641  26321.0  0.020340      0\n",
      "518   99776.0  0.019557      0\n",
      "1644  14601.0  0.019175      0\n",
      "3820  39381.0  0.019147      0\n",
      "1988  64315.0  0.018653      0\n",
      "1491  85077.0  0.018650      0\n",
      "1064  73889.0  0.016378      0\n",
      "4373  67492.0  0.016261      0\n",
      "1577   9233.0  0.015473      0\n",
      "        pred_0    pred_1  index_  Class\n",
      "2645  0.133333  0.866667   24071      1\n",
      "738   0.141667  0.858333   96087      1\n",
      "2822  0.156667  0.843333   75363      1\n",
      "2726  0.286667  0.713333   50250      1\n",
      "3166  0.302222  0.697778   41964      1\n",
      "3007  0.303333  0.696667   64536      1\n",
      "1276  0.326667  0.673333   55752      1\n",
      "4288  0.334444  0.665556   95808      1\n",
      "169   0.346667  0.653333   54141      1\n",
      "3604  0.347778  0.652222   15471      1\n",
      "815   0.353333  0.646667   92845      1\n",
      "2246  0.390000  0.610000   64702      1\n",
      "983   0.410556  0.589444   31807      1\n",
      "3998  0.446667  0.553333   65980      1\n",
      "3739  0.450000  0.550000   90167      1\n",
      "3682  0.468889  0.531111   48322      1\n",
      "757   0.481667  0.518333   90668      1\n",
      "1033  0.486667  0.513333   12983      1\n",
      "2611  0.491667  0.508333   75450      1\n",
      "1059  0.500000  0.500000   12225      1\n",
      "4316  0.503333  0.496667   82742      1\n",
      "130   0.523778  0.476222   34553      1\n",
      "2972  0.531111  0.468889   72348      1\n",
      "1851  0.540556  0.459444   45496      1\n",
      "3363  0.543333  0.456667   97512      1\n",
      "3482  0.555000  0.445000   27650      1\n",
      "2371  0.584667  0.415333   61220      1\n",
      "2029  0.588889  0.411111   10962      1\n",
      "1266  0.608556  0.391444   29789      1\n",
      "3436  0.610000  0.390000   48471      1\n",
      "4172  0.618222  0.381778     511      1\n",
      "1644  0.618667  0.381333   14601      1\n",
      "3925  0.639444  0.360556   19881      1\n",
      "3213  0.644111  0.355889   66083      1\n",
      "1417  0.657778  0.342222   31575      1\n",
      "695   0.660000  0.340000   41088      1\n",
      "4183  0.662889  0.337111   23732      1\n",
      "3791  0.677491  0.322509   45318      1\n",
      "4044  0.686667  0.313333   21964      1\n",
      "809   0.688651  0.311349   91343      1\n",
      "3602  0.706667  0.293333   89254      0\n",
      "3178  0.733333  0.266667   71981      0\n",
      "1629  0.748667  0.251333    4029      0\n",
      "1187  0.751111  0.248889   27710      0\n",
      "2640  0.755556  0.244444   20927      0\n",
      "76    0.760000  0.240000   92583      0\n",
      "1451  0.773333  0.226667   36661      0\n",
      "3062  0.780000  0.220000   15755      0\n",
      "790   0.786667  0.213333   83653      0\n",
      "3081  0.793333  0.206667   64106      0\n",
      "Normal count: 21007\n",
      "Attack count: 506\n",
      "Epoch 1/32\n",
      "379/379 [==============================] - 7s 16ms/step - loss: 0.0084 - val_loss: 0.0090\n",
      "Epoch 2/32\n",
      "379/379 [==============================] - 5s 13ms/step - loss: 0.0071 - val_loss: 0.0089\n",
      "Epoch 3/32\n",
      "379/379 [==============================] - 5s 13ms/step - loss: 0.0070 - val_loss: 0.0088\n",
      "Epoch 4/32\n",
      "379/379 [==============================] - 5s 13ms/step - loss: 0.0070 - val_loss: 0.0087\n",
      "Epoch 5/32\n",
      "379/379 [==============================] - 5s 13ms/step - loss: 0.0068 - val_loss: 0.0086\n",
      "Epoch 6/32\n",
      "379/379 [==============================] - 5s 14ms/step - loss: 0.0066 - val_loss: 0.0084\n",
      "Epoch 7/32\n",
      "379/379 [==============================] - 6s 15ms/step - loss: 0.0065 - val_loss: 0.0085\n",
      "Epoch 8/32\n",
      "379/379 [==============================] - 6s 16ms/step - loss: 0.0065 - val_loss: 0.0083\n",
      "Epoch 9/32\n",
      "379/379 [==============================] - 7s 17ms/step - loss: 0.0065 - val_loss: 0.0083\n",
      "Epoch 10/32\n",
      "379/379 [==============================] - 5s 14ms/step - loss: 0.0064 - val_loss: 0.0083\n",
      "Epoch 11/32\n",
      "379/379 [==============================] - 5s 13ms/step - loss: 0.0064 - val_loss: 0.0083\n",
      "Epoch 12/32\n",
      "379/379 [==============================] - 5s 13ms/step - loss: 0.0064 - val_loss: 0.0082\n",
      "Epoch 13/32\n",
      "379/379 [==============================] - 5s 13ms/step - loss: 0.0064 - val_loss: 0.0082\n",
      "Epoch 14/32\n",
      "379/379 [==============================] - 5s 13ms/step - loss: 0.0064 - val_loss: 0.0082\n",
      "Epoch 15/32\n",
      "379/379 [==============================] - 5s 14ms/step - loss: 0.0063 - val_loss: 0.0082\n",
      "Epoch 16/32\n",
      "379/379 [==============================] - 5s 13ms/step - loss: 0.0063 - val_loss: 0.0080\n",
      "Epoch 17/32\n",
      "379/379 [==============================] - 5s 14ms/step - loss: 0.0059 - val_loss: 0.0076\n",
      "Epoch 18/32\n",
      "379/379 [==============================] - 5s 13ms/step - loss: 0.0057 - val_loss: 0.0075\n",
      "Epoch 19/32\n",
      "379/379 [==============================] - 5s 13ms/step - loss: 0.0057 - val_loss: 0.0075\n",
      "Epoch 20/32\n",
      "379/379 [==============================] - 5s 14ms/step - loss: 0.0056 - val_loss: 0.0074\n",
      "Epoch 21/32\n",
      "379/379 [==============================] - 5s 14ms/step - loss: 0.0056 - val_loss: 0.0074\n",
      "Epoch 22/32\n",
      "379/379 [==============================] - 5s 14ms/step - loss: 0.0055 - val_loss: 0.0073\n",
      "Epoch 23/32\n",
      "379/379 [==============================] - 5s 14ms/step - loss: 0.0055 - val_loss: 0.0073\n",
      "Epoch 24/32\n",
      "379/379 [==============================] - 5s 13ms/step - loss: 0.0054 - val_loss: 0.0072\n",
      "Epoch 25/32\n",
      "379/379 [==============================] - 5s 14ms/step - loss: 0.0054 - val_loss: 0.0072\n",
      "Epoch 26/32\n",
      "379/379 [==============================] - 5s 13ms/step - loss: 0.0053 - val_loss: 0.0071\n",
      "Epoch 27/32\n",
      "379/379 [==============================] - 5s 14ms/step - loss: 0.0052 - val_loss: 0.0069\n",
      "Epoch 28/32\n",
      "379/379 [==============================] - 5s 14ms/step - loss: 0.0051 - val_loss: 0.0068\n",
      "Epoch 29/32\n",
      "379/379 [==============================] - 5s 14ms/step - loss: 0.0050 - val_loss: 0.0067\n",
      "Epoch 30/32\n",
      "379/379 [==============================] - 5s 14ms/step - loss: 0.0049 - val_loss: 0.0067\n",
      "Epoch 31/32\n",
      "379/379 [==============================] - 5s 14ms/step - loss: 0.0049 - val_loss: 0.0067\n",
      "Epoch 32/32\n",
      "379/379 [==============================] - 5s 13ms/step - loss: 0.0048 - val_loss: 0.0066\n",
      "\n",
      " >>> TIMESTEP 6 =============================================================== \n",
      "\n",
      "       index_       mse  Class\n",
      "3724  16608.0  0.210627      1\n",
      "599   25322.0  0.182015      1\n",
      "2941  26352.0  0.167583      1\n",
      "2006  53682.0  0.120384      1\n",
      "3944  84040.0  0.116847      1\n",
      "2582   6057.0  0.088504      1\n",
      "3841  66401.0  0.072611      1\n",
      "1505  25507.0  0.069068      1\n",
      "1701  27957.0  0.054500      1\n",
      "2904  76636.0  0.052432      1\n",
      "550   40396.0  0.051906      1\n",
      "2761  10437.0  0.044882      1\n",
      "611   38294.0  0.044787      1\n",
      "514   74072.0  0.043642      1\n",
      "3677  95748.0  0.042388      1\n",
      "4241  76998.0  0.042310      1\n",
      "2224  93830.0  0.041492      1\n",
      "3436  86198.0  0.040513      1\n",
      "628   63353.0  0.040473      1\n",
      "1976  62137.0  0.039297      1\n",
      "875    5559.0  0.036911      1\n",
      "1574   9018.0  0.036201      1\n",
      "1002  21611.0  0.035038      1\n",
      "1753  65991.0  0.034879      1\n",
      "3650  67443.0  0.033739      1\n",
      "1279  85193.0  0.032024      1\n",
      "807   38175.0  0.030699      1\n",
      "3063  95664.0  0.030080      1\n",
      "4100  63964.0  0.030019      1\n",
      "3143  32019.0  0.029849      0\n",
      "3172  66095.0  0.029719      0\n",
      "4291   9130.0  0.028238      0\n",
      "1928  21491.0  0.027602      0\n",
      "3569  78070.0  0.026592      0\n",
      "173   35920.0  0.026176      0\n",
      "2279  84157.0  0.025521      0\n",
      "1829  99741.0  0.025271      0\n",
      "1647  72271.0  0.024991      0\n",
      "1777  21519.0  0.023074      0\n",
      "1438  21531.0  0.020361      0\n",
      "1417  23082.0  0.020319      0\n",
      "2929  64649.0  0.020214      0\n",
      "1957  94699.0  0.019226      0\n",
      "4308  40806.0  0.018987      0\n",
      "1555  35929.0  0.018841      0\n",
      "1934  88692.0  0.018804      0\n",
      "3571   2574.0  0.018524      0\n",
      "2036  30475.0  0.018091      0\n",
      "3750  95540.0  0.017787      0\n",
      "1954  58073.0  0.017102      0\n",
      "        pred_0    pred_1  index_  Class\n",
      "550   0.116667  0.883333   40396      1\n",
      "3293  0.118889  0.881111   93528      1\n",
      "3677  0.191111  0.808889   95748      1\n",
      "3716  0.213333  0.786667   45646      1\n",
      "259   0.226667  0.773333   12290      1\n",
      "3650  0.247778  0.752222   67443      1\n",
      "1976  0.260000  0.740000   62137      1\n",
      "1753  0.273333  0.726667   65991      1\n",
      "4308  0.273333  0.726667   40806      1\n",
      "1002  0.275000  0.725000   21611      1\n",
      "628   0.301333  0.698667   63353      1\n",
      "514   0.328095  0.671905   74072      1\n",
      "611   0.341333  0.658667   38294      1\n",
      "2006  0.347778  0.652222   53682      1\n",
      "1202  0.386667  0.613333   14816      1\n",
      "3063  0.398667  0.601333   95664      1\n",
      "2941  0.405556  0.594444   26352      1\n",
      "807   0.457778  0.542222   38175      1\n",
      "3655  0.460000  0.540000   27254      1\n",
      "2289  0.500000  0.500000   38165      1\n",
      "3436  0.540667  0.459333   86198      1\n",
      "3944  0.542778  0.457222   84040      1\n",
      "4241  0.559444  0.440556   76998      1\n",
      "3143  0.572778  0.427222   32019      1\n",
      "1505  0.575444  0.424556   25507      1\n",
      "875   0.587857  0.412143    5559      1\n",
      "3172  0.607222  0.392778   66095      1\n",
      "4100  0.612778  0.387222   63964      1\n",
      "1928  0.626667  0.373333   21491      1\n",
      "2904  0.629556  0.370444   76636      1\n",
      "1279  0.631444  0.368556   85193      1\n",
      "3569  0.635556  0.364444   78070      1\n",
      "2582  0.638556  0.361444    6057      1\n",
      "3869  0.642222  0.357778   80341      1\n",
      "4111  0.643333  0.356667   30301      1\n",
      "3724  0.643667  0.356333   16608      1\n",
      "599   0.647222  0.352778   25322      1\n",
      "3605  0.648889  0.351111   33519      1\n",
      "1555  0.655556  0.344444   35929      1\n",
      "2224  0.687778  0.312222   93830      1\n",
      "1701  0.688778  0.311222   27957      1\n",
      "3841  0.699444  0.300556   66401      1\n",
      "3750  0.706000  0.294000   95540      0\n",
      "3608  0.706667  0.293333   96005      0\n",
      "2761  0.707222  0.292778   10437      0\n",
      "1574  0.709444  0.290556    9018      0\n",
      "4291  0.721111  0.278889    9130      0\n",
      "3136  0.726444  0.273556   10475      0\n",
      "841   0.730000  0.270000   19898      0\n",
      "2737  0.748778  0.251222   69482      0\n",
      "Normal count: 21036\n",
      "Attack count: 577\n",
      "Epoch 1/32\n",
      "379/379 [==============================] - 7s 15ms/step - loss: 0.0084 - val_loss: 0.0092\n",
      "Epoch 2/32\n",
      "379/379 [==============================] - 5s 14ms/step - loss: 0.0071 - val_loss: 0.0091\n",
      "Epoch 3/32\n",
      "379/379 [==============================] - 5s 13ms/step - loss: 0.0070 - val_loss: 0.0090\n",
      "Epoch 4/32\n",
      "379/379 [==============================] - 5s 14ms/step - loss: 0.0069 - val_loss: 0.0088\n",
      "Epoch 5/32\n",
      "379/379 [==============================] - 5s 14ms/step - loss: 0.0067 - val_loss: 0.0086\n",
      "Epoch 6/32\n",
      "379/379 [==============================] - 6s 15ms/step - loss: 0.0065 - val_loss: 0.0086\n",
      "Epoch 7/32\n",
      "379/379 [==============================] - 5s 14ms/step - loss: 0.0065 - val_loss: 0.0085\n",
      "Epoch 8/32\n",
      "379/379 [==============================] - 5s 14ms/step - loss: 0.0064 - val_loss: 0.0085\n",
      "Epoch 9/32\n",
      "379/379 [==============================] - 5s 13ms/step - loss: 0.0064 - val_loss: 0.0084\n",
      "Epoch 10/32\n",
      "379/379 [==============================] - 5s 14ms/step - loss: 0.0064 - val_loss: 0.0084\n",
      "Epoch 11/32\n",
      "379/379 [==============================] - 6s 15ms/step - loss: 0.0064 - val_loss: 0.0085\n",
      "Epoch 12/32\n",
      "379/379 [==============================] - 6s 16ms/step - loss: 0.0064 - val_loss: 0.0084\n",
      "Epoch 13/32\n",
      "379/379 [==============================] - 6s 15ms/step - loss: 0.0064 - val_loss: 0.0084\n",
      "Epoch 14/32\n",
      "379/379 [==============================] - 5s 14ms/step - loss: 0.0063 - val_loss: 0.0084\n",
      "Epoch 15/32\n",
      "379/379 [==============================] - 5s 13ms/step - loss: 0.0063 - val_loss: 0.0084\n",
      "Epoch 16/32\n",
      "379/379 [==============================] - 5s 13ms/step - loss: 0.0063 - val_loss: 0.0083\n",
      "Epoch 17/32\n",
      "379/379 [==============================] - 5s 14ms/step - loss: 0.0061 - val_loss: 0.0080\n",
      "Epoch 18/32\n",
      "379/379 [==============================] - 5s 13ms/step - loss: 0.0059 - val_loss: 0.0078\n",
      "Epoch 19/32\n",
      "379/379 [==============================] - 5s 14ms/step - loss: 0.0057 - val_loss: 0.0076\n",
      "Epoch 20/32\n",
      "379/379 [==============================] - 5s 14ms/step - loss: 0.0056 - val_loss: 0.0076\n",
      "Epoch 21/32\n",
      "379/379 [==============================] - 5s 14ms/step - loss: 0.0055 - val_loss: 0.0075\n",
      "Epoch 22/32\n",
      "379/379 [==============================] - 5s 14ms/step - loss: 0.0054 - val_loss: 0.0074\n",
      "Epoch 23/32\n",
      "379/379 [==============================] - 6s 15ms/step - loss: 0.0053 - val_loss: 0.0073\n",
      "Epoch 24/32\n",
      "379/379 [==============================] - 6s 15ms/step - loss: 0.0053 - val_loss: 0.0072\n",
      "Epoch 25/32\n",
      "379/379 [==============================] - 5s 14ms/step - loss: 0.0052 - val_loss: 0.0072\n",
      "Epoch 26/32\n",
      "379/379 [==============================] - 5s 14ms/step - loss: 0.0052 - val_loss: 0.0071\n",
      "Epoch 27/32\n",
      "379/379 [==============================] - 5s 14ms/step - loss: 0.0051 - val_loss: 0.0071\n",
      "Epoch 28/32\n",
      "379/379 [==============================] - 5s 14ms/step - loss: 0.0050 - val_loss: 0.0070\n",
      "Epoch 29/32\n",
      "379/379 [==============================] - 5s 13ms/step - loss: 0.0050 - val_loss: 0.0070\n",
      "Epoch 30/32\n",
      "379/379 [==============================] - 5s 13ms/step - loss: 0.0049 - val_loss: 0.0069\n",
      "Epoch 31/32\n",
      "379/379 [==============================] - 5s 12ms/step - loss: 0.0049 - val_loss: 0.0069\n",
      "Epoch 32/32\n",
      "379/379 [==============================] - 5s 12ms/step - loss: 0.0049 - val_loss: 0.0068\n",
      "\n",
      " >>> TIMESTEP 7 =============================================================== \n",
      "\n",
      "       index_       mse  Class\n",
      "4266  59564.0  6.088809      1\n",
      "150   12084.0  1.092978      1\n",
      "3015  67253.0  0.385453      1\n",
      "2418  19573.0  0.260728      1\n",
      "3197  98014.0  0.204348      1\n",
      "2629  18877.0  0.071423      1\n",
      "1269  13629.0  0.053288      1\n",
      "3825  87724.0  0.048640      1\n",
      "1364  52514.0  0.046099      1\n",
      "696    9524.0  0.043969      1\n",
      "2048  10381.0  0.042588      1\n",
      "3225  86202.0  0.040151      1\n",
      "158   83763.0  0.037128      1\n",
      "3456  58253.0  0.036576      1\n",
      "3548  95616.0  0.034700      1\n",
      "3532  65978.0  0.033215      1\n",
      "2798  33353.0  0.032295      1\n",
      "2624  50085.0  0.032252      1\n",
      "1899   3527.0  0.027637      0\n",
      "3397  99621.0  0.026807      0\n",
      "4025  35137.0  0.025343      0\n",
      "3042   7611.0  0.022935      0\n",
      "1294  19628.0  0.022889      0\n",
      "3760  15036.0  0.021911      0\n",
      "3123  45876.0  0.021795      0\n",
      "3216  64169.0  0.017617      0\n",
      "814     764.0  0.017548      0\n",
      "3093  73280.0  0.017507      0\n",
      "3311  80453.0  0.016379      0\n",
      "3487   7913.0  0.016061      0\n",
      "600   27430.0  0.014891      0\n",
      "1421  70193.0  0.014704      0\n",
      "2616  75646.0  0.014379      0\n",
      "494   86913.0  0.014316      0\n",
      "195     875.0  0.014275      0\n",
      "101   79806.0  0.014093      0\n",
      "2129  57485.0  0.013838      0\n",
      "3304  80035.0  0.013754      0\n",
      "4072  47727.0  0.013601      0\n",
      "2983  72938.0  0.013410      0\n",
      "1149  28835.0  0.013169      0\n",
      "139   94018.0  0.013140      0\n",
      "618   98670.0  0.013114      0\n",
      "4135  88988.0  0.012900      0\n",
      "1170  76920.0  0.012856      0\n",
      "3943  92305.0  0.012850      0\n",
      "635   83612.0  0.012849      0\n",
      "1013  58835.0  0.012686      0\n",
      "2695  48219.0  0.012649      0\n",
      "3705   4986.0  0.012642      0\n",
      "        pred_0    pred_1  index_  Class\n",
      "158   0.257111  0.742889   83763      1\n",
      "3548  0.260000  0.740000   95616      1\n",
      "4120  0.300000  0.700000   12132      1\n",
      "3258  0.351667  0.648333   93674      1\n",
      "1033  0.354444  0.645556   83830      1\n",
      "2624  0.362222  0.637778   50085      1\n",
      "4304  0.366667  0.633333   14565      1\n",
      "2798  0.380111  0.619889   33353      1\n",
      "1154  0.387778  0.612222   93556      1\n",
      "2629  0.426111  0.573889   18877      1\n",
      "3364  0.440000  0.560000   83986      1\n",
      "1358  0.453333  0.546667   30438      1\n",
      "2048  0.455000  0.545000   10381      1\n",
      "4189  0.460000  0.540000   83798      1\n",
      "1257  0.469778  0.530222   45384      1\n",
      "3825  0.480079  0.519921   87724      1\n",
      "1364  0.480667  0.519333   52514      1\n",
      "3135  0.493333  0.506667   57564      1\n",
      "3225  0.499444  0.500556   86202      1\n",
      "2652  0.500000  0.500000    4744      1\n",
      "3532  0.521746  0.478254   65978      1\n",
      "2771  0.523889  0.476111    7598      1\n",
      "1269  0.539000  0.461000   13629      1\n",
      "4266  0.549444  0.450556   59564      1\n",
      "1678  0.553333  0.446667   87602      1\n",
      "150   0.560000  0.440000   12084      1\n",
      "3015  0.562889  0.437111   67253      1\n",
      "2040  0.586667  0.413333   26418      1\n",
      "696   0.607667  0.392333    9524      1\n",
      "3197  0.618444  0.381556   98014      1\n",
      "3397  0.625079  0.374921   99621      1\n",
      "4159  0.673333  0.326667   94901      1\n",
      "3456  0.675556  0.324444   58253      1\n",
      "845   0.680000  0.320000   52472      1\n",
      "1702  0.686667  0.313333   44992      1\n",
      "2418  0.697222  0.302778   19573      1\n",
      "1294  0.706111  0.293889   19628      0\n",
      "1201  0.706667  0.293333   34190      0\n",
      "388   0.721667  0.278333   21753      0\n",
      "4176  0.733333  0.266667   79091      0\n",
      "1807  0.780000  0.220000   64747      0\n",
      "2185  0.780000  0.220000   59141      0\n",
      "1125  0.783333  0.216667   52865      0\n",
      "3013  0.786667  0.213333   85391      0\n",
      "789   0.791111  0.208889   18032      0\n",
      "3760  0.795000  0.205000   15036      0\n",
      "3123  0.795889  0.204111   45876      0\n",
      "910   0.800000  0.200000   35333      0\n",
      "2856  0.813333  0.186667   79640      0\n",
      "2396  0.818889  0.181111   34591      0\n",
      "Normal count: 21082\n",
      "Attack count: 631\n",
      "Epoch 1/32\n",
      "380/380 [==============================] - 6s 12ms/step - loss: 0.0084 - val_loss: 0.0095\n",
      "Epoch 2/32\n",
      "380/380 [==============================] - 5s 12ms/step - loss: 0.0071 - val_loss: 0.0093\n",
      "Epoch 3/32\n",
      "380/380 [==============================] - 4s 12ms/step - loss: 0.0070 - val_loss: 0.0092\n",
      "Epoch 4/32\n",
      "380/380 [==============================] - 4s 11ms/step - loss: 0.0070 - val_loss: 0.0093\n",
      "Epoch 5/32\n",
      "380/380 [==============================] - 4s 12ms/step - loss: 0.0070 - val_loss: 0.0092\n",
      "Epoch 6/32\n",
      "380/380 [==============================] - 4s 12ms/step - loss: 0.0069 - val_loss: 0.0090\n",
      "Epoch 7/32\n",
      "380/380 [==============================] - 4s 11ms/step - loss: 0.0066 - val_loss: 0.0088\n",
      "Epoch 8/32\n",
      "380/380 [==============================] - 4s 12ms/step - loss: 0.0065 - val_loss: 0.0087\n",
      "Epoch 9/32\n",
      "380/380 [==============================] - 5s 12ms/step - loss: 0.0062 - val_loss: 0.0081\n",
      "Epoch 10/32\n",
      "380/380 [==============================] - 5s 12ms/step - loss: 0.0058 - val_loss: 0.0081\n",
      "Epoch 11/32\n",
      "380/380 [==============================] - 5s 12ms/step - loss: 0.0057 - val_loss: 0.0079\n",
      "Epoch 12/32\n",
      "380/380 [==============================] - 5s 12ms/step - loss: 0.0057 - val_loss: 0.0079\n",
      "Epoch 13/32\n",
      "380/380 [==============================] - 4s 12ms/step - loss: 0.0057 - val_loss: 0.0078\n",
      "Epoch 14/32\n",
      "380/380 [==============================] - 5s 12ms/step - loss: 0.0056 - val_loss: 0.0078\n",
      "Epoch 15/32\n",
      "380/380 [==============================] - 4s 12ms/step - loss: 0.0056 - val_loss: 0.0078\n",
      "Epoch 16/32\n",
      "380/380 [==============================] - 4s 11ms/step - loss: 0.0055 - val_loss: 0.0077\n",
      "Epoch 17/32\n",
      "380/380 [==============================] - 4s 12ms/step - loss: 0.0054 - val_loss: 0.0076\n",
      "Epoch 18/32\n",
      "380/380 [==============================] - 4s 12ms/step - loss: 0.0053 - val_loss: 0.0075\n",
      "Epoch 19/32\n",
      "380/380 [==============================] - 4s 11ms/step - loss: 0.0052 - val_loss: 0.0073\n",
      "Epoch 20/32\n",
      "380/380 [==============================] - 4s 11ms/step - loss: 0.0051 - val_loss: 0.0073\n",
      "Epoch 21/32\n",
      "380/380 [==============================] - 4s 11ms/step - loss: 0.0050 - val_loss: 0.0072\n",
      "Epoch 22/32\n",
      "380/380 [==============================] - 4s 11ms/step - loss: 0.0049 - val_loss: 0.0071\n",
      "Epoch 23/32\n",
      "380/380 [==============================] - 5s 12ms/step - loss: 0.0048 - val_loss: 0.0070\n",
      "Epoch 24/32\n",
      "380/380 [==============================] - 5s 13ms/step - loss: 0.0048 - val_loss: 0.0070\n",
      "Epoch 25/32\n",
      "380/380 [==============================] - 4s 12ms/step - loss: 0.0047 - val_loss: 0.0069\n",
      "Epoch 26/32\n",
      "380/380 [==============================] - 5s 12ms/step - loss: 0.0047 - val_loss: 0.0069\n",
      "Epoch 27/32\n",
      "380/380 [==============================] - 5s 12ms/step - loss: 0.0047 - val_loss: 0.0068\n",
      "Epoch 28/32\n",
      "380/380 [==============================] - 5s 12ms/step - loss: 0.0046 - val_loss: 0.0068\n",
      "Epoch 29/32\n",
      "380/380 [==============================] - 5s 12ms/step - loss: 0.0046 - val_loss: 0.0068\n",
      "Epoch 30/32\n",
      "380/380 [==============================] - 4s 12ms/step - loss: 0.0046 - val_loss: 0.0068\n",
      "Epoch 31/32\n",
      "380/380 [==============================] - 5s 12ms/step - loss: 0.0046 - val_loss: 0.0068\n",
      "Epoch 32/32\n",
      "380/380 [==============================] - 4s 12ms/step - loss: 0.0046 - val_loss: 0.0067\n",
      "\n",
      " >>> TIMESTEP 8 =============================================================== \n",
      "\n",
      "       index_       mse  Class\n",
      "3591  63191.0  0.596641      1\n",
      "3721  79451.0  0.434866      1\n",
      "900   11223.0  0.095726      1\n",
      "3832  24831.0  0.064253      1\n",
      "2158  48525.0  0.061424      1\n",
      "339   95606.0  0.059209      1\n",
      "2817  23721.0  0.055786      1\n",
      "3081  31616.0  0.047557      1\n",
      "3382  15451.0  0.045199      1\n",
      "3338  81114.0  0.045112      1\n",
      "3106  18998.0  0.041881      1\n",
      "1997  47848.0  0.039303      1\n",
      "3258  48949.0  0.039169      1\n",
      "852   66035.0  0.037670      1\n",
      "3131  27221.0  0.036488      1\n",
      "856   64620.0  0.036455      1\n",
      "4205  74979.0  0.036144      1\n",
      "2962  69412.0  0.035854      1\n",
      "3225  90013.0  0.035585      1\n",
      "688   69495.0  0.035477      1\n",
      "3732  12587.0  0.032251      1\n",
      "2595  31545.0  0.030577      1\n",
      "3790  73490.0  0.030103      1\n",
      "312   75897.0  0.029512      0\n",
      "1507  60853.0  0.028566      0\n",
      "875   91524.0  0.027347      0\n",
      "3699  16851.0  0.026410      0\n",
      "3717  14263.0  0.024210      0\n",
      "1095   7002.0  0.024030      0\n",
      "4109  45599.0  0.023454      0\n",
      "3904  75451.0  0.022887      0\n",
      "3383  20363.0  0.021747      0\n",
      "3226  93676.0  0.021536      0\n",
      "3487  83210.0  0.020920      0\n",
      "4106  43762.0  0.020209      0\n",
      "1588  65772.0  0.019431      0\n",
      "4154  33737.0  0.018734      0\n",
      "2152  98428.0  0.018384      0\n",
      "370   12246.0  0.017631      0\n",
      "884    6191.0  0.015778      0\n",
      "97    33291.0  0.015662      0\n",
      "1589  11624.0  0.014874      0\n",
      "4231  55749.0  0.014670      0\n",
      "232   71197.0  0.014539      0\n",
      "1461  91224.0  0.014526      0\n",
      "3534  53722.0  0.013584      0\n",
      "593   87034.0  0.012735      0\n",
      "3452  57664.0  0.012373      0\n",
      "1892  62538.0  0.012039      0\n",
      "4111  75306.0  0.011999      0\n",
      "        pred_0    pred_1  index_  Class\n",
      "339   0.099444  0.900556   95606      1\n",
      "3131  0.138889  0.861111   27221      1\n",
      "852   0.180000  0.820000   66035      1\n",
      "2962  0.188889  0.811111   69412      1\n",
      "688   0.203333  0.796667   69495      1\n",
      "2520  0.206667  0.793333   26815      1\n",
      "4205  0.210778  0.789222   74979      1\n",
      "158   0.220000  0.780000   51182      1\n",
      "856   0.229333  0.770667   64620      1\n",
      "2595  0.278333  0.721667   31545      1\n",
      "2430  0.280000  0.720000   43755      1\n",
      "3832  0.311111  0.688889   24831      1\n",
      "3879  0.313333  0.686667    2523      1\n",
      "2158  0.321444  0.678556   48525      1\n",
      "3338  0.350000  0.650000   81114      1\n",
      "521   0.357778  0.642222   96085      1\n",
      "3106  0.396667  0.603333   18998      1\n",
      "3382  0.400444  0.599556   15451      1\n",
      "3225  0.426111  0.573889   90013      1\n",
      "993   0.436667  0.563333   72725      1\n",
      "2206  0.446667  0.553333   24201      1\n",
      "3904  0.473333  0.526667   75451      1\n",
      "3591  0.488333  0.511667   63191      1\n",
      "1997  0.510000  0.490000   47848      1\n",
      "3258  0.511333  0.488667   48949      1\n",
      "1507  0.515111  0.484889   60853      1\n",
      "3721  0.535000  0.465000   79451      1\n",
      "3081  0.541111  0.458889   31616      1\n",
      "2817  0.560111  0.439889   23721      1\n",
      "3717  0.597667  0.402333   14263      1\n",
      "312   0.604444  0.395556   75897      1\n",
      "3790  0.605556  0.394444   73490      1\n",
      "875   0.635556  0.364444   91524      1\n",
      "3699  0.635889  0.364111   16851      1\n",
      "3732  0.690000  0.310000   12587      1\n",
      "1588  0.700000  0.300000   65772      0\n",
      "900   0.715000  0.285000   11223      0\n",
      "3226  0.745556  0.254444   93676      0\n",
      "3202  0.753333  0.246667   86507      0\n",
      "1095  0.759222  0.240778    7002      0\n",
      "4109  0.762778  0.237222   45599      0\n",
      "3520  0.766667  0.233333   68739      0\n",
      "1910  0.773333  0.226667   55354      0\n",
      "2026  0.778889  0.221111   97018      0\n",
      "729   0.806667  0.193333   90196      0\n",
      "1059  0.806667  0.193333   51703      0\n",
      "3179  0.806667  0.193333   69925      0\n",
      "3519  0.813333  0.186667   54383      0\n",
      "1089  0.813333  0.186667   52672      0\n",
      "893   0.813333  0.186667    3594      0\n",
      "Normal count: 21124\n",
      "Attack count: 689\n",
      "Epoch 1/32\n",
      "381/381 [==============================] - 6s 13ms/step - loss: 0.0085 - val_loss: 0.0098\n",
      "Epoch 2/32\n",
      "381/381 [==============================] - 5s 13ms/step - loss: 0.0071 - val_loss: 0.0096\n",
      "Epoch 3/32\n",
      "381/381 [==============================] - 4s 11ms/step - loss: 0.0070 - val_loss: 0.0095\n",
      "Epoch 4/32\n",
      "381/381 [==============================] - 4s 11ms/step - loss: 0.0069 - val_loss: 0.0092\n",
      "Epoch 5/32\n",
      "381/381 [==============================] - 4s 12ms/step - loss: 0.0064 - val_loss: 0.0086\n",
      "Epoch 6/32\n",
      "381/381 [==============================] - 4s 12ms/step - loss: 0.0060 - val_loss: 0.0084\n",
      "Epoch 7/32\n",
      "381/381 [==============================] - 4s 12ms/step - loss: 0.0059 - val_loss: 0.0083\n",
      "Epoch 8/32\n",
      "381/381 [==============================] - 5s 12ms/step - loss: 0.0058 - val_loss: 0.0083\n",
      "Epoch 9/32\n",
      "381/381 [==============================] - 5s 12ms/step - loss: 0.0057 - val_loss: 0.0082\n",
      "Epoch 10/32\n",
      "381/381 [==============================] - 4s 11ms/step - loss: 0.0057 - val_loss: 0.0082\n",
      "Epoch 11/32\n",
      "381/381 [==============================] - 4s 12ms/step - loss: 0.0057 - val_loss: 0.0082\n",
      "Epoch 12/32\n",
      "381/381 [==============================] - 5s 12ms/step - loss: 0.0057 - val_loss: 0.0081\n",
      "Epoch 13/32\n",
      "381/381 [==============================] - 4s 11ms/step - loss: 0.0056 - val_loss: 0.0081\n",
      "Epoch 14/32\n",
      "381/381 [==============================] - 4s 12ms/step - loss: 0.0056 - val_loss: 0.0081\n",
      "Epoch 15/32\n",
      "381/381 [==============================] - 4s 12ms/step - loss: 0.0056 - val_loss: 0.0080\n",
      "Epoch 16/32\n",
      "381/381 [==============================] - 4s 11ms/step - loss: 0.0056 - val_loss: 0.0080\n",
      "Epoch 17/32\n",
      "381/381 [==============================] - 4s 11ms/step - loss: 0.0055 - val_loss: 0.0080\n",
      "Epoch 18/32\n",
      "381/381 [==============================] - 4s 11ms/step - loss: 0.0055 - val_loss: 0.0079\n",
      "Epoch 19/32\n",
      "381/381 [==============================] - 4s 11ms/step - loss: 0.0055 - val_loss: 0.0079\n",
      "Epoch 20/32\n",
      "381/381 [==============================] - 4s 11ms/step - loss: 0.0054 - val_loss: 0.0078\n",
      "Epoch 21/32\n",
      "381/381 [==============================] - 4s 12ms/step - loss: 0.0053 - val_loss: 0.0077\n",
      "Epoch 22/32\n",
      "381/381 [==============================] - 5s 12ms/step - loss: 0.0052 - val_loss: 0.0077\n",
      "Epoch 23/32\n",
      "381/381 [==============================] - 5s 13ms/step - loss: 0.0052 - val_loss: 0.0076\n",
      "Epoch 24/32\n",
      "381/381 [==============================] - 5s 13ms/step - loss: 0.0051 - val_loss: 0.0076\n",
      "Epoch 25/32\n",
      "381/381 [==============================] - 5s 13ms/step - loss: 0.0051 - val_loss: 0.0076\n",
      "Epoch 26/32\n",
      "381/381 [==============================] - 5s 13ms/step - loss: 0.0050 - val_loss: 0.0075\n",
      "Epoch 27/32\n",
      "381/381 [==============================] - 5s 12ms/step - loss: 0.0050 - val_loss: 0.0075\n",
      "Epoch 28/32\n",
      "381/381 [==============================] - 5s 12ms/step - loss: 0.0050 - val_loss: 0.0074\n",
      "Epoch 29/32\n",
      "381/381 [==============================] - 5s 12ms/step - loss: 0.0050 - val_loss: 0.0074\n",
      "Epoch 30/32\n",
      "381/381 [==============================] - 5s 12ms/step - loss: 0.0049 - val_loss: 0.0074\n",
      "Epoch 31/32\n",
      "381/381 [==============================] - 5s 12ms/step - loss: 0.0049 - val_loss: 0.0074\n",
      "Epoch 32/32\n",
      "381/381 [==============================] - 4s 12ms/step - loss: 0.0049 - val_loss: 0.0074\n",
      "\n",
      " >>> TIMESTEP 9 =============================================================== \n",
      "\n",
      "       index_       mse  Class\n",
      "2904  48934.0  0.929936      1\n",
      "2912  92337.0  0.150756      1\n",
      "1817  43231.0  0.098030      1\n",
      "1096  78053.0  0.059745      1\n",
      "197   74196.0  0.057120      1\n",
      "754   50117.0  0.052902      1\n",
      "3951  61679.0  0.051192      1\n",
      "2511  49104.0  0.047149      1\n",
      "3022  82943.0  0.046156      1\n",
      "2296  95567.0  0.043808      1\n",
      "740   19822.0  0.042317      1\n",
      "4018  47764.0  0.041986      1\n",
      "1800  54169.0  0.040952      1\n",
      "186   92928.0  0.040859      1\n",
      "3714  55475.0  0.038547      1\n",
      "2961  11933.0  0.037401      1\n",
      "3819  21422.0  0.036434      1\n",
      "1423  28804.0  0.036051      1\n",
      "561   67976.0  0.035377      1\n",
      "1645  75422.0  0.034706      1\n",
      "3788  74109.0  0.032176      1\n",
      "3302   9637.0  0.031203      1\n",
      "3749  95709.0  0.030507      1\n",
      "1849  98005.0  0.028973      0\n",
      "2105  93828.0  0.028964      0\n",
      "3133  65094.0  0.028586      0\n",
      "1433  88480.0  0.028394      0\n",
      "2468  95582.0  0.028244      0\n",
      "354    5754.0  0.025935      0\n",
      "1379  26363.0  0.025323      0\n",
      "2181  28770.0  0.024033      0\n",
      "988   63281.0  0.023639      0\n",
      "2647  56402.0  0.023605      0\n",
      "3717  40132.0  0.023513      0\n",
      "3248  42836.0  0.023318      0\n",
      "2770  44367.0  0.023055      0\n",
      "1392   7214.0  0.022617      0\n",
      "3607  11586.0  0.022572      0\n",
      "1181  63321.0  0.022015      0\n",
      "949   19632.0  0.020724      0\n",
      "555    4644.0  0.020145      0\n",
      "246   74350.0  0.018318      0\n",
      "1038  50295.0  0.017068      0\n",
      "844   59033.0  0.015992      0\n",
      "1679  55698.0  0.015926      0\n",
      "757   76196.0  0.015625      0\n",
      "1533  63858.0  0.015589      0\n",
      "688    4448.0  0.015224      0\n",
      "318    8238.0  0.014575      0\n",
      "2620  91004.0  0.014444      0\n",
      "        pred_0    pred_1  index_  Class\n",
      "2296  0.078333  0.921667   95567      1\n",
      "1958  0.133333  0.866667    2553      1\n",
      "1096  0.138333  0.861667   78053      1\n",
      "1423  0.206667  0.793333   28804      1\n",
      "3788  0.220540  0.779460   74109      1\n",
      "563   0.226667  0.773333   84627      1\n",
      "4018  0.260000  0.740000   47764      1\n",
      "1272  0.286667  0.713333   45738      1\n",
      "3749  0.290000  0.710000   95709      1\n",
      "3022  0.316444  0.683556   82943      1\n",
      "1800  0.317222  0.682778   54169      1\n",
      "999   0.391667  0.608333   18410      1\n",
      "3819  0.396667  0.603333   21422      1\n",
      "3951  0.397222  0.602778   61679      1\n",
      "2468  0.415222  0.584778   95582      1\n",
      "2113  0.453333  0.546667   31831      1\n",
      "3302  0.453333  0.546667    9637      1\n",
      "1849  0.453889  0.546111   98005      1\n",
      "740   0.456667  0.543333   19822      1\n",
      "2961  0.472667  0.527333   11933      1\n",
      "1645  0.475000  0.525000   75422      1\n",
      "3872  0.480000  0.520000   88685      1\n",
      "754   0.493889  0.506111   50117      1\n",
      "2904  0.503778  0.496222   48934      1\n",
      "561   0.516667  0.483333   67976      1\n",
      "2912  0.527429  0.472571   92337      1\n",
      "186   0.533889  0.466111   92928      1\n",
      "3714  0.549111  0.450889   55475      1\n",
      "2738  0.553333  0.446667   87840      1\n",
      "2511  0.573333  0.426667   49104      1\n",
      "758   0.573333  0.426667   62377      1\n",
      "197   0.590667  0.409333   74196      1\n",
      "2105  0.600778  0.399222   93828      1\n",
      "928   0.606667  0.393333   72821      1\n",
      "2940  0.607778  0.392222   24805      1\n",
      "988   0.634333  0.365667   63281      1\n",
      "1662  0.636667  0.363333   66048      1\n",
      "3045  0.636667  0.363333   64691      1\n",
      "1379  0.637667  0.362333   26363      1\n",
      "512   0.639556  0.360444   87416      1\n",
      "1392  0.668889  0.331111    7214      1\n",
      "1817  0.669889  0.330111   43231      1\n",
      "3003  0.683333  0.316667   84105      1\n",
      "2770  0.689444  0.310556   44367      1\n",
      "1433  0.696667  0.303333   88480      1\n",
      "3133  0.710000  0.290000   65094      0\n",
      "1898  0.713333  0.286667   92689      0\n",
      "3898  0.722143  0.277857   96841      0\n",
      "3423  0.727778  0.272222   52578      0\n",
      "3496  0.730556  0.269444   27125      0\n",
      "Normal count: 21156\n",
      "Attack count: 757\n",
      "Epoch 1/32\n",
      "381/381 [==============================] - 6s 12ms/step - loss: 0.0083 - val_loss: 0.0099\n",
      "Epoch 2/32\n",
      "381/381 [==============================] - 5s 12ms/step - loss: 0.0071 - val_loss: 0.0098\n",
      "Epoch 3/32\n",
      "381/381 [==============================] - 5s 13ms/step - loss: 0.0070 - val_loss: 0.0097\n",
      "Epoch 4/32\n",
      "381/381 [==============================] - 5s 12ms/step - loss: 0.0069 - val_loss: 0.0094\n",
      "Epoch 5/32\n",
      "381/381 [==============================] - 5s 13ms/step - loss: 0.0063 - val_loss: 0.0087\n",
      "Epoch 6/32\n",
      "381/381 [==============================] - 5s 13ms/step - loss: 0.0059 - val_loss: 0.0085\n",
      "Epoch 7/32\n",
      "381/381 [==============================] - 5s 12ms/step - loss: 0.0058 - val_loss: 0.0085\n",
      "Epoch 8/32\n",
      "381/381 [==============================] - 5s 12ms/step - loss: 0.0058 - val_loss: 0.0085\n",
      "Epoch 9/32\n",
      "381/381 [==============================] - 5s 12ms/step - loss: 0.0057 - val_loss: 0.0084\n",
      "Epoch 10/32\n",
      "381/381 [==============================] - 5s 12ms/step - loss: 0.0057 - val_loss: 0.0084\n",
      "Epoch 11/32\n",
      "381/381 [==============================] - 5s 12ms/step - loss: 0.0057 - val_loss: 0.0084\n",
      "Epoch 12/32\n",
      "381/381 [==============================] - 5s 12ms/step - loss: 0.0056 - val_loss: 0.0083\n",
      "Epoch 13/32\n",
      "381/381 [==============================] - 5s 12ms/step - loss: 0.0056 - val_loss: 0.0083\n",
      "Epoch 14/32\n",
      "381/381 [==============================] - 5s 12ms/step - loss: 0.0056 - val_loss: 0.0083\n",
      "Epoch 15/32\n",
      "381/381 [==============================] - 5s 13ms/step - loss: 0.0056 - val_loss: 0.0083\n",
      "Epoch 16/32\n",
      "381/381 [==============================] - 5s 12ms/step - loss: 0.0056 - val_loss: 0.0082\n",
      "Epoch 17/32\n",
      "381/381 [==============================] - 5s 12ms/step - loss: 0.0055 - val_loss: 0.0082\n",
      "Epoch 18/32\n",
      "381/381 [==============================] - 5s 12ms/step - loss: 0.0055 - val_loss: 0.0082\n",
      "Epoch 19/32\n",
      "381/381 [==============================] - 5s 13ms/step - loss: 0.0055 - val_loss: 0.0082\n",
      "Epoch 20/32\n",
      "381/381 [==============================] - 5s 12ms/step - loss: 0.0054 - val_loss: 0.0080\n",
      "Epoch 21/32\n",
      "381/381 [==============================] - 5s 12ms/step - loss: 0.0053 - val_loss: 0.0080\n",
      "Epoch 22/32\n",
      "381/381 [==============================] - 5s 12ms/step - loss: 0.0052 - val_loss: 0.0079\n",
      "Epoch 23/32\n",
      "381/381 [==============================] - 5s 12ms/step - loss: 0.0052 - val_loss: 0.0078\n",
      "Epoch 24/32\n",
      "381/381 [==============================] - 4s 12ms/step - loss: 0.0051 - val_loss: 0.0078\n",
      "Epoch 25/32\n",
      "381/381 [==============================] - 5s 12ms/step - loss: 0.0051 - val_loss: 0.0078\n",
      "Epoch 26/32\n",
      "381/381 [==============================] - 4s 12ms/step - loss: 0.0051 - val_loss: 0.0077\n",
      "Epoch 27/32\n",
      "381/381 [==============================] - 4s 12ms/step - loss: 0.0050 - val_loss: 0.0077\n",
      "Epoch 28/32\n",
      "381/381 [==============================] - 5s 12ms/step - loss: 0.0050 - val_loss: 0.0077\n",
      "Epoch 29/32\n",
      "381/381 [==============================] - 5s 12ms/step - loss: 0.0050 - val_loss: 0.0077\n",
      "Epoch 30/32\n",
      "381/381 [==============================] - 4s 12ms/step - loss: 0.0050 - val_loss: 0.0077\n",
      "Epoch 31/32\n",
      "381/381 [==============================] - 4s 12ms/step - loss: 0.0050 - val_loss: 0.0077\n",
      "Epoch 32/32\n",
      "381/381 [==============================] - 4s 12ms/step - loss: 0.0049 - val_loss: 0.0076\n",
      "\n",
      " >>> TIMESTEP 10 =============================================================== \n",
      "\n",
      "       index_       mse  Class\n",
      "2321  92587.0  0.122028      1\n",
      "1131  43525.0  0.064873      1\n",
      "2406  68228.0  0.060595      1\n",
      "1635  37349.0  0.042984      1\n",
      "758   76660.0  0.041279      1\n",
      "3838  36019.0  0.041003      1\n",
      "2521  98016.0  0.040624      1\n",
      "3792  65500.0  0.038523      1\n",
      "1801  14229.0  0.037126      1\n",
      "1169  29099.0  0.033515      1\n",
      "2166  57309.0  0.033132      1\n",
      "64    13031.0  0.031384      1\n",
      "1208  25410.0  0.030728      1\n",
      "253   36787.0  0.030409      1\n",
      "88    98901.0  0.029579      0\n",
      "1596  74075.0  0.028458      0\n",
      "2823  10004.0  0.028385      0\n",
      "73    10555.0  0.027976      0\n",
      "310   29192.0  0.027751      0\n",
      "2941  16893.0  0.026838      0\n",
      "1636  81341.0  0.025613      0\n",
      "1664  81461.0  0.025571      0\n",
      "1217  94335.0  0.024828      0\n",
      "3338  24986.0  0.024285      0\n",
      "1666  45388.0  0.024224      0\n",
      "82    56092.0  0.023476      0\n",
      "1850  19713.0  0.023391      0\n",
      "3212  60766.0  0.023308      0\n",
      "1051  52471.0  0.022718      0\n",
      "1123  66799.0  0.021949      0\n",
      "2398  49088.0  0.018917      0\n",
      "3511  38447.0  0.018770      0\n",
      "518   22025.0  0.018577      0\n",
      "2263  52008.0  0.017994      0\n",
      "5     40124.0  0.017913      0\n",
      "3203  38170.0  0.016171      0\n",
      "3851  96301.0  0.016007      0\n",
      "500   19385.0  0.015997      0\n",
      "2942  42014.0  0.015198      0\n",
      "2000  45362.0  0.014753      0\n",
      "2445   3665.0  0.014492      0\n",
      "3924  77876.0  0.014354      0\n",
      "3043  72103.0  0.014296      0\n",
      "1897  46870.0  0.013807      0\n",
      "725   99188.0  0.013492      0\n",
      "2818  41020.0  0.013390      0\n",
      "3867  79339.0  0.013027      0\n",
      "2128  94625.0  0.012796      0\n",
      "344   60046.0  0.012748      0\n",
      "3146  48823.0  0.012666      0\n",
      "        pred_0    pred_1  index_  Class\n",
      "2166  0.100000  0.900000   57309      1\n",
      "1517  0.146667  0.853333   27840      1\n",
      "2616  0.147778  0.852222   93519      1\n",
      "2113  0.160000  0.840000    3671      1\n",
      "758   0.196667  0.803333   76660      1\n",
      "3792  0.240000  0.760000   65500      1\n",
      "1801  0.241667  0.758333   14229      1\n",
      "3838  0.266667  0.733333   36019      1\n",
      "1493  0.280000  0.720000   74388      1\n",
      "2521  0.314444  0.685556   98016      1\n",
      "2862  0.327778  0.672222   62378      1\n",
      "88    0.348889  0.651111   98901      1\n",
      "1208  0.370000  0.630000   25410      1\n",
      "2406  0.383333  0.616667   68228      1\n",
      "1009  0.393333  0.606667   64658      1\n",
      "1596  0.413333  0.586667   74075      1\n",
      "3615  0.426667  0.573333   52613      1\n",
      "64    0.496778  0.503222   13031      1\n",
      "253   0.502968  0.497032   36787      1\n",
      "1635  0.509444  0.490556   37349      1\n",
      "1169  0.512222  0.487778   29099      1\n",
      "1131  0.516111  0.483889   43525      1\n",
      "2321  0.541333  0.458667   92587      1\n",
      "2300  0.640000  0.360000   42105      1\n",
      "1666  0.666667  0.333333   45388      1\n",
      "1636  0.670556  0.329444   81341      1\n",
      "2261  0.693333  0.306667   24574      1\n",
      "1051  0.694921  0.305079   52471      1\n",
      "1158  0.706667  0.293333    2472      0\n",
      "2119  0.713333  0.286667   15679      0\n",
      "2470  0.740000  0.260000   93832      0\n",
      "1664  0.748444  0.251556   81461      0\n",
      "2941  0.749667  0.250333   16893      0\n",
      "387   0.773333  0.226667    5696      0\n",
      "1102  0.787333  0.212667   28965      0\n",
      "1378  0.790000  0.210000   39854      0\n",
      "2708  0.793333  0.206667   18616      0\n",
      "2823  0.795079  0.204921   10004      0\n",
      "1730  0.796667  0.203333   36833      0\n",
      "254   0.798333  0.201667   99975      0\n",
      "2885  0.800000  0.200000   57899      0\n",
      "3094  0.802667  0.197333   85462      0\n",
      "243   0.805444  0.194556   48773      0\n",
      "1570  0.806667  0.193333   28240      0\n",
      "1720  0.806667  0.193333   91623      0\n",
      "3669  0.811667  0.188333   84332      0\n",
      "82    0.812222  0.187778   56092      0\n",
      "1048  0.813333  0.186667   68527      0\n",
      "2233  0.813333  0.186667   68485      0\n",
      "3616  0.813333  0.186667   92059      0\n",
      "Normal count: 21214\n",
      "Attack count: 799\n",
      "Epoch 1/32\n",
      "382/382 [==============================] - 6s 12ms/step - loss: 0.0085 - val_loss: 0.0102\n",
      "Epoch 2/32\n",
      "382/382 [==============================] - 4s 11ms/step - loss: 0.0071 - val_loss: 0.0101\n",
      "Epoch 3/32\n",
      "382/382 [==============================] - 4s 11ms/step - loss: 0.0070 - val_loss: 0.0100\n",
      "Epoch 4/32\n",
      "382/382 [==============================] - 4s 11ms/step - loss: 0.0069 - val_loss: 0.0098\n",
      "Epoch 5/32\n",
      "382/382 [==============================] - 6s 16ms/step - loss: 0.0066 - val_loss: 0.0095\n",
      "Epoch 6/32\n",
      "382/382 [==============================] - 5s 13ms/step - loss: 0.0065 - val_loss: 0.0094\n",
      "Epoch 7/32\n",
      "382/382 [==============================] - 4s 12ms/step - loss: 0.0064 - val_loss: 0.0093\n",
      "Epoch 8/32\n",
      "382/382 [==============================] - 6s 17ms/step - loss: 0.0061 - val_loss: 0.0088\n",
      "Epoch 9/32\n",
      "382/382 [==============================] - 6s 16ms/step - loss: 0.0058 - val_loss: 0.0087\n",
      "Epoch 10/32\n",
      "382/382 [==============================] - 5s 13ms/step - loss: 0.0057 - val_loss: 0.0086\n",
      "Epoch 11/32\n",
      "382/382 [==============================] - 5s 13ms/step - loss: 0.0057 - val_loss: 0.0086\n",
      "Epoch 12/32\n",
      "382/382 [==============================] - 5s 13ms/step - loss: 0.0057 - val_loss: 0.0086\n",
      "Epoch 13/32\n",
      "382/382 [==============================] - 5s 13ms/step - loss: 0.0056 - val_loss: 0.0086\n",
      "Epoch 14/32\n",
      "382/382 [==============================] - 5s 12ms/step - loss: 0.0056 - val_loss: 0.0086\n",
      "Epoch 15/32\n",
      "382/382 [==============================] - 5s 12ms/step - loss: 0.0056 - val_loss: 0.0085\n",
      "Epoch 16/32\n",
      "382/382 [==============================] - 5s 13ms/step - loss: 0.0055 - val_loss: 0.0084\n",
      "Epoch 17/32\n",
      "382/382 [==============================] - 4s 12ms/step - loss: 0.0054 - val_loss: 0.0084\n",
      "Epoch 18/32\n",
      "382/382 [==============================] - 5s 12ms/step - loss: 0.0053 - val_loss: 0.0083\n",
      "Epoch 19/32\n",
      "382/382 [==============================] - 5s 13ms/step - loss: 0.0053 - val_loss: 0.0082\n",
      "Epoch 20/32\n",
      "382/382 [==============================] - 5s 12ms/step - loss: 0.0052 - val_loss: 0.0082\n",
      "Epoch 21/32\n",
      "382/382 [==============================] - 5s 12ms/step - loss: 0.0052 - val_loss: 0.0081\n",
      "Epoch 22/32\n",
      "382/382 [==============================] - 5s 14ms/step - loss: 0.0051 - val_loss: 0.0081\n",
      "Epoch 23/32\n",
      "382/382 [==============================] - 5s 13ms/step - loss: 0.0051 - val_loss: 0.0080\n",
      "Epoch 24/32\n",
      "382/382 [==============================] - 5s 13ms/step - loss: 0.0050 - val_loss: 0.0079\n",
      "Epoch 25/32\n",
      "382/382 [==============================] - 5s 12ms/step - loss: 0.0049 - val_loss: 0.0079\n",
      "Epoch 26/32\n",
      "382/382 [==============================] - 5s 13ms/step - loss: 0.0049 - val_loss: 0.0078\n",
      "Epoch 27/32\n",
      "382/382 [==============================] - 5s 14ms/step - loss: 0.0048 - val_loss: 0.0078\n",
      "Epoch 28/32\n",
      "382/382 [==============================] - 5s 14ms/step - loss: 0.0048 - val_loss: 0.0078\n",
      "Epoch 29/32\n",
      "382/382 [==============================] - 5s 13ms/step - loss: 0.0048 - val_loss: 0.0077\n",
      "Epoch 30/32\n",
      "382/382 [==============================] - 5s 12ms/step - loss: 0.0047 - val_loss: 0.0077\n",
      "Epoch 31/32\n",
      "382/382 [==============================] - 5s 14ms/step - loss: 0.0047 - val_loss: 0.0077\n",
      "Epoch 32/32\n",
      "382/382 [==============================] - 5s 14ms/step - loss: 0.0047 - val_loss: 0.0077\n",
      "\n",
      " >>> TIMESTEP 11 =============================================================== \n",
      "\n",
      "       index_       mse  Class\n",
      "2031  17764.0  0.160047      1\n",
      "3570  58256.0  0.101778      1\n",
      "3176  21497.0  0.077307      1\n",
      "3028  70739.0  0.072453      1\n",
      "682   93572.0  0.069190      1\n",
      "3019  75418.0  0.060333      1\n",
      "2189  34624.0  0.060079      1\n",
      "993    4740.0  0.057681      1\n",
      "1114  83252.0  0.056627      1\n",
      "3056   6223.0  0.045518      1\n",
      "185   98001.0  0.043856      1\n",
      "169   76612.0  0.042905      1\n",
      "3247  34486.0  0.042559      1\n",
      "2822  72166.0  0.040587      1\n",
      "3574  77989.0  0.040568      1\n",
      "172   14405.0  0.039317      1\n",
      "2125  61746.0  0.038131      1\n",
      "985   98848.0  0.036494      1\n",
      "2833  98335.0  0.035018      1\n",
      "2242  95583.0  0.033032      1\n",
      "3699  85967.0  0.032031      1\n",
      "298   97987.0  0.031926      1\n",
      "3133  71941.0  0.031074      1\n",
      "1976  14526.0  0.030685      1\n",
      "2362  83862.0  0.029966      0\n",
      "2494  50905.0  0.029291      0\n",
      "2385  64183.0  0.028302      0\n",
      "287   14225.0  0.027362      0\n",
      "2667  57448.0  0.026304      0\n",
      "2017  66955.0  0.026064      0\n",
      "821   29910.0  0.023254      0\n",
      "1679  30276.0  0.023177      0\n",
      "441   14723.0  0.022976      0\n",
      "1231   1426.0  0.022724      0\n",
      "2736  21521.0  0.021954      0\n",
      "280     770.0  0.021746      0\n",
      "889    1869.0  0.021275      0\n",
      "1016  31154.0  0.019585      0\n",
      "3148  14714.0  0.018274      0\n",
      "2504  31599.0  0.017598      0\n",
      "3046  47631.0  0.016776      0\n",
      "506    4121.0  0.016722      0\n",
      "422   49220.0  0.016617      0\n",
      "1439  74056.0  0.016351      0\n",
      "2850  62020.0  0.016280      0\n",
      "1199  66766.0  0.016185      0\n",
      "1007  90643.0  0.016081      0\n",
      "979    6135.0  0.015385      0\n",
      "1239  41974.0  0.015075      0\n",
      "827   47135.0  0.014668      0\n",
      "        pred_0    pred_1  index_  Class\n",
      "3019  0.116333  0.883667   75418      1\n",
      "185   0.134444  0.865556   98001      1\n",
      "3574  0.156667  0.843333   77989      1\n",
      "298   0.170000  0.830000   97987      1\n",
      "172   0.207778  0.792222   14405      1\n",
      "2242  0.223333  0.776667   95583      1\n",
      "3699  0.224444  0.775556   85967      1\n",
      "2822  0.275000  0.725000   72166      1\n",
      "882   0.298333  0.701667   32074      1\n",
      "2449  0.298333  0.701667   75479      1\n",
      "3056  0.302778  0.697222    6223      1\n",
      "811   0.304444  0.695556   39615      1\n",
      "2125  0.336667  0.663333   61746      1\n",
      "1454  0.337333  0.662667   83957      1\n",
      "169   0.359444  0.640556   76612      1\n",
      "3570  0.374667  0.625333   58256      1\n",
      "1114  0.383333  0.616667   83252      1\n",
      "3133  0.432476  0.567524   71941      1\n",
      "2494  0.441111  0.558889   50905      1\n",
      "3176  0.464571  0.535429   21497      1\n",
      "2833  0.468889  0.531111   98335      1\n",
      "2362  0.479333  0.520667   83862      1\n",
      "682   0.481333  0.518667   93572      1\n",
      "2189  0.483556  0.516444   34624      1\n",
      "3247  0.505000  0.495000   34486      1\n",
      "993   0.507778  0.492222    4740      1\n",
      "2031  0.517222  0.482778   17764      1\n",
      "25    0.526667  0.473333   88726      1\n",
      "985   0.544000  0.456000   98848      1\n",
      "1545  0.563222  0.436778   18431      1\n",
      "1976  0.565667  0.434333   14526      1\n",
      "3028  0.608889  0.391111   70739      1\n",
      "1730  0.680000  0.320000   99253      1\n",
      "1679  0.706667  0.293333   30276      0\n",
      "960   0.706667  0.293333   31993      0\n",
      "2385  0.725556  0.274444   64183      0\n",
      "80    0.733333  0.266667   34174      0\n",
      "2667  0.748889  0.251111   57448      0\n",
      "1547  0.770000  0.230000   62646      0\n",
      "796   0.773333  0.226667   38875      0\n",
      "3626  0.773333  0.226667   26231      0\n",
      "287   0.773778  0.226222   14225      0\n",
      "188   0.776667  0.223333   65055      0\n",
      "775   0.780000  0.220000   27245      0\n",
      "709   0.789222  0.210778   20113      0\n",
      "3140  0.793333  0.206667   22404      0\n",
      "1845  0.800000  0.200000   37621      0\n",
      "562   0.806667  0.193333    1625      0\n",
      "3380  0.813333  0.186667   14775      0\n",
      "2906  0.816667  0.183333     693      0\n",
      "Normal count: 21257\n",
      "Attack count: 856\n",
      "Epoch 1/32\n",
      "383/383 [==============================] - 6s 13ms/step - loss: 0.0085 - val_loss: 0.0104\n",
      "Epoch 2/32\n",
      "383/383 [==============================] - 5s 12ms/step - loss: 0.0071 - val_loss: 0.0103\n",
      "Epoch 3/32\n",
      "383/383 [==============================] - 5s 12ms/step - loss: 0.0070 - val_loss: 0.0102\n",
      "Epoch 4/32\n",
      "383/383 [==============================] - 5s 12ms/step - loss: 0.0068 - val_loss: 0.0099\n",
      "Epoch 5/32\n",
      "383/383 [==============================] - 5s 12ms/step - loss: 0.0062 - val_loss: 0.0092\n",
      "Epoch 6/32\n",
      "383/383 [==============================] - 5s 12ms/step - loss: 0.0059 - val_loss: 0.0090\n",
      "Epoch 7/32\n",
      "383/383 [==============================] - 5s 14ms/step - loss: 0.0058 - val_loss: 0.0090\n",
      "Epoch 8/32\n",
      "383/383 [==============================] - 6s 15ms/step - loss: 0.0057 - val_loss: 0.0090\n",
      "Epoch 9/32\n",
      "383/383 [==============================] - 6s 17ms/step - loss: 0.0057 - val_loss: 0.0089\n",
      "Epoch 10/32\n",
      "383/383 [==============================] - 6s 15ms/step - loss: 0.0057 - val_loss: 0.0089\n",
      "Epoch 11/32\n",
      "383/383 [==============================] - 4s 12ms/step - loss: 0.0057 - val_loss: 0.0089\n",
      "Epoch 12/32\n",
      "383/383 [==============================] - 4s 12ms/step - loss: 0.0056 - val_loss: 0.0089\n",
      "Epoch 13/32\n",
      "383/383 [==============================] - 5s 13ms/step - loss: 0.0056 - val_loss: 0.0088\n",
      "Epoch 14/32\n",
      "383/383 [==============================] - 6s 15ms/step - loss: 0.0056 - val_loss: 0.0088\n",
      "Epoch 15/32\n",
      "383/383 [==============================] - 5s 13ms/step - loss: 0.0055 - val_loss: 0.0087\n",
      "Epoch 16/32\n",
      "383/383 [==============================] - 5s 13ms/step - loss: 0.0054 - val_loss: 0.0086\n",
      "Epoch 17/32\n",
      "383/383 [==============================] - 5s 13ms/step - loss: 0.0053 - val_loss: 0.0085\n",
      "Epoch 18/32\n",
      "383/383 [==============================] - 5s 12ms/step - loss: 0.0053 - val_loss: 0.0084\n",
      "Epoch 19/32\n",
      "383/383 [==============================] - 5s 13ms/step - loss: 0.0052 - val_loss: 0.0084\n",
      "Epoch 20/32\n",
      "383/383 [==============================] - 5s 14ms/step - loss: 0.0052 - val_loss: 0.0084\n",
      "Epoch 21/32\n",
      "383/383 [==============================] - 6s 15ms/step - loss: 0.0051 - val_loss: 0.0084\n",
      "Epoch 22/32\n",
      "383/383 [==============================] - 6s 16ms/step - loss: 0.0051 - val_loss: 0.0083\n",
      "Epoch 23/32\n",
      "383/383 [==============================] - 6s 16ms/step - loss: 0.0051 - val_loss: 0.0083\n",
      "Epoch 24/32\n",
      "383/383 [==============================] - 6s 14ms/step - loss: 0.0051 - val_loss: 0.0083\n",
      "Epoch 25/32\n",
      "383/383 [==============================] - 5s 14ms/step - loss: 0.0050 - val_loss: 0.0083\n",
      "Epoch 26/32\n",
      "383/383 [==============================] - 5s 13ms/step - loss: 0.0050 - val_loss: 0.0083\n",
      "Epoch 27/32\n",
      "383/383 [==============================] - 5s 13ms/step - loss: 0.0050 - val_loss: 0.0083\n",
      "Epoch 28/32\n",
      "383/383 [==============================] - 5s 14ms/step - loss: 0.0050 - val_loss: 0.0082\n",
      "Epoch 29/32\n",
      "383/383 [==============================] - 5s 13ms/step - loss: 0.0049 - val_loss: 0.0081\n",
      "Epoch 30/32\n",
      "383/383 [==============================] - 5s 13ms/step - loss: 0.0049 - val_loss: 0.0081\n",
      "Epoch 31/32\n",
      "383/383 [==============================] - 5s 13ms/step - loss: 0.0049 - val_loss: 0.0081\n",
      "Epoch 32/32\n",
      "383/383 [==============================] - 5s 13ms/step - loss: 0.0048 - val_loss: 0.0081\n",
      "\n",
      " >>> TIMESTEP 12 =============================================================== \n",
      "\n",
      "       index_       mse  Class\n",
      "1840  45431.0  0.141707      1\n",
      "2357  51761.0  0.101303      1\n",
      "310   91261.0  0.083745      1\n",
      "3296  54881.0  0.076739      1\n",
      "961   50897.0  0.069636      1\n",
      "353   68138.0  0.053241      1\n",
      "3388  45352.0  0.051820      1\n",
      "646   33155.0  0.050885      1\n",
      "3144  74644.0  0.050444      1\n",
      "2082  74159.0  0.043613      1\n",
      "2598  15453.0  0.041717      1\n",
      "2739  31913.0  0.041216      1\n",
      "1132  49990.0  0.038966      1\n",
      "3260  95802.0  0.038614      1\n",
      "72    47698.0  0.036925      1\n",
      "591   55109.0  0.035683      1\n",
      "1917  79897.0  0.035407      1\n",
      "2656  14413.0  0.033394      1\n",
      "917   55413.0  0.032791      1\n",
      "337   14206.0  0.032233      1\n",
      "1893  66063.0  0.030282      1\n",
      "2653  75349.0  0.030019      1\n",
      "730   33566.0  0.029859      0\n",
      "632   65321.0  0.029251      0\n",
      "3066  64611.0  0.028347      0\n",
      "703   95821.0  0.028224      0\n",
      "1446  75182.0  0.027875      0\n",
      "3380  91289.0  0.027546      0\n",
      "2530  17228.0  0.026040      0\n",
      "2665  84195.0  0.024788      0\n",
      "2476  86037.0  0.024426      0\n",
      "3019  64057.0  0.024339      0\n",
      "3465  67380.0  0.024021      0\n",
      "2734  31534.0  0.023871      0\n",
      "3387   7552.0  0.022484      0\n",
      "92    97381.0  0.022308      0\n",
      "1949  20430.0  0.021485      0\n",
      "604   66393.0  0.018740      0\n",
      "2887  79625.0  0.018477      0\n",
      "778   81791.0  0.018303      0\n",
      "1208  65417.0  0.017995      0\n",
      "1978   8608.0  0.015513      0\n",
      "3203   6229.0  0.015062      0\n",
      "512   14703.0  0.014639      0\n",
      "1850  99042.0  0.014519      0\n",
      "1722  76941.0  0.013548      0\n",
      "363   30859.0  0.013162      0\n",
      "1726  20874.0  0.012737      0\n",
      "2713  11113.0  0.012637      0\n",
      "3128  19377.0  0.012116      0\n",
      "        pred_0    pred_1  index_  Class\n",
      "3260  0.021333  0.978667   95802      1\n",
      "2061  0.086667  0.913333   27745      1\n",
      "2656  0.096667  0.903333   14413      1\n",
      "646   0.120000  0.880000   33155      1\n",
      "1132  0.120000  0.880000   49990      1\n",
      "72    0.146667  0.853333   47698      1\n",
      "917   0.170000  0.830000   55413      1\n",
      "730   0.195556  0.804444   33566      1\n",
      "591   0.206667  0.793333   55109      1\n",
      "2739  0.210000  0.790000   31913      1\n",
      "997   0.231667  0.768333   45476      1\n",
      "3380  0.296667  0.703333   91289      1\n",
      "2598  0.308889  0.691111   15453      1\n",
      "1728  0.340000  0.660000   55680      1\n",
      "2082  0.342222  0.657778   74159      1\n",
      "3296  0.350000  0.650000   54881      1\n",
      "3144  0.361222  0.638778   74644      1\n",
      "1232  0.386667  0.613333   83944      1\n",
      "703   0.399778  0.600222   95821      1\n",
      "3388  0.405000  0.595000   45352      1\n",
      "2104  0.415714  0.584286   64807      1\n",
      "353   0.421111  0.578889   68138      1\n",
      "1893  0.423333  0.576667   66063      1\n",
      "1917  0.428778  0.571222   79897      1\n",
      "310   0.432056  0.567944   91261      1\n",
      "2357  0.445556  0.554444   51761      1\n",
      "337   0.452222  0.547778   14206      1\n",
      "961   0.503111  0.496889   50897      1\n",
      "2653  0.517778  0.482222   75349      1\n",
      "340   0.527667  0.472333   95928      1\n",
      "1840  0.532444  0.467556   45431      1\n",
      "634   0.564444  0.435556   19843      1\n",
      "1296  0.566667  0.433333   81592      1\n",
      "3066  0.570222  0.429778   64611      1\n",
      "632   0.576222  0.423778   65321      1\n",
      "2476  0.635667  0.364333   86037      1\n",
      "1446  0.673889  0.326111   75182      1\n",
      "2763  0.700000  0.300000   93677      0\n",
      "533   0.716667  0.283333   62278      0\n",
      "234   0.760000  0.240000   26596      0\n",
      "2557  0.766667  0.233333   93471      0\n",
      "1082  0.766667  0.233333   56241      0\n",
      "1723  0.773333  0.226667   76913      0\n",
      "2022  0.773333  0.226667   30870      0\n",
      "2665  0.776667  0.223333   84195      0\n",
      "2734  0.777222  0.222778   31534      0\n",
      "3295  0.780000  0.220000   50927      0\n",
      "545   0.786667  0.213333   18766      0\n",
      "2310  0.791111  0.208889   70862      0\n",
      "1439  0.800000  0.200000   50271      0\n",
      "Normal count: 21298\n",
      "Attack count: 915\n",
      "Epoch 1/32\n",
      "384/384 [==============================] - 7s 14ms/step - loss: 0.0082 - val_loss: 0.0107\n",
      "Epoch 2/32\n",
      "384/384 [==============================] - 5s 13ms/step - loss: 0.0071 - val_loss: 0.0105\n",
      "Epoch 3/32\n",
      "384/384 [==============================] - 5s 13ms/step - loss: 0.0070 - val_loss: 0.0105\n",
      "Epoch 4/32\n",
      "384/384 [==============================] - 5s 13ms/step - loss: 0.0068 - val_loss: 0.0100\n",
      "Epoch 5/32\n",
      "384/384 [==============================] - 5s 13ms/step - loss: 0.0064 - val_loss: 0.0097\n",
      "Epoch 6/32\n",
      "384/384 [==============================] - 5s 14ms/step - loss: 0.0062 - val_loss: 0.0095\n",
      "Epoch 7/32\n",
      "384/384 [==============================] - 5s 14ms/step - loss: 0.0059 - val_loss: 0.0094\n",
      "Epoch 8/32\n",
      "384/384 [==============================] - 5s 14ms/step - loss: 0.0058 - val_loss: 0.0093\n",
      "Epoch 9/32\n",
      "384/384 [==============================] - 5s 14ms/step - loss: 0.0058 - val_loss: 0.0092\n",
      "Epoch 10/32\n",
      "384/384 [==============================] - 5s 13ms/step - loss: 0.0057 - val_loss: 0.0092\n",
      "Epoch 11/32\n",
      "384/384 [==============================] - 5s 14ms/step - loss: 0.0057 - val_loss: 0.0092\n",
      "Epoch 12/32\n",
      "384/384 [==============================] - 5s 14ms/step - loss: 0.0056 - val_loss: 0.0091\n",
      "Epoch 13/32\n",
      "384/384 [==============================] - 5s 13ms/step - loss: 0.0056 - val_loss: 0.0090\n",
      "Epoch 14/32\n",
      "384/384 [==============================] - 5s 13ms/step - loss: 0.0054 - val_loss: 0.0088\n",
      "Epoch 15/32\n",
      "384/384 [==============================] - 5s 14ms/step - loss: 0.0053 - val_loss: 0.0087\n",
      "Epoch 16/32\n",
      "384/384 [==============================] - 5s 13ms/step - loss: 0.0052 - val_loss: 0.0086\n",
      "Epoch 17/32\n",
      "384/384 [==============================] - 5s 13ms/step - loss: 0.0052 - val_loss: 0.0086\n",
      "Epoch 18/32\n",
      "384/384 [==============================] - 5s 13ms/step - loss: 0.0052 - val_loss: 0.0086\n",
      "Epoch 19/32\n",
      "384/384 [==============================] - 5s 13ms/step - loss: 0.0051 - val_loss: 0.0086\n",
      "Epoch 20/32\n",
      "384/384 [==============================] - 5s 13ms/step - loss: 0.0051 - val_loss: 0.0086\n",
      "Epoch 21/32\n",
      "384/384 [==============================] - 5s 14ms/step - loss: 0.0051 - val_loss: 0.0085\n",
      "Epoch 22/32\n",
      "384/384 [==============================] - 5s 13ms/step - loss: 0.0051 - val_loss: 0.0085\n",
      "Epoch 23/32\n",
      "384/384 [==============================] - 5s 14ms/step - loss: 0.0051 - val_loss: 0.0084\n",
      "Epoch 24/32\n",
      "384/384 [==============================] - 5s 13ms/step - loss: 0.0050 - val_loss: 0.0085\n",
      "Epoch 25/32\n",
      "384/384 [==============================] - 6s 16ms/step - loss: 0.0050 - val_loss: 0.0085\n",
      "Epoch 26/32\n",
      "384/384 [==============================] - 8s 22ms/step - loss: 0.0050 - val_loss: 0.0084\n",
      "Epoch 27/32\n",
      "384/384 [==============================] - 6s 16ms/step - loss: 0.0050 - val_loss: 0.0084\n",
      "Epoch 28/32\n",
      "384/384 [==============================] - 6s 15ms/step - loss: 0.0050 - val_loss: 0.0085\n",
      "Epoch 29/32\n",
      "384/384 [==============================] - 6s 15ms/step - loss: 0.0050 - val_loss: 0.0084\n",
      "Epoch 30/32\n",
      "384/384 [==============================] - 6s 14ms/step - loss: 0.0050 - val_loss: 0.0084\n",
      "Epoch 31/32\n",
      "384/384 [==============================] - 5s 14ms/step - loss: 0.0050 - val_loss: 0.0084\n",
      "Epoch 32/32\n",
      "384/384 [==============================] - 5s 14ms/step - loss: 0.0050 - val_loss: 0.0084\n",
      "\n",
      " >>> TIMESTEP 13 =============================================================== \n",
      "\n",
      "       index_       mse  Class\n",
      "3207  43630.0  0.195165      1\n",
      "1330  84359.0  0.154960      1\n",
      "1033  71378.0  0.120447      1\n",
      "2411  43462.0  0.117119      1\n",
      "2712  53744.0  0.101935      1\n",
      "805    5546.0  0.094689      1\n",
      "1545  48125.0  0.082462      1\n",
      "2158  24009.0  0.082379      1\n",
      "371    5863.0  0.075120      1\n",
      "185   76580.0  0.065833      1\n",
      "489   33117.0  0.053799      1\n",
      "742   64647.0  0.047930      1\n",
      "804     232.0  0.043019      1\n",
      "2530  86078.0  0.041184      1\n",
      "1316  33593.0  0.038174      1\n",
      "1657  84432.0  0.035386      1\n",
      "47    74243.0  0.035095      1\n",
      "1360  92049.0  0.032254      1\n",
      "62    77807.0  0.031927      1\n",
      "3137  23842.0  0.031361      1\n",
      "2090  23610.0  0.026910      0\n",
      "750   65021.0  0.026749      0\n",
      "767   93049.0  0.025014      0\n",
      "2684  22744.0  0.024805      0\n",
      "2117  27821.0  0.022946      0\n",
      "3040   4383.0  0.022005      0\n",
      "1910  21833.0  0.021176      0\n",
      "3026  95777.0  0.020690      0\n",
      "1888  93435.0  0.020670      0\n",
      "1205  49909.0  0.020418      0\n",
      "1052  65962.0  0.019197      0\n",
      "891   96445.0  0.018289      0\n",
      "541   34986.0  0.016640      0\n",
      "1065  99743.0  0.016376      0\n",
      "1109  83203.0  0.015543      0\n",
      "2433  63975.0  0.014667      0\n",
      "358   63731.0  0.014119      0\n",
      "1379  50254.0  0.013287      0\n",
      "2309  23199.0  0.013148      0\n",
      "529   63857.0  0.012823      0\n",
      "3027  23000.0  0.012479      0\n",
      "309   57214.0  0.012369      0\n",
      "1359  11431.0  0.012292      0\n",
      "1438   9123.0  0.011709      0\n",
      "1947  10686.0  0.011681      0\n",
      "99     1715.0  0.011593      0\n",
      "1750  55814.0  0.011544      0\n",
      "765   63254.0  0.011461      0\n",
      "3167  38608.0  0.011410      0\n",
      "688   87041.0  0.011344      0\n",
      "        pred_0    pred_1  index_  Class\n",
      "742   0.122222  0.877778   64647      1\n",
      "3301  0.173333  0.826667   30461      1\n",
      "58    0.246667  0.753333   74634      1\n",
      "2250  0.246667  0.753333   64804      1\n",
      "2530  0.248889  0.751111   86078      1\n",
      "1316  0.252778  0.747222   33593      1\n",
      "47    0.267944  0.732056   74243      1\n",
      "1006  0.273333  0.726667   48167      1\n",
      "1893  0.296667  0.703333   74311      1\n",
      "1033  0.336667  0.663333   71378      1\n",
      "62    0.343333  0.656667   77807      1\n",
      "3216  0.366667  0.633333   24379      1\n",
      "3196  0.373333  0.626667    4858      1\n",
      "489   0.373333  0.626667   33117      1\n",
      "2712  0.382222  0.617778   53744      1\n",
      "804   0.388889  0.611111     232      1\n",
      "1545  0.391111  0.608889   48125      1\n",
      "1205  0.393333  0.606667   49909      1\n",
      "2158  0.396889  0.603111   24009      1\n",
      "3207  0.405556  0.594444   43630      1\n",
      "2841  0.411111  0.588889   81848      1\n",
      "1330  0.423889  0.576111   84359      1\n",
      "3026  0.424524  0.575476   95777      1\n",
      "371   0.461222  0.538778    5863      1\n",
      "1123  0.480000  0.520000   30360      1\n",
      "2411  0.481000  0.519000   43462      1\n",
      "1360  0.481206  0.518794   92049      1\n",
      "805   0.498889  0.501111    5546      1\n",
      "1657  0.503333  0.496667   84432      1\n",
      "185   0.523333  0.476667   76580      1\n",
      "2572  0.523778  0.476222   36611      1\n",
      "2657  0.550000  0.450000   75509      1\n",
      "3137  0.554444  0.445556   23842      1\n",
      "1077  0.631111  0.368889   93803      1\n",
      "85    0.646667  0.353333   42967      1\n",
      "2545  0.673333  0.326667   96469      1\n",
      "1881  0.706667  0.293333   72090      0\n",
      "1847  0.733333  0.266667   81249      0\n",
      "1724  0.740000  0.260000   24644      0\n",
      "2256  0.746667  0.253333   26700      0\n",
      "2129  0.754444  0.245556   96345      0\n",
      "1339  0.760000  0.240000   74612      0\n",
      "590   0.765556  0.234444   92356      0\n",
      "2390  0.786667  0.213333   37128      0\n",
      "24    0.793333  0.206667   83883      0\n",
      "1679  0.793889  0.206111   14300      0\n",
      "2439  0.805000  0.195000   96051      0\n",
      "3040  0.806667  0.193333    4383      0\n",
      "450   0.826667  0.173333   58697      0\n",
      "228   0.826667  0.173333   49230      0\n",
      "Normal count: 21342\n",
      "Attack count: 971\n",
      "Epoch 1/32\n",
      "385/385 [==============================] - 7s 14ms/step - loss: 0.0084 - val_loss: 0.0109\n",
      "Epoch 2/32\n",
      "385/385 [==============================] - 5s 14ms/step - loss: 0.0071 - val_loss: 0.0107\n",
      "Epoch 3/32\n",
      "385/385 [==============================] - 5s 13ms/step - loss: 0.0070 - val_loss: 0.0108\n",
      "Epoch 4/32\n",
      "385/385 [==============================] - 5s 13ms/step - loss: 0.0070 - val_loss: 0.0106\n",
      "Epoch 5/32\n",
      "385/385 [==============================] - 5s 14ms/step - loss: 0.0067 - val_loss: 0.0101\n",
      "Epoch 6/32\n",
      "385/385 [==============================] - 5s 14ms/step - loss: 0.0062 - val_loss: 0.0096\n",
      "Epoch 7/32\n",
      "385/385 [==============================] - 5s 14ms/step - loss: 0.0059 - val_loss: 0.0097\n",
      "Epoch 8/32\n",
      "385/385 [==============================] - 5s 13ms/step - loss: 0.0058 - val_loss: 0.0095\n",
      "Epoch 9/32\n",
      "385/385 [==============================] - 5s 14ms/step - loss: 0.0057 - val_loss: 0.0094\n",
      "Epoch 10/32\n",
      "385/385 [==============================] - 5s 14ms/step - loss: 0.0057 - val_loss: 0.0094\n",
      "Epoch 11/32\n",
      "385/385 [==============================] - 5s 13ms/step - loss: 0.0057 - val_loss: 0.0094\n",
      "Epoch 12/32\n",
      "385/385 [==============================] - 5s 14ms/step - loss: 0.0057 - val_loss: 0.0094\n",
      "Epoch 13/32\n",
      "385/385 [==============================] - 5s 14ms/step - loss: 0.0056 - val_loss: 0.0093\n",
      "Epoch 14/32\n",
      "385/385 [==============================] - 5s 14ms/step - loss: 0.0056 - val_loss: 0.0093\n",
      "Epoch 15/32\n",
      "385/385 [==============================] - 5s 14ms/step - loss: 0.0056 - val_loss: 0.0093\n",
      "Epoch 16/32\n",
      "385/385 [==============================] - 5s 14ms/step - loss: 0.0056 - val_loss: 0.0093\n",
      "Epoch 17/32\n",
      "385/385 [==============================] - 5s 14ms/step - loss: 0.0056 - val_loss: 0.0093\n",
      "Epoch 18/32\n",
      "385/385 [==============================] - 5s 14ms/step - loss: 0.0055 - val_loss: 0.0092\n",
      "Epoch 19/32\n",
      "385/385 [==============================] - 5s 14ms/step - loss: 0.0055 - val_loss: 0.0091\n",
      "Epoch 20/32\n",
      "385/385 [==============================] - 5s 14ms/step - loss: 0.0054 - val_loss: 0.0091\n",
      "Epoch 21/32\n",
      "385/385 [==============================] - 5s 14ms/step - loss: 0.0053 - val_loss: 0.0089\n",
      "Epoch 22/32\n",
      "385/385 [==============================] - 5s 14ms/step - loss: 0.0052 - val_loss: 0.0089\n",
      "Epoch 23/32\n",
      "385/385 [==============================] - 5s 14ms/step - loss: 0.0052 - val_loss: 0.0088\n",
      "Epoch 24/32\n",
      "385/385 [==============================] - 5s 14ms/step - loss: 0.0051 - val_loss: 0.0087\n",
      "Epoch 25/32\n",
      "385/385 [==============================] - 5s 14ms/step - loss: 0.0051 - val_loss: 0.0087\n",
      "Epoch 26/32\n",
      "385/385 [==============================] - 5s 14ms/step - loss: 0.0051 - val_loss: 0.0087\n",
      "Epoch 27/32\n",
      "385/385 [==============================] - 5s 13ms/step - loss: 0.0050 - val_loss: 0.0087\n",
      "Epoch 28/32\n",
      "385/385 [==============================] - 5s 14ms/step - loss: 0.0050 - val_loss: 0.0086\n",
      "Epoch 29/32\n",
      "385/385 [==============================] - 6s 14ms/step - loss: 0.0050 - val_loss: 0.0087\n",
      "Epoch 30/32\n",
      "385/385 [==============================] - 5s 13ms/step - loss: 0.0050 - val_loss: 0.0086\n",
      "Epoch 31/32\n",
      "385/385 [==============================] - 6s 14ms/step - loss: 0.0050 - val_loss: 0.0086\n",
      "Epoch 32/32\n",
      "385/385 [==============================] - 6s 15ms/step - loss: 0.0050 - val_loss: 0.0086\n",
      "\n",
      " >>> TIMESTEP 14 =============================================================== \n",
      "\n",
      "       index_       mse  Class\n",
      "1361  17743.0  5.244719      1\n",
      "347   25944.0  1.489678      1\n",
      "2803  95235.0  0.199102      1\n",
      "1014  69314.0  0.188327      1\n",
      "2542  15412.0  0.133297      1\n",
      "2303  92469.0  0.123976      1\n",
      "1668   5418.0  0.107607      1\n",
      "2702  22852.0  0.077101      1\n",
      "2738  65974.0  0.056458      1\n",
      "2376  38356.0  0.053028      1\n",
      "966   31070.0  0.048604      1\n",
      "2935  25641.0  0.048063      1\n",
      "237   33650.0  0.047699      1\n",
      "969    7554.0  0.041292      1\n",
      "112   96404.0  0.039490      1\n",
      "614   88534.0  0.039190      1\n",
      "1940  50219.0  0.035297      1\n",
      "2725  98122.0  0.032274      1\n",
      "1272  75323.0  0.031038      1\n",
      "502   18194.0  0.029068      0\n",
      "1652  77993.0  0.028663      0\n",
      "2506  14514.0  0.026788      0\n",
      "1395  76611.0  0.026765      0\n",
      "1378  98403.0  0.024529      0\n",
      "124   35930.0  0.024222      0\n",
      "667    6410.0  0.023932      0\n",
      "95    14592.0  0.022959      0\n",
      "1530  48035.0  0.021095      0\n",
      "1542  43064.0  0.020535      0\n",
      "93     4458.0  0.020266      0\n",
      "2427  50125.0  0.020263      0\n",
      "2423  24185.0  0.020157      0\n",
      "1374  54074.0  0.019753      0\n",
      "2536  69517.0  0.019722      0\n",
      "2212  67209.0  0.019519      0\n",
      "280   79173.0  0.017773      0\n",
      "2611  95113.0  0.015598      0\n",
      "2067  17292.0  0.015318      0\n",
      "2385  78657.0  0.014473      0\n",
      "1860  16091.0  0.012955      0\n",
      "2614  71077.0  0.012698      0\n",
      "1674  75356.0  0.012668      0\n",
      "1298  19227.0  0.012265      0\n",
      "941   93565.0  0.012262      0\n",
      "851   41981.0  0.011889      0\n",
      "1051  93604.0  0.011647      0\n",
      "386   21318.0  0.011562      0\n",
      "487   22631.0  0.011403      0\n",
      "1595  30192.0  0.011202      0\n",
      "88    72620.0  0.011063      0\n",
      "        pred_0    pred_1  index_  Class\n",
      "941   0.031111  0.968889   93565      1\n",
      "614   0.172222  0.827778   88534      1\n",
      "1051  0.260000  0.740000   93604      1\n",
      "2702  0.266667  0.733333   22852      1\n",
      "237   0.294444  0.705556   33650      1\n",
      "518   0.306667  0.693333   26575      1\n",
      "969   0.316667  0.683333    7554      1\n",
      "966   0.329444  0.670556   31070      1\n",
      "2803  0.362222  0.637778   95235      1\n",
      "2376  0.388222  0.611778   38356      1\n",
      "1940  0.389444  0.610556   50219      1\n",
      "112   0.398333  0.601667   96404      1\n",
      "1652  0.411667  0.588333   77993      1\n",
      "2303  0.432778  0.567222   92469      1\n",
      "1361  0.436667  0.563333   17743      1\n",
      "2935  0.439444  0.560556   25641      1\n",
      "2725  0.447222  0.552778   98122      1\n",
      "2738  0.480222  0.519778   65974      1\n",
      "1395  0.489444  0.510556   76611      1\n",
      "502   0.490000  0.510000   18194      1\n",
      "2716  0.493333  0.506667   35149      1\n",
      "747   0.500000  0.500000   69762      1\n",
      "1014  0.507000  0.493000   69314      1\n",
      "347   0.531667  0.468333   25944      1\n",
      "1668  0.562667  0.437333    5418      1\n",
      "124   0.574444  0.425556   35930      1\n",
      "785   0.613333  0.386667   55250      1\n",
      "1110  0.620000  0.380000   19589      1\n",
      "2542  0.621111  0.378889   15412      1\n",
      "1941  0.633333  0.366667   81758      1\n",
      "1272  0.636000  0.364000   75323      1\n",
      "1433  0.642222  0.357778   20310      1\n",
      "1530  0.646222  0.353778   48035      1\n",
      "1286  0.653333  0.346667    3856      1\n",
      "2506  0.653333  0.346667   14514      1\n",
      "35    0.680000  0.320000   88458      1\n",
      "1298  0.681667  0.318333   19227      1\n",
      "1521  0.713333  0.286667   12189      0\n",
      "2296  0.720000  0.280000   54284      0\n",
      "336   0.725333  0.274667   62066      0\n",
      "2536  0.737778  0.262222   69517      0\n",
      "2815  0.740000  0.260000   26617      0\n",
      "2427  0.741111  0.258889   50125      0\n",
      "165   0.746667  0.253333    9701      0\n",
      "1760  0.760000  0.240000   55145      0\n",
      "1293  0.763333  0.236667   96327      0\n",
      "1666  0.766667  0.233333   26502      0\n",
      "3095  0.773333  0.226667   36525      0\n",
      "2752  0.773333  0.226667    4967      0\n",
      "2711  0.786111  0.213889   73089      0\n",
      "Normal count: 21386\n",
      "Attack count: 1027\n",
      "Epoch 1/32\n",
      "385/385 [==============================] - 6s 13ms/step - loss: 0.0082 - val_loss: 0.0110\n",
      "Epoch 2/32\n",
      "385/385 [==============================] - 5s 12ms/step - loss: 0.0071 - val_loss: 0.0109\n",
      "Epoch 3/32\n",
      "385/385 [==============================] - 5s 13ms/step - loss: 0.0070 - val_loss: 0.0108\n",
      "Epoch 4/32\n",
      "385/385 [==============================] - 5s 12ms/step - loss: 0.0069 - val_loss: 0.0106\n",
      "Epoch 5/32\n",
      "385/385 [==============================] - 5s 12ms/step - loss: 0.0065 - val_loss: 0.0100\n",
      "Epoch 6/32\n",
      "385/385 [==============================] - 5s 12ms/step - loss: 0.0060 - val_loss: 0.0098\n",
      "Epoch 7/32\n",
      "385/385 [==============================] - 6s 14ms/step - loss: 0.0058 - val_loss: 0.0096\n",
      "Epoch 8/32\n",
      "385/385 [==============================] - 6s 16ms/step - loss: 0.0058 - val_loss: 0.0095\n",
      "Epoch 9/32\n",
      "385/385 [==============================] - 6s 14ms/step - loss: 0.0057 - val_loss: 0.0095\n",
      "Epoch 10/32\n",
      "385/385 [==============================] - 5s 13ms/step - loss: 0.0057 - val_loss: 0.0095\n",
      "Epoch 11/32\n",
      "385/385 [==============================] - 6s 16ms/step - loss: 0.0056 - val_loss: 0.0095\n",
      "Epoch 12/32\n",
      "385/385 [==============================] - 5s 14ms/step - loss: 0.0056 - val_loss: 0.0094\n",
      "Epoch 13/32\n",
      "385/385 [==============================] - 7s 18ms/step - loss: 0.0056 - val_loss: 0.0094\n",
      "Epoch 14/32\n",
      "385/385 [==============================] - 5s 13ms/step - loss: 0.0056 - val_loss: 0.0094\n",
      "Epoch 15/32\n",
      "385/385 [==============================] - 5s 12ms/step - loss: 0.0055 - val_loss: 0.0094\n",
      "Epoch 16/32\n",
      "385/385 [==============================] - 5s 12ms/step - loss: 0.0055 - val_loss: 0.0093\n",
      "Epoch 17/32\n",
      "385/385 [==============================] - 5s 13ms/step - loss: 0.0055 - val_loss: 0.0093\n",
      "Epoch 18/32\n",
      "385/385 [==============================] - 5s 13ms/step - loss: 0.0054 - val_loss: 0.0092\n",
      "Epoch 19/32\n",
      "385/385 [==============================] - 5s 13ms/step - loss: 0.0053 - val_loss: 0.0090\n",
      "Epoch 20/32\n",
      "385/385 [==============================] - 5s 13ms/step - loss: 0.0052 - val_loss: 0.0090\n",
      "Epoch 21/32\n",
      "385/385 [==============================] - 5s 13ms/step - loss: 0.0052 - val_loss: 0.0089\n",
      "Epoch 22/32\n",
      "385/385 [==============================] - 5s 13ms/step - loss: 0.0051 - val_loss: 0.0089\n",
      "Epoch 23/32\n",
      "385/385 [==============================] - 5s 14ms/step - loss: 0.0051 - val_loss: 0.0089\n",
      "Epoch 24/32\n",
      "385/385 [==============================] - 6s 16ms/step - loss: 0.0051 - val_loss: 0.0089\n",
      "Epoch 25/32\n",
      "385/385 [==============================] - 5s 13ms/step - loss: 0.0050 - val_loss: 0.0088\n",
      "Epoch 26/32\n",
      "385/385 [==============================] - 5s 13ms/step - loss: 0.0050 - val_loss: 0.0088\n",
      "Epoch 27/32\n",
      "385/385 [==============================] - 5s 12ms/step - loss: 0.0050 - val_loss: 0.0088\n",
      "Epoch 28/32\n",
      "385/385 [==============================] - 5s 13ms/step - loss: 0.0050 - val_loss: 0.0088\n",
      "Epoch 29/32\n",
      "385/385 [==============================] - 5s 12ms/step - loss: 0.0050 - val_loss: 0.0087\n",
      "Epoch 30/32\n",
      "385/385 [==============================] - 5s 13ms/step - loss: 0.0050 - val_loss: 0.0088\n",
      "Epoch 31/32\n",
      "385/385 [==============================] - 5s 12ms/step - loss: 0.0049 - val_loss: 0.0088\n",
      "Epoch 32/32\n",
      "385/385 [==============================] - 5s 12ms/step - loss: 0.0049 - val_loss: 0.0087\n",
      "\n",
      " >>> TIMESTEP 15 =============================================================== \n",
      "\n",
      "       index_       mse  Class\n",
      "1072  49587.0  0.137880      1\n",
      "1577  47003.0  0.070692      1\n",
      "147   46811.0  0.052063      1\n",
      "462    4995.0  0.050790      1\n",
      "318   79753.0  0.050698      1\n",
      "2650  51367.0  0.044863      1\n",
      "2568  52508.0  0.038694      1\n",
      "2246   6121.0  0.037475      1\n",
      "2595  96960.0  0.036812      1\n",
      "819   88565.0  0.034575      1\n",
      "361   36237.0  0.033593      1\n",
      "2745  14270.0  0.033500      1\n",
      "2476  64548.0  0.033176      1\n",
      "1907   7503.0  0.027439      0\n",
      "2853  69794.0  0.024750      0\n",
      "1770  54498.0  0.023218      0\n",
      "71    72439.0  0.021090      0\n",
      "2361  80909.0  0.020942      0\n",
      "170    2124.0  0.020360      0\n",
      "2347  82225.0  0.019766      0\n",
      "1333  39181.0  0.018685      0\n",
      "2489  95740.0  0.018457      0\n",
      "1148  47801.0  0.017638      0\n",
      "1639  49952.0  0.017121      0\n",
      "853   83574.0  0.016902      0\n",
      "709   29330.0  0.016658      0\n",
      "1655  67828.0  0.015829      0\n",
      "1147  96751.0  0.015492      0\n",
      "2201  13956.0  0.014007      0\n",
      "1295  13898.0  0.013955      0\n",
      "2793  49916.0  0.013889      0\n",
      "1543  75190.0  0.013757      0\n",
      "913   87290.0  0.013312      0\n",
      "1211  76221.0  0.013284      0\n",
      "774   72231.0  0.012941      0\n",
      "2473  26401.0  0.012896      0\n",
      "1797  31127.0  0.012639      0\n",
      "2342  85669.0  0.012337      0\n",
      "1779  76974.0  0.012038      0\n",
      "425   63310.0  0.012024      0\n",
      "2020  79255.0  0.011945      0\n",
      "2584  27322.0  0.011912      0\n",
      "1102  22521.0  0.011881      0\n",
      "3     18672.0  0.011869      0\n",
      "1435  40891.0  0.011766      0\n",
      "1625  61784.0  0.011689      0\n",
      "857   71832.0  0.011465      0\n",
      "1807  72227.0  0.011325      0\n",
      "1404  47163.0  0.011192      0\n",
      "999   62769.0  0.010889      0\n",
      "        pred_0    pred_1  index_  Class\n",
      "2745  0.146667  0.853333   14270      1\n",
      "2476  0.158889  0.841111   64548      1\n",
      "2246  0.163333  0.836667    6121      1\n",
      "2028  0.213333  0.786667   29254      1\n",
      "2374  0.233333  0.766667   64712      1\n",
      "284   0.243333  0.756667   62284      1\n",
      "2489  0.258889  0.741111   95740      1\n",
      "180   0.293333  0.706667   90076      1\n",
      "443   0.300000  0.700000   20213      1\n",
      "2568  0.304444  0.695556   52508      1\n",
      "1072  0.333333  0.666667   49587      1\n",
      "318   0.365556  0.634444   79753      1\n",
      "1577  0.370000  0.630000   47003      1\n",
      "361   0.374444  0.625556   36237      1\n",
      "819   0.379889  0.620111   88565      1\n",
      "2650  0.394667  0.605333   51367      1\n",
      "147   0.417000  0.583000   46811      1\n",
      "462   0.446111  0.553889    4995      1\n",
      "554   0.460000  0.540000    4892      1\n",
      "2595  0.486222  0.513778   96960      1\n",
      "1908  0.546667  0.453333   50253      1\n",
      "1390  0.560000  0.440000   80721      1\n",
      "2229  0.580000  0.420000   50756      1\n",
      "1907  0.592444  0.407556    7503      1\n",
      "71    0.636111  0.363889   72439      1\n",
      "1148  0.658111  0.341889   47801      1\n",
      "1748  0.673333  0.326667   67051      1\n",
      "165   0.677778  0.322222   58379      1\n",
      "2347  0.698222  0.301778   82225      1\n",
      "2743  0.702222  0.297778   53621      0\n",
      "103   0.760000  0.240000   65017      0\n",
      "1316  0.773333  0.226667   41977      0\n",
      "146   0.793333  0.206667   42620      0\n",
      "1338  0.800000  0.200000   93199      0\n",
      "2064  0.800000  0.200000   70045      0\n",
      "913   0.815556  0.184444   87290      0\n",
      "1195  0.815556  0.184444   71828      0\n",
      "1580  0.817778  0.182222   14204      0\n",
      "1987  0.820000  0.180000   98409      0\n",
      "1042  0.820000  0.180000   56173      0\n",
      "969   0.826667  0.173333   88940      0\n",
      "399   0.833333  0.166667   54516      0\n",
      "218   0.835000  0.165000   20439      0\n",
      "2436  0.840000  0.160000   95528      0\n",
      "2619  0.846667  0.153333   56152      0\n",
      "944   0.846667  0.153333    1187      0\n",
      "2815  0.850000  0.150000   48328      0\n",
      "432   0.853333  0.146667   19679      0\n",
      "407   0.853333  0.146667    2139      0\n",
      "1251  0.860000  0.140000   48605      0\n",
      "Normal count: 21444\n",
      "Attack count: 1069\n",
      "Epoch 1/32\n",
      "386/386 [==============================] - 6s 13ms/step - loss: 0.0084 - val_loss: 0.0111\n",
      "Epoch 2/32\n",
      "386/386 [==============================] - 5s 12ms/step - loss: 0.0071 - val_loss: 0.0110\n",
      "Epoch 3/32\n",
      "386/386 [==============================] - 5s 12ms/step - loss: 0.0071 - val_loss: 0.0110\n",
      "Epoch 4/32\n",
      "386/386 [==============================] - 5s 13ms/step - loss: 0.0070 - val_loss: 0.0109\n",
      "Epoch 5/32\n",
      "386/386 [==============================] - 5s 14ms/step - loss: 0.0069 - val_loss: 0.0107\n",
      "Epoch 6/32\n",
      "386/386 [==============================] - 6s 15ms/step - loss: 0.0066 - val_loss: 0.0106\n",
      "Epoch 7/32\n",
      "386/386 [==============================] - 5s 13ms/step - loss: 0.0065 - val_loss: 0.0105\n",
      "Epoch 8/32\n",
      "386/386 [==============================] - 5s 12ms/step - loss: 0.0065 - val_loss: 0.0104\n",
      "Epoch 9/32\n",
      "386/386 [==============================] - 5s 12ms/step - loss: 0.0064 - val_loss: 0.0104\n",
      "Epoch 10/32\n",
      "386/386 [==============================] - 6s 15ms/step - loss: 0.0064 - val_loss: 0.0104\n",
      "Epoch 11/32\n",
      "386/386 [==============================] - 5s 14ms/step - loss: 0.0064 - val_loss: 0.0104\n",
      "Epoch 12/32\n",
      "386/386 [==============================] - 5s 12ms/step - loss: 0.0064 - val_loss: 0.0104\n",
      "Epoch 13/32\n",
      "386/386 [==============================] - 4s 12ms/step - loss: 0.0063 - val_loss: 0.0103\n",
      "Epoch 14/32\n",
      "386/386 [==============================] - 5s 12ms/step - loss: 0.0063 - val_loss: 0.0103\n",
      "Epoch 15/32\n",
      "386/386 [==============================] - 5s 13ms/step - loss: 0.0063 - val_loss: 0.0103\n",
      "Epoch 16/32\n",
      "386/386 [==============================] - 5s 13ms/step - loss: 0.0063 - val_loss: 0.0103\n",
      "Epoch 17/32\n",
      "386/386 [==============================] - 5s 13ms/step - loss: 0.0062 - val_loss: 0.0102\n",
      "Epoch 18/32\n",
      "386/386 [==============================] - 5s 13ms/step - loss: 0.0060 - val_loss: 0.0097\n",
      "Epoch 19/32\n",
      "386/386 [==============================] - 5s 13ms/step - loss: 0.0055 - val_loss: 0.0094\n",
      "Epoch 20/32\n",
      "386/386 [==============================] - 5s 14ms/step - loss: 0.0053 - val_loss: 0.0092\n",
      "Epoch 21/32\n",
      "386/386 [==============================] - 5s 13ms/step - loss: 0.0052 - val_loss: 0.0091\n",
      "Epoch 22/32\n",
      "386/386 [==============================] - 5s 12ms/step - loss: 0.0052 - val_loss: 0.0090\n",
      "Epoch 23/32\n",
      "386/386 [==============================] - 5s 12ms/step - loss: 0.0051 - val_loss: 0.0090\n",
      "Epoch 24/32\n",
      "386/386 [==============================] - 5s 12ms/step - loss: 0.0050 - val_loss: 0.0089\n",
      "Epoch 25/32\n",
      "386/386 [==============================] - 5s 13ms/step - loss: 0.0049 - val_loss: 0.0088\n",
      "Epoch 26/32\n",
      "386/386 [==============================] - 6s 15ms/step - loss: 0.0048 - val_loss: 0.0087\n",
      "Epoch 27/32\n",
      "386/386 [==============================] - 6s 15ms/step - loss: 0.0048 - val_loss: 0.0087\n",
      "Epoch 28/32\n",
      "386/386 [==============================] - 6s 15ms/step - loss: 0.0047 - val_loss: 0.0087\n",
      "Epoch 29/32\n",
      "386/386 [==============================] - 5s 13ms/step - loss: 0.0047 - val_loss: 0.0086\n",
      "Epoch 30/32\n",
      "386/386 [==============================] - 6s 14ms/step - loss: 0.0047 - val_loss: 0.0086\n",
      "Epoch 31/32\n",
      "386/386 [==============================] - 6s 16ms/step - loss: 0.0046 - val_loss: 0.0086\n",
      "Epoch 32/32\n",
      "386/386 [==============================] - 5s 13ms/step - loss: 0.0046 - val_loss: 0.0085\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "for timestep in range(n_timesteps):\n",
    "\n",
    "    print(f\"\\n >>> TIMESTEP {timestep + 1 } =============================================================== \\n\")\n",
    "\n",
    "    # clear garbage\n",
    "    gc.collect()\n",
    "\n",
    "    x_unseen = unseen_data[timestep][1]\n",
    "\n",
    "    # Getting ranking from unsupervised learning and classifying based on probability threshold \n",
    "    unsup_model = unsupervised_models[timestep]\n",
    "    mse_table = get_unseen_class_unsup(unsup_model, x_unseen, MSE_THRESHOLD)\n",
    "    print(mse_table)\n",
    "\n",
    "    # Getting ranking from supervised learning and classifying based on probability threshold \n",
    "    sup_model = supervised_models[timestep]\n",
    "    unseen_class = get_unseen_class(sup_model, x_unseen, PROB_THRESHOLD)\n",
    "    print(unseen_class)\n",
    "\n",
    "    # Updating training set based on input from Supervised and Unsupervised Learning\n",
    "    x_train_, y_train_ = get_new_train(unseen_class, mse_table, x_unseen, x_train_, y_train_)\n",
    "\n",
    "    # Evaluating new training set using Supervised Learning\n",
    "    new_sup_model, time_res = rfc_eval(x_train_,y_train_,x_test,y_test)\n",
    "    supervised_models.append(new_sup_model)\n",
    "\n",
    "    # Saving results to Dictionary\n",
    "    results_dict['timestep ' + str(timestep+1)] = time_res\n",
    "\n",
    "    # Training new unsupervised model for next timestep\n",
    "    x_normal_train_, x_attack_train_ = get_normal_attack(x_train_, y_train_)\n",
    "    model_, history_ = build_autoencoder(x_normal_train_, x_attack_train_)\n",
    "    unsupervised_models.append(model_)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysing model performance across timesteps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestep 0</th>\n",
       "      <th>timestep 1</th>\n",
       "      <th>timestep 2</th>\n",
       "      <th>timestep 3</th>\n",
       "      <th>timestep 4</th>\n",
       "      <th>timestep 5</th>\n",
       "      <th>timestep 6</th>\n",
       "      <th>timestep 7</th>\n",
       "      <th>timestep 8</th>\n",
       "      <th>timestep 9</th>\n",
       "      <th>timestep 10</th>\n",
       "      <th>timestep 11</th>\n",
       "      <th>timestep 12</th>\n",
       "      <th>timestep 13</th>\n",
       "      <th>timestep 14</th>\n",
       "      <th>timestep 15</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>loss</th>\n",
       "      <td>0.023700</td>\n",
       "      <td>0.023352</td>\n",
       "      <td>0.022222</td>\n",
       "      <td>0.022471</td>\n",
       "      <td>0.023276</td>\n",
       "      <td>0.025025</td>\n",
       "      <td>0.023977</td>\n",
       "      <td>0.025323</td>\n",
       "      <td>0.025211</td>\n",
       "      <td>0.027386</td>\n",
       "      <td>0.025936</td>\n",
       "      <td>0.027837</td>\n",
       "      <td>0.026624</td>\n",
       "      <td>0.026664</td>\n",
       "      <td>0.026686</td>\n",
       "      <td>0.028530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.992000</td>\n",
       "      <td>0.992600</td>\n",
       "      <td>0.992550</td>\n",
       "      <td>0.993250</td>\n",
       "      <td>0.993600</td>\n",
       "      <td>0.993100</td>\n",
       "      <td>0.993100</td>\n",
       "      <td>0.993250</td>\n",
       "      <td>0.992800</td>\n",
       "      <td>0.992850</td>\n",
       "      <td>0.992700</td>\n",
       "      <td>0.992700</td>\n",
       "      <td>0.992800</td>\n",
       "      <td>0.992500</td>\n",
       "      <td>0.992350</td>\n",
       "      <td>0.992300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>auc</th>\n",
       "      <td>0.973176</td>\n",
       "      <td>0.976294</td>\n",
       "      <td>0.981158</td>\n",
       "      <td>0.982143</td>\n",
       "      <td>0.980019</td>\n",
       "      <td>0.974603</td>\n",
       "      <td>0.982111</td>\n",
       "      <td>0.977615</td>\n",
       "      <td>0.976932</td>\n",
       "      <td>0.970447</td>\n",
       "      <td>0.975723</td>\n",
       "      <td>0.970505</td>\n",
       "      <td>0.976121</td>\n",
       "      <td>0.978975</td>\n",
       "      <td>0.978938</td>\n",
       "      <td>0.971272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recall</th>\n",
       "      <td>0.215686</td>\n",
       "      <td>0.284314</td>\n",
       "      <td>0.289216</td>\n",
       "      <td>0.362745</td>\n",
       "      <td>0.406863</td>\n",
       "      <td>0.406863</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>0.431373</td>\n",
       "      <td>0.450980</td>\n",
       "      <td>0.475490</td>\n",
       "      <td>0.470588</td>\n",
       "      <td>0.480392</td>\n",
       "      <td>0.490196</td>\n",
       "      <td>0.465686</td>\n",
       "      <td>0.475490</td>\n",
       "      <td>0.480392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>precision</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.966667</td>\n",
       "      <td>0.936508</td>\n",
       "      <td>0.936709</td>\n",
       "      <td>0.922222</td>\n",
       "      <td>0.830000</td>\n",
       "      <td>0.817308</td>\n",
       "      <td>0.822430</td>\n",
       "      <td>0.741935</td>\n",
       "      <td>0.729323</td>\n",
       "      <td>0.716418</td>\n",
       "      <td>0.710145</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.698529</td>\n",
       "      <td>0.678322</td>\n",
       "      <td>0.671233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1_score</th>\n",
       "      <td>0.354839</td>\n",
       "      <td>0.439394</td>\n",
       "      <td>0.441948</td>\n",
       "      <td>0.522968</td>\n",
       "      <td>0.564626</td>\n",
       "      <td>0.546053</td>\n",
       "      <td>0.551948</td>\n",
       "      <td>0.565916</td>\n",
       "      <td>0.560976</td>\n",
       "      <td>0.575668</td>\n",
       "      <td>0.568047</td>\n",
       "      <td>0.573099</td>\n",
       "      <td>0.581395</td>\n",
       "      <td>0.558824</td>\n",
       "      <td>0.559078</td>\n",
       "      <td>0.560000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F2_score</th>\n",
       "      <td>0.255814</td>\n",
       "      <td>0.331050</td>\n",
       "      <td>0.335609</td>\n",
       "      <td>0.413408</td>\n",
       "      <td>0.458057</td>\n",
       "      <td>0.453057</td>\n",
       "      <td>0.461957</td>\n",
       "      <td>0.476706</td>\n",
       "      <td>0.489362</td>\n",
       "      <td>0.511064</td>\n",
       "      <td>0.505263</td>\n",
       "      <td>0.513627</td>\n",
       "      <td>0.523013</td>\n",
       "      <td>0.498950</td>\n",
       "      <td>0.505735</td>\n",
       "      <td>0.509356</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           timestep 0  timestep 1  timestep 2  timestep 3  timestep 4  \\\n",
       "loss         0.023700    0.023352    0.022222    0.022471    0.023276   \n",
       "accuracy     0.992000    0.992600    0.992550    0.993250    0.993600   \n",
       "auc          0.973176    0.976294    0.981158    0.982143    0.980019   \n",
       "recall       0.215686    0.284314    0.289216    0.362745    0.406863   \n",
       "precision    1.000000    0.966667    0.936508    0.936709    0.922222   \n",
       "F1_score     0.354839    0.439394    0.441948    0.522968    0.564626   \n",
       "F2_score     0.255814    0.331050    0.335609    0.413408    0.458057   \n",
       "\n",
       "           timestep 5  timestep 6  timestep 7  timestep 8  timestep 9  \\\n",
       "loss         0.025025    0.023977    0.025323    0.025211    0.027386   \n",
       "accuracy     0.993100    0.993100    0.993250    0.992800    0.992850   \n",
       "auc          0.974603    0.982111    0.977615    0.976932    0.970447   \n",
       "recall       0.406863    0.416667    0.431373    0.450980    0.475490   \n",
       "precision    0.830000    0.817308    0.822430    0.741935    0.729323   \n",
       "F1_score     0.546053    0.551948    0.565916    0.560976    0.575668   \n",
       "F2_score     0.453057    0.461957    0.476706    0.489362    0.511064   \n",
       "\n",
       "           timestep 10  timestep 11  timestep 12  timestep 13  timestep 14  \\\n",
       "loss          0.025936     0.027837     0.026624     0.026664     0.026686   \n",
       "accuracy      0.992700     0.992700     0.992800     0.992500     0.992350   \n",
       "auc           0.975723     0.970505     0.976121     0.978975     0.978938   \n",
       "recall        0.470588     0.480392     0.490196     0.465686     0.475490   \n",
       "precision     0.716418     0.710145     0.714286     0.698529     0.678322   \n",
       "F1_score      0.568047     0.573099     0.581395     0.558824     0.559078   \n",
       "F2_score      0.505263     0.513627     0.523013     0.498950     0.505735   \n",
       "\n",
       "           timestep 15  \n",
       "loss          0.028530  \n",
       "accuracy      0.992300  \n",
       "auc           0.971272  \n",
       "recall        0.480392  \n",
       "precision     0.671233  \n",
       "F1_score      0.560000  \n",
       "F2_score      0.509356  "
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(results_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Recall across Timesteps')"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1296x864 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "rs = pd.DataFrame(results_dict).T\n",
    "\n",
    "fig = plt.figure(figsize = (18, 12))\n",
    "\n",
    "ax = fig.add_subplot(2, 2, 1)\n",
    "rs.auc.plot(ax=ax)\n",
    "plt.title('AUC across Timesteps')\n",
    "\n",
    "ax = fig.add_subplot(2, 2, 2)\n",
    "rs.F2_score.plot(ax=ax)\n",
    "plt.title('F2_score across Timesteps')\n",
    "\n",
    "ax = fig.add_subplot(2, 2, 3)\n",
    "rs.precision.plot(ax=ax)\n",
    "plt.title('Precision across Timesteps')\n",
    "\n",
    "ax = fig.add_subplot(2, 2, 4)\n",
    "rs.recall.plot(ax=ax)\n",
    "plt.title('Recall across Timesteps')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c61f45fa4287766d1fe49b974af5d77a1d3358a56f5db6d660bceae78d85026e"
  },
  "kernelspec": {
   "display_name": "Python 3.8.11 64-bit ('gpuEnv': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
